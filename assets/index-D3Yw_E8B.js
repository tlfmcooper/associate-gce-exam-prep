(function(){const ae=document.createElement("link").relList;if(ae&&ae.supports&&ae.supports("modulepreload"))return;for(const R of document.querySelectorAll('link[rel="modulepreload"]'))m(R);new MutationObserver(R=>{for(const _ of R)if(_.type==="childList")for(const J of _.addedNodes)J.tagName==="LINK"&&J.rel==="modulepreload"&&m(J)}).observe(document,{childList:!0,subtree:!0});function Z(R){const _={};return R.integrity&&(_.integrity=R.integrity),R.referrerPolicy&&(_.referrerPolicy=R.referrerPolicy),R.crossOrigin==="use-credentials"?_.credentials="include":R.crossOrigin==="anonymous"?_.credentials="omit":_.credentials="same-origin",_}function m(R){if(R.ep)return;R.ep=!0;const _=Z(R);fetch(R.href,_)}})();var sl={exports:{}},xi={};/**
 * @license React
 * react-jsx-runtime.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var hp;function _h(){if(hp)return xi;hp=1;var P=Symbol.for("react.transitional.element"),ae=Symbol.for("react.fragment");function Z(m,R,_){var J=null;if(_!==void 0&&(J=""+_),R.key!==void 0&&(J=""+R.key),"key"in R){_={};for(var Se in R)Se!=="key"&&(_[Se]=R[Se])}else _=R;return R=_.ref,{$$typeof:P,type:m,key:J,ref:R!==void 0?R:null,props:_}}return xi.Fragment=ae,xi.jsx=Z,xi.jsxs=Z,xi}var mp;function Kh(){return mp||(mp=1,sl.exports=_h()),sl.exports}var A=Kh(),rl={exports:{}},Y={};/**
 * @license React
 * react.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var fp;function Fh(){if(fp)return Y;fp=1;var P=Symbol.for("react.transitional.element"),ae=Symbol.for("react.portal"),Z=Symbol.for("react.fragment"),m=Symbol.for("react.strict_mode"),R=Symbol.for("react.profiler"),_=Symbol.for("react.consumer"),J=Symbol.for("react.context"),Se=Symbol.for("react.forward_ref"),D=Symbol.for("react.suspense"),x=Symbol.for("react.memo"),N=Symbol.for("react.lazy"),V=Symbol.for("react.activity"),ne=Symbol.iterator;function Ne(l){return l===null||typeof l!="object"?null:(l=ne&&l[ne]||l["@@iterator"],typeof l=="function"?l:null)}var be={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},fe=Object.assign,et={};function ze(l,f,S){this.props=l,this.context=f,this.refs=et,this.updater=S||be}ze.prototype.isReactComponent={},ze.prototype.setState=function(l,f){if(typeof l!="object"&&typeof l!="function"&&l!=null)throw Error("takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,l,f,"setState")},ze.prototype.forceUpdate=function(l){this.updater.enqueueForceUpdate(this,l,"forceUpdate")};function Oe(){}Oe.prototype=ze.prototype;function ve(l,f,S){this.props=l,this.context=f,this.refs=et,this.updater=S||be}var Qe=ve.prototype=new Oe;Qe.constructor=ve,fe(Qe,ze.prototype),Qe.isPureReactComponent=!0;var tt=Array.isArray;function He(){}var O={H:null,A:null,T:null,S:null},$=Object.prototype.hasOwnProperty;function le(l,f,S){var k=S.ref;return{$$typeof:P,type:l,key:f,ref:k!==void 0?k:null,props:S}}function It(l,f){return le(l.type,f,l.props)}function nt(l){return typeof l=="object"&&l!==null&&l.$$typeof===P}function Ge(l){var f={"=":"=0",":":"=2"};return"$"+l.replace(/[=:]/g,function(S){return f[S]})}var Mt=/\/+/g;function ht(l,f){return typeof l=="object"&&l!==null&&l.key!=null?Ge(""+l.key):f.toString(36)}function ot(l){switch(l.status){case"fulfilled":return l.value;case"rejected":throw l.reason;default:switch(typeof l.status=="string"?l.then(He,He):(l.status="pending",l.then(function(f){l.status==="pending"&&(l.status="fulfilled",l.value=f)},function(f){l.status==="pending"&&(l.status="rejected",l.reason=f)})),l.status){case"fulfilled":return l.value;case"rejected":throw l.reason}}throw l}function v(l,f,S,k,z){var I=typeof l;(I==="undefined"||I==="boolean")&&(l=null);var Q=!1;if(l===null)Q=!0;else switch(I){case"bigint":case"string":case"number":Q=!0;break;case"object":switch(l.$$typeof){case P:case ae:Q=!0;break;case N:return Q=l._init,v(Q(l._payload),f,S,k,z)}}if(Q)return z=z(l),Q=k===""?"."+ht(l,0):k,tt(z)?(S="",Q!=null&&(S=Q.replace(Mt,"$&/")+"/"),v(z,f,S,"",function(Gt){return Gt})):z!=null&&(nt(z)&&(z=It(z,S+(z.key==null||l&&l.key===z.key?"":(""+z.key).replace(Mt,"$&/")+"/")+Q)),f.push(z)),1;Q=0;var Pe=k===""?".":k+":";if(tt(l))for(var ge=0;ge<l.length;ge++)k=l[ge],I=Pe+ht(k,ge),Q+=v(k,f,S,I,z);else if(ge=Ne(l),typeof ge=="function")for(l=ge.call(l),ge=0;!(k=l.next()).done;)k=k.value,I=Pe+ht(k,ge++),Q+=v(k,f,S,I,z);else if(I==="object"){if(typeof l.then=="function")return v(ot(l),f,S,k,z);throw f=String(l),Error("Objects are not valid as a React child (found: "+(f==="[object Object]"?"object with keys {"+Object.keys(l).join(", ")+"}":f)+"). If you meant to render a collection of children, use an array instead.")}return Q}function T(l,f,S){if(l==null)return l;var k=[],z=0;return v(l,k,"","",function(I){return f.call(S,I,z++)}),k}function j(l){if(l._status===-1){var f=l._result;f=f(),f.then(function(S){(l._status===0||l._status===-1)&&(l._status=1,l._result=S)},function(S){(l._status===0||l._status===-1)&&(l._status=2,l._result=S)}),l._status===-1&&(l._status=0,l._result=f)}if(l._status===1)return l._result.default;throw l._result}var oe=typeof reportError=="function"?reportError:function(l){if(typeof window=="object"&&typeof window.ErrorEvent=="function"){var f=new window.ErrorEvent("error",{bubbles:!0,cancelable:!0,message:typeof l=="object"&&l!==null&&typeof l.message=="string"?String(l.message):String(l),error:l});if(!window.dispatchEvent(f))return}else if(typeof process=="object"&&typeof process.emit=="function"){process.emit("uncaughtException",l);return}console.error(l)},E={map:T,forEach:function(l,f,S){T(l,function(){f.apply(this,arguments)},S)},count:function(l){var f=0;return T(l,function(){f++}),f},toArray:function(l){return T(l,function(f){return f})||[]},only:function(l){if(!nt(l))throw Error("React.Children.only expected to receive a single React element child.");return l}};return Y.Activity=V,Y.Children=E,Y.Component=ze,Y.Fragment=Z,Y.Profiler=R,Y.PureComponent=ve,Y.StrictMode=m,Y.Suspense=D,Y.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE=O,Y.__COMPILER_RUNTIME={__proto__:null,c:function(l){return O.H.useMemoCache(l)}},Y.cache=function(l){return function(){return l.apply(null,arguments)}},Y.cacheSignal=function(){return null},Y.cloneElement=function(l,f,S){if(l==null)throw Error("The argument must be a React element, but you passed "+l+".");var k=fe({},l.props),z=l.key;if(f!=null)for(I in f.key!==void 0&&(z=""+f.key),f)!$.call(f,I)||I==="key"||I==="__self"||I==="__source"||I==="ref"&&f.ref===void 0||(k[I]=f[I]);var I=arguments.length-2;if(I===1)k.children=S;else if(1<I){for(var Q=Array(I),Pe=0;Pe<I;Pe++)Q[Pe]=arguments[Pe+2];k.children=Q}return le(l.type,z,k)},Y.createContext=function(l){return l={$$typeof:J,_currentValue:l,_currentValue2:l,_threadCount:0,Provider:null,Consumer:null},l.Provider=l,l.Consumer={$$typeof:_,_context:l},l},Y.createElement=function(l,f,S){var k,z={},I=null;if(f!=null)for(k in f.key!==void 0&&(I=""+f.key),f)$.call(f,k)&&k!=="key"&&k!=="__self"&&k!=="__source"&&(z[k]=f[k]);var Q=arguments.length-2;if(Q===1)z.children=S;else if(1<Q){for(var Pe=Array(Q),ge=0;ge<Q;ge++)Pe[ge]=arguments[ge+2];z.children=Pe}if(l&&l.defaultProps)for(k in Q=l.defaultProps,Q)z[k]===void 0&&(z[k]=Q[k]);return le(l,I,z)},Y.createRef=function(){return{current:null}},Y.forwardRef=function(l){return{$$typeof:Se,render:l}},Y.isValidElement=nt,Y.lazy=function(l){return{$$typeof:N,_payload:{_status:-1,_result:l},_init:j}},Y.memo=function(l,f){return{$$typeof:x,type:l,compare:f===void 0?null:f}},Y.startTransition=function(l){var f=O.T,S={};O.T=S;try{var k=l(),z=O.S;z!==null&&z(S,k),typeof k=="object"&&k!==null&&typeof k.then=="function"&&k.then(He,oe)}catch(I){oe(I)}finally{f!==null&&S.types!==null&&(f.types=S.types),O.T=f}},Y.unstable_useCacheRefresh=function(){return O.H.useCacheRefresh()},Y.use=function(l){return O.H.use(l)},Y.useActionState=function(l,f,S){return O.H.useActionState(l,f,S)},Y.useCallback=function(l,f){return O.H.useCallback(l,f)},Y.useContext=function(l){return O.H.useContext(l)},Y.useDebugValue=function(){},Y.useDeferredValue=function(l,f){return O.H.useDeferredValue(l,f)},Y.useEffect=function(l,f){return O.H.useEffect(l,f)},Y.useEffectEvent=function(l){return O.H.useEffectEvent(l)},Y.useId=function(){return O.H.useId()},Y.useImperativeHandle=function(l,f,S){return O.H.useImperativeHandle(l,f,S)},Y.useInsertionEffect=function(l,f){return O.H.useInsertionEffect(l,f)},Y.useLayoutEffect=function(l,f){return O.H.useLayoutEffect(l,f)},Y.useMemo=function(l,f){return O.H.useMemo(l,f)},Y.useOptimistic=function(l,f){return O.H.useOptimistic(l,f)},Y.useReducer=function(l,f,S){return O.H.useReducer(l,f,S)},Y.useRef=function(l){return O.H.useRef(l)},Y.useState=function(l){return O.H.useState(l)},Y.useSyncExternalStore=function(l,f,S){return O.H.useSyncExternalStore(l,f,S)},Y.useTransition=function(){return O.H.useTransition()},Y.version="19.2.0",Y}var yp;function pl(){return yp||(yp=1,rl.exports=Fh()),rl.exports}var Be=pl(),ll={exports:{}},Ai={},cl={exports:{}},ul={};/**
 * @license React
 * scheduler.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var bp;function Xh(){return bp||(bp=1,(function(P){function ae(v,T){var j=v.length;v.push(T);e:for(;0<j;){var oe=j-1>>>1,E=v[oe];if(0<R(E,T))v[oe]=T,v[j]=E,j=oe;else break e}}function Z(v){return v.length===0?null:v[0]}function m(v){if(v.length===0)return null;var T=v[0],j=v.pop();if(j!==T){v[0]=j;e:for(var oe=0,E=v.length,l=E>>>1;oe<l;){var f=2*(oe+1)-1,S=v[f],k=f+1,z=v[k];if(0>R(S,j))k<E&&0>R(z,S)?(v[oe]=z,v[k]=j,oe=k):(v[oe]=S,v[f]=j,oe=f);else if(k<E&&0>R(z,j))v[oe]=z,v[k]=j,oe=k;else break e}}return T}function R(v,T){var j=v.sortIndex-T.sortIndex;return j!==0?j:v.id-T.id}if(P.unstable_now=void 0,typeof performance=="object"&&typeof performance.now=="function"){var _=performance;P.unstable_now=function(){return _.now()}}else{var J=Date,Se=J.now();P.unstable_now=function(){return J.now()-Se}}var D=[],x=[],N=1,V=null,ne=3,Ne=!1,be=!1,fe=!1,et=!1,ze=typeof setTimeout=="function"?setTimeout:null,Oe=typeof clearTimeout=="function"?clearTimeout:null,ve=typeof setImmediate<"u"?setImmediate:null;function Qe(v){for(var T=Z(x);T!==null;){if(T.callback===null)m(x);else if(T.startTime<=v)m(x),T.sortIndex=T.expirationTime,ae(D,T);else break;T=Z(x)}}function tt(v){if(fe=!1,Qe(v),!be)if(Z(D)!==null)be=!0,He||(He=!0,Ge());else{var T=Z(x);T!==null&&ot(tt,T.startTime-v)}}var He=!1,O=-1,$=5,le=-1;function It(){return et?!0:!(P.unstable_now()-le<$)}function nt(){if(et=!1,He){var v=P.unstable_now();le=v;var T=!0;try{e:{be=!1,fe&&(fe=!1,Oe(O),O=-1),Ne=!0;var j=ne;try{t:{for(Qe(v),V=Z(D);V!==null&&!(V.expirationTime>v&&It());){var oe=V.callback;if(typeof oe=="function"){V.callback=null,ne=V.priorityLevel;var E=oe(V.expirationTime<=v);if(v=P.unstable_now(),typeof E=="function"){V.callback=E,Qe(v),T=!0;break t}V===Z(D)&&m(D),Qe(v)}else m(D);V=Z(D)}if(V!==null)T=!0;else{var l=Z(x);l!==null&&ot(tt,l.startTime-v),T=!1}}break e}finally{V=null,ne=j,Ne=!1}T=void 0}}finally{T?Ge():He=!1}}}var Ge;if(typeof ve=="function")Ge=function(){ve(nt)};else if(typeof MessageChannel<"u"){var Mt=new MessageChannel,ht=Mt.port2;Mt.port1.onmessage=nt,Ge=function(){ht.postMessage(null)}}else Ge=function(){ze(nt,0)};function ot(v,T){O=ze(function(){v(P.unstable_now())},T)}P.unstable_IdlePriority=5,P.unstable_ImmediatePriority=1,P.unstable_LowPriority=4,P.unstable_NormalPriority=3,P.unstable_Profiling=null,P.unstable_UserBlockingPriority=2,P.unstable_cancelCallback=function(v){v.callback=null},P.unstable_forceFrameRate=function(v){0>v||125<v?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):$=0<v?Math.floor(1e3/v):5},P.unstable_getCurrentPriorityLevel=function(){return ne},P.unstable_next=function(v){switch(ne){case 1:case 2:case 3:var T=3;break;default:T=ne}var j=ne;ne=T;try{return v()}finally{ne=j}},P.unstable_requestPaint=function(){et=!0},P.unstable_runWithPriority=function(v,T){switch(v){case 1:case 2:case 3:case 4:case 5:break;default:v=3}var j=ne;ne=v;try{return T()}finally{ne=j}},P.unstable_scheduleCallback=function(v,T,j){var oe=P.unstable_now();switch(typeof j=="object"&&j!==null?(j=j.delay,j=typeof j=="number"&&0<j?oe+j:oe):j=oe,v){case 1:var E=-1;break;case 2:E=250;break;case 5:E=1073741823;break;case 4:E=1e4;break;default:E=5e3}return E=j+E,v={id:N++,callback:T,priorityLevel:v,startTime:j,expirationTime:E,sortIndex:-1},j>oe?(v.sortIndex=j,ae(x,v),Z(D)===null&&v===Z(x)&&(fe?(Oe(O),O=-1):fe=!0,ot(tt,j-oe))):(v.sortIndex=E,ae(D,v),be||Ne||(be=!0,He||(He=!0,Ge()))),v},P.unstable_shouldYield=It,P.unstable_wrapCallback=function(v){var T=ne;return function(){var j=ne;ne=T;try{return v.apply(this,arguments)}finally{ne=j}}}})(ul)),ul}var vp;function Zh(){return vp||(vp=1,cl.exports=Xh()),cl.exports}var dl={exports:{}},Re={};/**
 * @license React
 * react-dom.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var wp;function Jh(){if(wp)return Re;wp=1;var P=pl();function ae(D){var x="https://react.dev/errors/"+D;if(1<arguments.length){x+="?args[]="+encodeURIComponent(arguments[1]);for(var N=2;N<arguments.length;N++)x+="&args[]="+encodeURIComponent(arguments[N])}return"Minified React error #"+D+"; visit "+x+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}function Z(){}var m={d:{f:Z,r:function(){throw Error(ae(522))},D:Z,C:Z,L:Z,m:Z,X:Z,S:Z,M:Z},p:0,findDOMNode:null},R=Symbol.for("react.portal");function _(D,x,N){var V=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:R,key:V==null?null:""+V,children:D,containerInfo:x,implementation:N}}var J=P.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE;function Se(D,x){if(D==="font")return"";if(typeof x=="string")return x==="use-credentials"?x:""}return Re.__DOM_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE=m,Re.createPortal=function(D,x){var N=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!x||x.nodeType!==1&&x.nodeType!==9&&x.nodeType!==11)throw Error(ae(299));return _(D,x,null,N)},Re.flushSync=function(D){var x=J.T,N=m.p;try{if(J.T=null,m.p=2,D)return D()}finally{J.T=x,m.p=N,m.d.f()}},Re.preconnect=function(D,x){typeof D=="string"&&(x?(x=x.crossOrigin,x=typeof x=="string"?x==="use-credentials"?x:"":void 0):x=null,m.d.C(D,x))},Re.prefetchDNS=function(D){typeof D=="string"&&m.d.D(D)},Re.preinit=function(D,x){if(typeof D=="string"&&x&&typeof x.as=="string"){var N=x.as,V=Se(N,x.crossOrigin),ne=typeof x.integrity=="string"?x.integrity:void 0,Ne=typeof x.fetchPriority=="string"?x.fetchPriority:void 0;N==="style"?m.d.S(D,typeof x.precedence=="string"?x.precedence:void 0,{crossOrigin:V,integrity:ne,fetchPriority:Ne}):N==="script"&&m.d.X(D,{crossOrigin:V,integrity:ne,fetchPriority:Ne,nonce:typeof x.nonce=="string"?x.nonce:void 0})}},Re.preinitModule=function(D,x){if(typeof D=="string")if(typeof x=="object"&&x!==null){if(x.as==null||x.as==="script"){var N=Se(x.as,x.crossOrigin);m.d.M(D,{crossOrigin:N,integrity:typeof x.integrity=="string"?x.integrity:void 0,nonce:typeof x.nonce=="string"?x.nonce:void 0})}}else x==null&&m.d.M(D)},Re.preload=function(D,x){if(typeof D=="string"&&typeof x=="object"&&x!==null&&typeof x.as=="string"){var N=x.as,V=Se(N,x.crossOrigin);m.d.L(D,N,{crossOrigin:V,integrity:typeof x.integrity=="string"?x.integrity:void 0,nonce:typeof x.nonce=="string"?x.nonce:void 0,type:typeof x.type=="string"?x.type:void 0,fetchPriority:typeof x.fetchPriority=="string"?x.fetchPriority:void 0,referrerPolicy:typeof x.referrerPolicy=="string"?x.referrerPolicy:void 0,imageSrcSet:typeof x.imageSrcSet=="string"?x.imageSrcSet:void 0,imageSizes:typeof x.imageSizes=="string"?x.imageSizes:void 0,media:typeof x.media=="string"?x.media:void 0})}},Re.preloadModule=function(D,x){if(typeof D=="string")if(x){var N=Se(x.as,x.crossOrigin);m.d.m(D,{as:typeof x.as=="string"&&x.as!=="script"?x.as:void 0,crossOrigin:N,integrity:typeof x.integrity=="string"?x.integrity:void 0})}else m.d.m(D)},Re.requestFormReset=function(D){m.d.r(D)},Re.unstable_batchedUpdates=function(D,x){return D(x)},Re.useFormState=function(D,x,N){return J.H.useFormState(D,x,N)},Re.useFormStatus=function(){return J.H.useHostTransitionStatus()},Re.version="19.2.0",Re}var Cp;function $h(){if(Cp)return dl.exports;Cp=1;function P(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(P)}catch(ae){console.error(ae)}}return P(),dl.exports=Jh(),dl.exports}/**
 * @license React
 * react-dom-client.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Sp;function em(){if(Sp)return Ai;Sp=1;var P=Zh(),ae=pl(),Z=$h();function m(e){var t="https://react.dev/errors/"+e;if(1<arguments.length){t+="?args[]="+encodeURIComponent(arguments[1]);for(var n=2;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n])}return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}function R(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11)}function _(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do t=e,(t.flags&4098)!==0&&(n=t.return),e=t.return;while(e)}return t.tag===3?n:null}function J(e){if(e.tag===13){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function Se(e){if(e.tag===31){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function D(e){if(_(e)!==e)throw Error(m(188))}function x(e){var t=e.alternate;if(!t){if(t=_(e),t===null)throw Error(m(188));return t!==e?null:e}for(var n=e,o=t;;){var i=n.return;if(i===null)break;var a=i.alternate;if(a===null){if(o=i.return,o!==null){n=o;continue}break}if(i.child===a.child){for(a=i.child;a;){if(a===n)return D(i),e;if(a===o)return D(i),t;a=a.sibling}throw Error(m(188))}if(n.return!==o.return)n=i,o=a;else{for(var s=!1,r=i.child;r;){if(r===n){s=!0,n=i,o=a;break}if(r===o){s=!0,o=i,n=a;break}r=r.sibling}if(!s){for(r=a.child;r;){if(r===n){s=!0,n=a,o=i;break}if(r===o){s=!0,o=a,n=i;break}r=r.sibling}if(!s)throw Error(m(189))}}if(n.alternate!==o)throw Error(m(190))}if(n.tag!==3)throw Error(m(188));return n.stateNode.current===n?e:t}function N(e){var t=e.tag;if(t===5||t===26||t===27||t===6)return e;for(e=e.child;e!==null;){if(t=N(e),t!==null)return t;e=e.sibling}return null}var V=Object.assign,ne=Symbol.for("react.element"),Ne=Symbol.for("react.transitional.element"),be=Symbol.for("react.portal"),fe=Symbol.for("react.fragment"),et=Symbol.for("react.strict_mode"),ze=Symbol.for("react.profiler"),Oe=Symbol.for("react.consumer"),ve=Symbol.for("react.context"),Qe=Symbol.for("react.forward_ref"),tt=Symbol.for("react.suspense"),He=Symbol.for("react.suspense_list"),O=Symbol.for("react.memo"),$=Symbol.for("react.lazy"),le=Symbol.for("react.activity"),It=Symbol.for("react.memo_cache_sentinel"),nt=Symbol.iterator;function Ge(e){return e===null||typeof e!="object"?null:(e=nt&&e[nt]||e["@@iterator"],typeof e=="function"?e:null)}var Mt=Symbol.for("react.client.reference");function ht(e){if(e==null)return null;if(typeof e=="function")return e.$$typeof===Mt?null:e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case fe:return"Fragment";case ze:return"Profiler";case et:return"StrictMode";case tt:return"Suspense";case He:return"SuspenseList";case le:return"Activity"}if(typeof e=="object")switch(e.$$typeof){case be:return"Portal";case ve:return e.displayName||"Context";case Oe:return(e._context.displayName||"Context")+".Consumer";case Qe:var t=e.render;return e=e.displayName,e||(e=t.displayName||t.name||"",e=e!==""?"ForwardRef("+e+")":"ForwardRef"),e;case O:return t=e.displayName||null,t!==null?t:ht(e.type)||"Memo";case $:t=e._payload,e=e._init;try{return ht(e(t))}catch{}}return null}var ot=Array.isArray,v=ae.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,T=Z.__DOM_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,j={pending:!1,data:null,method:null,action:null},oe=[],E=-1;function l(e){return{current:e}}function f(e){0>E||(e.current=oe[E],oe[E]=null,E--)}function S(e,t){E++,oe[E]=e.current,e.current=t}var k=l(null),z=l(null),I=l(null),Q=l(null);function Pe(e,t){switch(S(I,t),S(z,e),S(k,null),t.nodeType){case 9:case 11:e=(e=t.documentElement)&&(e=e.namespaceURI)?jd(e):0;break;default:if(e=t.tagName,t=t.namespaceURI)t=jd(t),e=Yd(t,e);else switch(e){case"svg":e=1;break;case"math":e=2;break;default:e=0}}f(k),S(k,e)}function ge(){f(k),f(z),f(I)}function Gt(e){e.memoizedState!==null&&S(Q,e);var t=k.current,n=Yd(t,e.type);t!==n&&(S(z,e),S(k,n))}function Vt(e){z.current===e&&(f(k),f(z)),Q.current===e&&(f(Q),vi._currentValue=j)}var Ra,gl;function Tn(e){if(Ra===void 0)try{throw Error()}catch(n){var t=n.stack.trim().match(/\n( *(at )?)/);Ra=t&&t[1]||"",gl=-1<n.stack.indexOf(`
    at`)?" (<anonymous>)":-1<n.stack.indexOf("@")?"@unknown:0:0":""}return`
`+Ra+e+gl}var Oa=!1;function Qa(e,t){if(!e||Oa)return"";Oa=!0;var n=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{var o={DetermineComponentFrameRoot:function(){try{if(t){var C=function(){throw Error()};if(Object.defineProperty(C.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(C,[])}catch(y){var h=y}Reflect.construct(e,[],C)}else{try{C.call()}catch(y){h=y}e.call(C.prototype)}}else{try{throw Error()}catch(y){h=y}(C=e())&&typeof C.catch=="function"&&C.catch(function(){})}}catch(y){if(y&&h&&typeof y.stack=="string")return[y.stack,h.stack]}return[null,null]}};o.DetermineComponentFrameRoot.displayName="DetermineComponentFrameRoot";var i=Object.getOwnPropertyDescriptor(o.DetermineComponentFrameRoot,"name");i&&i.configurable&&Object.defineProperty(o.DetermineComponentFrameRoot,"name",{value:"DetermineComponentFrameRoot"});var a=o.DetermineComponentFrameRoot(),s=a[0],r=a[1];if(s&&r){var c=s.split(`
`),g=r.split(`
`);for(i=o=0;o<c.length&&!c[o].includes("DetermineComponentFrameRoot");)o++;for(;i<g.length&&!g[i].includes("DetermineComponentFrameRoot");)i++;if(o===c.length||i===g.length)for(o=c.length-1,i=g.length-1;1<=o&&0<=i&&c[o]!==g[i];)i--;for(;1<=o&&0<=i;o--,i--)if(c[o]!==g[i]){if(o!==1||i!==1)do if(o--,i--,0>i||c[o]!==g[i]){var b=`
`+c[o].replace(" at new "," at ");return e.displayName&&b.includes("<anonymous>")&&(b=b.replace("<anonymous>",e.displayName)),b}while(1<=o&&0<=i);break}}}finally{Oa=!1,Error.prepareStackTrace=n}return(n=e?e.displayName||e.name:"")?Tn(n):""}function Ap(e,t){switch(e.tag){case 26:case 27:case 5:return Tn(e.type);case 16:return Tn("Lazy");case 13:return e.child!==t&&t!==null?Tn("Suspense Fallback"):Tn("Suspense");case 19:return Tn("SuspenseList");case 0:case 15:return Qa(e.type,!1);case 11:return Qa(e.type.render,!1);case 1:return Qa(e.type,!0);case 31:return Tn("Activity");default:return""}}function hl(e){try{var t="",n=null;do t+=Ap(e,n),n=e,e=e.return;while(e);return t}catch(o){return`
Error generating stack: `+o.message+`
`+o.stack}}var Ha=Object.prototype.hasOwnProperty,Wa=P.unstable_scheduleCallback,_a=P.unstable_cancelCallback,kp=P.unstable_shouldYield,Ep=P.unstable_requestPaint,it=P.unstable_now,Tp=P.unstable_getCurrentPriorityLevel,ml=P.unstable_ImmediatePriority,fl=P.unstable_UserBlockingPriority,ki=P.unstable_NormalPriority,Mp=P.unstable_LowPriority,yl=P.unstable_IdlePriority,Pp=P.log,qp=P.unstable_setDisableYieldValue,zo=null,at=null;function en(e){if(typeof Pp=="function"&&qp(e),at&&typeof at.setStrictMode=="function")try{at.setStrictMode(zo,e)}catch{}}var st=Math.clz32?Math.clz32:Ip,zp=Math.log,Dp=Math.LN2;function Ip(e){return e>>>=0,e===0?32:31-(zp(e)/Dp|0)|0}var Ei=256,Ti=262144,Mi=4194304;function Mn(e){var t=e&42;if(t!==0)return t;switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:return 64;case 128:return 128;case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:return e&261888;case 262144:case 524288:case 1048576:case 2097152:return e&3932160;case 4194304:case 8388608:case 16777216:case 33554432:return e&62914560;case 67108864:return 67108864;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 0;default:return e}}function Pi(e,t,n){var o=e.pendingLanes;if(o===0)return 0;var i=0,a=e.suspendedLanes,s=e.pingedLanes;e=e.warmLanes;var r=o&134217727;return r!==0?(o=r&~a,o!==0?i=Mn(o):(s&=r,s!==0?i=Mn(s):n||(n=r&~e,n!==0&&(i=Mn(n))))):(r=o&~a,r!==0?i=Mn(r):s!==0?i=Mn(s):n||(n=o&~e,n!==0&&(i=Mn(n)))),i===0?0:t!==0&&t!==i&&(t&a)===0&&(a=i&-i,n=t&-t,a>=n||a===32&&(n&4194048)!==0)?t:i}function Do(e,t){return(e.pendingLanes&~(e.suspendedLanes&~e.pingedLanes)&t)===0}function Gp(e,t){switch(e){case 1:case 2:case 4:case 8:case 64:return t+250;case 16:case 32:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;case 4194304:case 8388608:case 16777216:case 33554432:return-1;case 67108864:case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function bl(){var e=Mi;return Mi<<=1,(Mi&62914560)===0&&(Mi=4194304),e}function Ka(e){for(var t=[],n=0;31>n;n++)t.push(e);return t}function Io(e,t){e.pendingLanes|=t,t!==268435456&&(e.suspendedLanes=0,e.pingedLanes=0,e.warmLanes=0)}function Vp(e,t,n,o,i,a){var s=e.pendingLanes;e.pendingLanes=n,e.suspendedLanes=0,e.pingedLanes=0,e.warmLanes=0,e.expiredLanes&=n,e.entangledLanes&=n,e.errorRecoveryDisabledLanes&=n,e.shellSuspendCounter=0;var r=e.entanglements,c=e.expirationTimes,g=e.hiddenUpdates;for(n=s&~n;0<n;){var b=31-st(n),C=1<<b;r[b]=0,c[b]=-1;var h=g[b];if(h!==null)for(g[b]=null,b=0;b<h.length;b++){var y=h[b];y!==null&&(y.lane&=-536870913)}n&=~C}o!==0&&vl(e,o,0),a!==0&&i===0&&e.tag!==0&&(e.suspendedLanes|=a&~(s&~t))}function vl(e,t,n){e.pendingLanes|=t,e.suspendedLanes&=~t;var o=31-st(t);e.entangledLanes|=t,e.entanglements[o]=e.entanglements[o]|1073741824|n&261930}function wl(e,t){var n=e.entangledLanes|=t;for(e=e.entanglements;n;){var o=31-st(n),i=1<<o;i&t|e[o]&t&&(e[o]|=t),n&=~i}}function Cl(e,t){var n=t&-t;return n=(n&42)!==0?1:Fa(n),(n&(e.suspendedLanes|t))!==0?0:n}function Fa(e){switch(e){case 2:e=1;break;case 8:e=4;break;case 32:e=16;break;case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:e=128;break;case 268435456:e=134217728;break;default:e=0}return e}function Xa(e){return e&=-e,2<e?8<e?(e&134217727)!==0?32:268435456:8:2}function Sl(){var e=T.p;return e!==0?e:(e=window.event,e===void 0?32:rp(e.type))}function xl(e,t){var n=T.p;try{return T.p=e,t()}finally{T.p=n}}var tn=Math.random().toString(36).slice(2),Ve="__reactFiber$"+tn,_e="__reactProps$"+tn,Wn="__reactContainer$"+tn,Za="__reactEvents$"+tn,Up="__reactListeners$"+tn,jp="__reactHandles$"+tn,Al="__reactResources$"+tn,Go="__reactMarker$"+tn;function Ja(e){delete e[Ve],delete e[_e],delete e[Za],delete e[Up],delete e[jp]}function _n(e){var t=e[Ve];if(t)return t;for(var n=e.parentNode;n;){if(t=n[Wn]||n[Ve]){if(n=t.alternate,t.child!==null||n!==null&&n.child!==null)for(e=Hd(e);e!==null;){if(n=e[Ve])return n;e=Hd(e)}return t}e=n,n=e.parentNode}return null}function Kn(e){if(e=e[Ve]||e[Wn]){var t=e.tag;if(t===5||t===6||t===13||t===31||t===26||t===27||t===3)return e}return null}function Vo(e){var t=e.tag;if(t===5||t===26||t===27||t===6)return e.stateNode;throw Error(m(33))}function Fn(e){var t=e[Al];return t||(t=e[Al]={hoistableStyles:new Map,hoistableScripts:new Map}),t}function De(e){e[Go]=!0}var kl=new Set,El={};function Pn(e,t){Xn(e,t),Xn(e+"Capture",t)}function Xn(e,t){for(El[e]=t,e=0;e<t.length;e++)kl.add(t[e])}var Yp=RegExp("^[:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD][:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\-.0-9\\u00B7\\u0300-\\u036F\\u203F-\\u2040]*$"),Tl={},Ml={};function Lp(e){return Ha.call(Ml,e)?!0:Ha.call(Tl,e)?!1:Yp.test(e)?Ml[e]=!0:(Tl[e]=!0,!1)}function qi(e,t,n){if(Lp(t))if(n===null)e.removeAttribute(t);else{switch(typeof n){case"undefined":case"function":case"symbol":e.removeAttribute(t);return;case"boolean":var o=t.toLowerCase().slice(0,5);if(o!=="data-"&&o!=="aria-"){e.removeAttribute(t);return}}e.setAttribute(t,""+n)}}function zi(e,t,n){if(n===null)e.removeAttribute(t);else{switch(typeof n){case"undefined":case"function":case"symbol":case"boolean":e.removeAttribute(t);return}e.setAttribute(t,""+n)}}function Ut(e,t,n,o){if(o===null)e.removeAttribute(n);else{switch(typeof o){case"undefined":case"function":case"symbol":case"boolean":e.removeAttribute(n);return}e.setAttributeNS(t,n,""+o)}}function mt(e){switch(typeof e){case"bigint":case"boolean":case"number":case"string":case"undefined":return e;case"object":return e;default:return""}}function Pl(e){var t=e.type;return(e=e.nodeName)&&e.toLowerCase()==="input"&&(t==="checkbox"||t==="radio")}function Bp(e,t,n){var o=Object.getOwnPropertyDescriptor(e.constructor.prototype,t);if(!e.hasOwnProperty(t)&&typeof o<"u"&&typeof o.get=="function"&&typeof o.set=="function"){var i=o.get,a=o.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(s){n=""+s,a.call(this,s)}}),Object.defineProperty(e,t,{enumerable:o.enumerable}),{getValue:function(){return n},setValue:function(s){n=""+s},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}function $a(e){if(!e._valueTracker){var t=Pl(e)?"checked":"value";e._valueTracker=Bp(e,t,""+e[t])}}function ql(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),o="";return e&&(o=Pl(e)?e.checked?"true":"false":e.value),e=o,e!==n?(t.setValue(e),!0):!1}function Di(e){if(e=e||(typeof document<"u"?document:void 0),typeof e>"u")return null;try{return e.activeElement||e.body}catch{return e.body}}var Np=/[\n"\\]/g;function ft(e){return e.replace(Np,function(t){return"\\"+t.charCodeAt(0).toString(16)+" "})}function es(e,t,n,o,i,a,s,r){e.name="",s!=null&&typeof s!="function"&&typeof s!="symbol"&&typeof s!="boolean"?e.type=s:e.removeAttribute("type"),t!=null?s==="number"?(t===0&&e.value===""||e.value!=t)&&(e.value=""+mt(t)):e.value!==""+mt(t)&&(e.value=""+mt(t)):s!=="submit"&&s!=="reset"||e.removeAttribute("value"),t!=null?ts(e,s,mt(t)):n!=null?ts(e,s,mt(n)):o!=null&&e.removeAttribute("value"),i==null&&a!=null&&(e.defaultChecked=!!a),i!=null&&(e.checked=i&&typeof i!="function"&&typeof i!="symbol"),r!=null&&typeof r!="function"&&typeof r!="symbol"&&typeof r!="boolean"?e.name=""+mt(r):e.removeAttribute("name")}function zl(e,t,n,o,i,a,s,r){if(a!=null&&typeof a!="function"&&typeof a!="symbol"&&typeof a!="boolean"&&(e.type=a),t!=null||n!=null){if(!(a!=="submit"&&a!=="reset"||t!=null)){$a(e);return}n=n!=null?""+mt(n):"",t=t!=null?""+mt(t):n,r||t===e.value||(e.value=t),e.defaultValue=t}o=o??i,o=typeof o!="function"&&typeof o!="symbol"&&!!o,e.checked=r?e.checked:!!o,e.defaultChecked=!!o,s!=null&&typeof s!="function"&&typeof s!="symbol"&&typeof s!="boolean"&&(e.name=s),$a(e)}function ts(e,t,n){t==="number"&&Di(e.ownerDocument)===e||e.defaultValue===""+n||(e.defaultValue=""+n)}function Zn(e,t,n,o){if(e=e.options,t){t={};for(var i=0;i<n.length;i++)t["$"+n[i]]=!0;for(n=0;n<e.length;n++)i=t.hasOwnProperty("$"+e[n].value),e[n].selected!==i&&(e[n].selected=i),i&&o&&(e[n].defaultSelected=!0)}else{for(n=""+mt(n),t=null,i=0;i<e.length;i++){if(e[i].value===n){e[i].selected=!0,o&&(e[i].defaultSelected=!0);return}t!==null||e[i].disabled||(t=e[i])}t!==null&&(t.selected=!0)}}function Dl(e,t,n){if(t!=null&&(t=""+mt(t),t!==e.value&&(e.value=t),n==null)){e.defaultValue!==t&&(e.defaultValue=t);return}e.defaultValue=n!=null?""+mt(n):""}function Il(e,t,n,o){if(t==null){if(o!=null){if(n!=null)throw Error(m(92));if(ot(o)){if(1<o.length)throw Error(m(93));o=o[0]}n=o}n==null&&(n=""),t=n}n=mt(t),e.defaultValue=n,o=e.textContent,o===n&&o!==""&&o!==null&&(e.value=o),$a(e)}function Jn(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&n.nodeType===3){n.nodeValue=t;return}}e.textContent=t}var Rp=new Set("animationIterationCount aspectRatio borderImageOutset borderImageSlice borderImageWidth boxFlex boxFlexGroup boxOrdinalGroup columnCount columns flex flexGrow flexPositive flexShrink flexNegative flexOrder gridArea gridRow gridRowEnd gridRowSpan gridRowStart gridColumn gridColumnEnd gridColumnSpan gridColumnStart fontWeight lineClamp lineHeight opacity order orphans scale tabSize widows zIndex zoom fillOpacity floodOpacity stopOpacity strokeDasharray strokeDashoffset strokeMiterlimit strokeOpacity strokeWidth MozAnimationIterationCount MozBoxFlex MozBoxFlexGroup MozLineClamp msAnimationIterationCount msFlex msZoom msFlexGrow msFlexNegative msFlexOrder msFlexPositive msFlexShrink msGridColumn msGridColumnSpan msGridRow msGridRowSpan WebkitAnimationIterationCount WebkitBoxFlex WebKitBoxFlexGroup WebkitBoxOrdinalGroup WebkitColumnCount WebkitColumns WebkitFlex WebkitFlexGrow WebkitFlexPositive WebkitFlexShrink WebkitLineClamp".split(" "));function Gl(e,t,n){var o=t.indexOf("--")===0;n==null||typeof n=="boolean"||n===""?o?e.setProperty(t,""):t==="float"?e.cssFloat="":e[t]="":o?e.setProperty(t,n):typeof n!="number"||n===0||Rp.has(t)?t==="float"?e.cssFloat=n:e[t]=(""+n).trim():e[t]=n+"px"}function Vl(e,t,n){if(t!=null&&typeof t!="object")throw Error(m(62));if(e=e.style,n!=null){for(var o in n)!n.hasOwnProperty(o)||t!=null&&t.hasOwnProperty(o)||(o.indexOf("--")===0?e.setProperty(o,""):o==="float"?e.cssFloat="":e[o]="");for(var i in t)o=t[i],t.hasOwnProperty(i)&&n[i]!==o&&Gl(e,i,o)}else for(var a in t)t.hasOwnProperty(a)&&Gl(e,a,t[a])}function ns(e){if(e.indexOf("-")===-1)return!1;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var Op=new Map([["acceptCharset","accept-charset"],["htmlFor","for"],["httpEquiv","http-equiv"],["crossOrigin","crossorigin"],["accentHeight","accent-height"],["alignmentBaseline","alignment-baseline"],["arabicForm","arabic-form"],["baselineShift","baseline-shift"],["capHeight","cap-height"],["clipPath","clip-path"],["clipRule","clip-rule"],["colorInterpolation","color-interpolation"],["colorInterpolationFilters","color-interpolation-filters"],["colorProfile","color-profile"],["colorRendering","color-rendering"],["dominantBaseline","dominant-baseline"],["enableBackground","enable-background"],["fillOpacity","fill-opacity"],["fillRule","fill-rule"],["floodColor","flood-color"],["floodOpacity","flood-opacity"],["fontFamily","font-family"],["fontSize","font-size"],["fontSizeAdjust","font-size-adjust"],["fontStretch","font-stretch"],["fontStyle","font-style"],["fontVariant","font-variant"],["fontWeight","font-weight"],["glyphName","glyph-name"],["glyphOrientationHorizontal","glyph-orientation-horizontal"],["glyphOrientationVertical","glyph-orientation-vertical"],["horizAdvX","horiz-adv-x"],["horizOriginX","horiz-origin-x"],["imageRendering","image-rendering"],["letterSpacing","letter-spacing"],["lightingColor","lighting-color"],["markerEnd","marker-end"],["markerMid","marker-mid"],["markerStart","marker-start"],["overlinePosition","overline-position"],["overlineThickness","overline-thickness"],["paintOrder","paint-order"],["panose-1","panose-1"],["pointerEvents","pointer-events"],["renderingIntent","rendering-intent"],["shapeRendering","shape-rendering"],["stopColor","stop-color"],["stopOpacity","stop-opacity"],["strikethroughPosition","strikethrough-position"],["strikethroughThickness","strikethrough-thickness"],["strokeDasharray","stroke-dasharray"],["strokeDashoffset","stroke-dashoffset"],["strokeLinecap","stroke-linecap"],["strokeLinejoin","stroke-linejoin"],["strokeMiterlimit","stroke-miterlimit"],["strokeOpacity","stroke-opacity"],["strokeWidth","stroke-width"],["textAnchor","text-anchor"],["textDecoration","text-decoration"],["textRendering","text-rendering"],["transformOrigin","transform-origin"],["underlinePosition","underline-position"],["underlineThickness","underline-thickness"],["unicodeBidi","unicode-bidi"],["unicodeRange","unicode-range"],["unitsPerEm","units-per-em"],["vAlphabetic","v-alphabetic"],["vHanging","v-hanging"],["vIdeographic","v-ideographic"],["vMathematical","v-mathematical"],["vectorEffect","vector-effect"],["vertAdvY","vert-adv-y"],["vertOriginX","vert-origin-x"],["vertOriginY","vert-origin-y"],["wordSpacing","word-spacing"],["writingMode","writing-mode"],["xmlnsXlink","xmlns:xlink"],["xHeight","x-height"]]),Qp=/^[\u0000-\u001F ]*j[\r\n\t]*a[\r\n\t]*v[\r\n\t]*a[\r\n\t]*s[\r\n\t]*c[\r\n\t]*r[\r\n\t]*i[\r\n\t]*p[\r\n\t]*t[\r\n\t]*:/i;function Ii(e){return Qp.test(""+e)?"javascript:throw new Error('React has blocked a javascript: URL as a security precaution.')":e}function jt(){}var os=null;function is(e){return e=e.target||e.srcElement||window,e.correspondingUseElement&&(e=e.correspondingUseElement),e.nodeType===3?e.parentNode:e}var $n=null,eo=null;function Ul(e){var t=Kn(e);if(t&&(e=t.stateNode)){var n=e[_e]||null;e:switch(e=t.stateNode,t.type){case"input":if(es(e,n.value,n.defaultValue,n.defaultValue,n.checked,n.defaultChecked,n.type,n.name),t=n.name,n.type==="radio"&&t!=null){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll('input[name="'+ft(""+t)+'"][type="radio"]'),t=0;t<n.length;t++){var o=n[t];if(o!==e&&o.form===e.form){var i=o[_e]||null;if(!i)throw Error(m(90));es(o,i.value,i.defaultValue,i.defaultValue,i.checked,i.defaultChecked,i.type,i.name)}}for(t=0;t<n.length;t++)o=n[t],o.form===e.form&&ql(o)}break e;case"textarea":Dl(e,n.value,n.defaultValue);break e;case"select":t=n.value,t!=null&&Zn(e,!!n.multiple,t,!1)}}}var as=!1;function jl(e,t,n){if(as)return e(t,n);as=!0;try{var o=e(t);return o}finally{if(as=!1,($n!==null||eo!==null)&&(wa(),$n&&(t=$n,e=eo,eo=$n=null,Ul(t),e)))for(t=0;t<e.length;t++)Ul(e[t])}}function Uo(e,t){var n=e.stateNode;if(n===null)return null;var o=n[_e]||null;if(o===null)return null;n=o[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(o=!o.disabled)||(e=e.type,o=!(e==="button"||e==="input"||e==="select"||e==="textarea")),e=!o;break e;default:e=!1}if(e)return null;if(n&&typeof n!="function")throw Error(m(231,t,typeof n));return n}var Yt=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),ss=!1;if(Yt)try{var jo={};Object.defineProperty(jo,"passive",{get:function(){ss=!0}}),window.addEventListener("test",jo,jo),window.removeEventListener("test",jo,jo)}catch{ss=!1}var nn=null,rs=null,Gi=null;function Yl(){if(Gi)return Gi;var e,t=rs,n=t.length,o,i="value"in nn?nn.value:nn.textContent,a=i.length;for(e=0;e<n&&t[e]===i[e];e++);var s=n-e;for(o=1;o<=s&&t[n-o]===i[a-o];o++);return Gi=i.slice(e,1<o?1-o:void 0)}function Vi(e){var t=e.keyCode;return"charCode"in e?(e=e.charCode,e===0&&t===13&&(e=13)):e=t,e===10&&(e=13),32<=e||e===13?e:0}function Ui(){return!0}function Ll(){return!1}function Ke(e){function t(n,o,i,a,s){this._reactName=n,this._targetInst=i,this.type=o,this.nativeEvent=a,this.target=s,this.currentTarget=null;for(var r in e)e.hasOwnProperty(r)&&(n=e[r],this[r]=n?n(a):a[r]);return this.isDefaultPrevented=(a.defaultPrevented!=null?a.defaultPrevented:a.returnValue===!1)?Ui:Ll,this.isPropagationStopped=Ll,this}return V(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var n=this.nativeEvent;n&&(n.preventDefault?n.preventDefault():typeof n.returnValue!="unknown"&&(n.returnValue=!1),this.isDefaultPrevented=Ui)},stopPropagation:function(){var n=this.nativeEvent;n&&(n.stopPropagation?n.stopPropagation():typeof n.cancelBubble!="unknown"&&(n.cancelBubble=!0),this.isPropagationStopped=Ui)},persist:function(){},isPersistent:Ui}),t}var qn={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},ji=Ke(qn),Yo=V({},qn,{view:0,detail:0}),Hp=Ke(Yo),ls,cs,Lo,Yi=V({},Yo,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:ds,button:0,buttons:0,relatedTarget:function(e){return e.relatedTarget===void 0?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==Lo&&(Lo&&e.type==="mousemove"?(ls=e.screenX-Lo.screenX,cs=e.screenY-Lo.screenY):cs=ls=0,Lo=e),ls)},movementY:function(e){return"movementY"in e?e.movementY:cs}}),Bl=Ke(Yi),Wp=V({},Yi,{dataTransfer:0}),_p=Ke(Wp),Kp=V({},Yo,{relatedTarget:0}),us=Ke(Kp),Fp=V({},qn,{animationName:0,elapsedTime:0,pseudoElement:0}),Xp=Ke(Fp),Zp=V({},qn,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),Jp=Ke(Zp),$p=V({},qn,{data:0}),Nl=Ke($p),eg={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},tg={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},ng={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function og(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):(e=ng[e])?!!t[e]:!1}function ds(){return og}var ig=V({},Yo,{key:function(e){if(e.key){var t=eg[e.key]||e.key;if(t!=="Unidentified")return t}return e.type==="keypress"?(e=Vi(e),e===13?"Enter":String.fromCharCode(e)):e.type==="keydown"||e.type==="keyup"?tg[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:ds,charCode:function(e){return e.type==="keypress"?Vi(e):0},keyCode:function(e){return e.type==="keydown"||e.type==="keyup"?e.keyCode:0},which:function(e){return e.type==="keypress"?Vi(e):e.type==="keydown"||e.type==="keyup"?e.keyCode:0}}),ag=Ke(ig),sg=V({},Yi,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),Rl=Ke(sg),rg=V({},Yo,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:ds}),lg=Ke(rg),cg=V({},qn,{propertyName:0,elapsedTime:0,pseudoElement:0}),ug=Ke(cg),dg=V({},Yi,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),pg=Ke(dg),gg=V({},qn,{newState:0,oldState:0}),hg=Ke(gg),mg=[9,13,27,32],ps=Yt&&"CompositionEvent"in window,Bo=null;Yt&&"documentMode"in document&&(Bo=document.documentMode);var fg=Yt&&"TextEvent"in window&&!Bo,Ol=Yt&&(!ps||Bo&&8<Bo&&11>=Bo),Ql=" ",Hl=!1;function Wl(e,t){switch(e){case"keyup":return mg.indexOf(t.keyCode)!==-1;case"keydown":return t.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function _l(e){return e=e.detail,typeof e=="object"&&"data"in e?e.data:null}var to=!1;function yg(e,t){switch(e){case"compositionend":return _l(t);case"keypress":return t.which!==32?null:(Hl=!0,Ql);case"textInput":return e=t.data,e===Ql&&Hl?null:e;default:return null}}function bg(e,t){if(to)return e==="compositionend"||!ps&&Wl(e,t)?(e=Yl(),Gi=rs=nn=null,to=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return Ol&&t.locale!=="ko"?null:t.data;default:return null}}var vg={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Kl(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t==="input"?!!vg[e.type]:t==="textarea"}function Fl(e,t,n,o){$n?eo?eo.push(o):eo=[o]:$n=o,t=Ta(t,"onChange"),0<t.length&&(n=new ji("onChange","change",null,n,o),e.push({event:n,listeners:t}))}var No=null,Ro=null;function wg(e){zd(e,0)}function Li(e){var t=Vo(e);if(ql(t))return e}function Xl(e,t){if(e==="change")return t}var Zl=!1;if(Yt){var gs;if(Yt){var hs="oninput"in document;if(!hs){var Jl=document.createElement("div");Jl.setAttribute("oninput","return;"),hs=typeof Jl.oninput=="function"}gs=hs}else gs=!1;Zl=gs&&(!document.documentMode||9<document.documentMode)}function $l(){No&&(No.detachEvent("onpropertychange",ec),Ro=No=null)}function ec(e){if(e.propertyName==="value"&&Li(Ro)){var t=[];Fl(t,Ro,e,is(e)),jl(wg,t)}}function Cg(e,t,n){e==="focusin"?($l(),No=t,Ro=n,No.attachEvent("onpropertychange",ec)):e==="focusout"&&$l()}function Sg(e){if(e==="selectionchange"||e==="keyup"||e==="keydown")return Li(Ro)}function xg(e,t){if(e==="click")return Li(t)}function Ag(e,t){if(e==="input"||e==="change")return Li(t)}function kg(e,t){return e===t&&(e!==0||1/e===1/t)||e!==e&&t!==t}var rt=typeof Object.is=="function"?Object.is:kg;function Oo(e,t){if(rt(e,t))return!0;if(typeof e!="object"||e===null||typeof t!="object"||t===null)return!1;var n=Object.keys(e),o=Object.keys(t);if(n.length!==o.length)return!1;for(o=0;o<n.length;o++){var i=n[o];if(!Ha.call(t,i)||!rt(e[i],t[i]))return!1}return!0}function tc(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function nc(e,t){var n=tc(e);e=0;for(var o;n;){if(n.nodeType===3){if(o=e+n.textContent.length,e<=t&&o>=t)return{node:n,offset:t-e};e=o}e:{for(;n;){if(n.nextSibling){n=n.nextSibling;break e}n=n.parentNode}n=void 0}n=tc(n)}}function oc(e,t){return e&&t?e===t?!0:e&&e.nodeType===3?!1:t&&t.nodeType===3?oc(e,t.parentNode):"contains"in e?e.contains(t):e.compareDocumentPosition?!!(e.compareDocumentPosition(t)&16):!1:!1}function ic(e){e=e!=null&&e.ownerDocument!=null&&e.ownerDocument.defaultView!=null?e.ownerDocument.defaultView:window;for(var t=Di(e.document);t instanceof e.HTMLIFrameElement;){try{var n=typeof t.contentWindow.location.href=="string"}catch{n=!1}if(n)e=t.contentWindow;else break;t=Di(e.document)}return t}function ms(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(t==="input"&&(e.type==="text"||e.type==="search"||e.type==="tel"||e.type==="url"||e.type==="password")||t==="textarea"||e.contentEditable==="true")}var Eg=Yt&&"documentMode"in document&&11>=document.documentMode,no=null,fs=null,Qo=null,ys=!1;function ac(e,t,n){var o=n.window===n?n.document:n.nodeType===9?n:n.ownerDocument;ys||no==null||no!==Di(o)||(o=no,"selectionStart"in o&&ms(o)?o={start:o.selectionStart,end:o.selectionEnd}:(o=(o.ownerDocument&&o.ownerDocument.defaultView||window).getSelection(),o={anchorNode:o.anchorNode,anchorOffset:o.anchorOffset,focusNode:o.focusNode,focusOffset:o.focusOffset}),Qo&&Oo(Qo,o)||(Qo=o,o=Ta(fs,"onSelect"),0<o.length&&(t=new ji("onSelect","select",null,t,n),e.push({event:t,listeners:o}),t.target=no)))}function zn(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n["Webkit"+e]="webkit"+t,n["Moz"+e]="moz"+t,n}var oo={animationend:zn("Animation","AnimationEnd"),animationiteration:zn("Animation","AnimationIteration"),animationstart:zn("Animation","AnimationStart"),transitionrun:zn("Transition","TransitionRun"),transitionstart:zn("Transition","TransitionStart"),transitioncancel:zn("Transition","TransitionCancel"),transitionend:zn("Transition","TransitionEnd")},bs={},sc={};Yt&&(sc=document.createElement("div").style,"AnimationEvent"in window||(delete oo.animationend.animation,delete oo.animationiteration.animation,delete oo.animationstart.animation),"TransitionEvent"in window||delete oo.transitionend.transition);function Dn(e){if(bs[e])return bs[e];if(!oo[e])return e;var t=oo[e],n;for(n in t)if(t.hasOwnProperty(n)&&n in sc)return bs[e]=t[n];return e}var rc=Dn("animationend"),lc=Dn("animationiteration"),cc=Dn("animationstart"),Tg=Dn("transitionrun"),Mg=Dn("transitionstart"),Pg=Dn("transitioncancel"),uc=Dn("transitionend"),dc=new Map,vs="abort auxClick beforeToggle cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");vs.push("scrollEnd");function kt(e,t){dc.set(e,t),Pn(t,[e])}var Bi=typeof reportError=="function"?reportError:function(e){if(typeof window=="object"&&typeof window.ErrorEvent=="function"){var t=new window.ErrorEvent("error",{bubbles:!0,cancelable:!0,message:typeof e=="object"&&e!==null&&typeof e.message=="string"?String(e.message):String(e),error:e});if(!window.dispatchEvent(t))return}else if(typeof process=="object"&&typeof process.emit=="function"){process.emit("uncaughtException",e);return}console.error(e)},yt=[],io=0,ws=0;function Ni(){for(var e=io,t=ws=io=0;t<e;){var n=yt[t];yt[t++]=null;var o=yt[t];yt[t++]=null;var i=yt[t];yt[t++]=null;var a=yt[t];if(yt[t++]=null,o!==null&&i!==null){var s=o.pending;s===null?i.next=i:(i.next=s.next,s.next=i),o.pending=i}a!==0&&pc(n,i,a)}}function Ri(e,t,n,o){yt[io++]=e,yt[io++]=t,yt[io++]=n,yt[io++]=o,ws|=o,e.lanes|=o,e=e.alternate,e!==null&&(e.lanes|=o)}function Cs(e,t,n,o){return Ri(e,t,n,o),Oi(e)}function In(e,t){return Ri(e,null,null,t),Oi(e)}function pc(e,t,n){e.lanes|=n;var o=e.alternate;o!==null&&(o.lanes|=n);for(var i=!1,a=e.return;a!==null;)a.childLanes|=n,o=a.alternate,o!==null&&(o.childLanes|=n),a.tag===22&&(e=a.stateNode,e===null||e._visibility&1||(i=!0)),e=a,a=a.return;return e.tag===3?(a=e.stateNode,i&&t!==null&&(i=31-st(n),e=a.hiddenUpdates,o=e[i],o===null?e[i]=[t]:o.push(t),t.lane=n|536870912),a):null}function Oi(e){if(50<pi)throw pi=0,qr=null,Error(m(185));for(var t=e.return;t!==null;)e=t,t=e.return;return e.tag===3?e.stateNode:null}var ao={};function qg(e,t,n,o){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.refCleanup=this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=o,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function lt(e,t,n,o){return new qg(e,t,n,o)}function Ss(e){return e=e.prototype,!(!e||!e.isReactComponent)}function Lt(e,t){var n=e.alternate;return n===null?(n=lt(e.tag,t,e.key,e.mode),n.elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.type=e.type,n.flags=0,n.subtreeFlags=0,n.deletions=null),n.flags=e.flags&65011712,n.childLanes=e.childLanes,n.lanes=e.lanes,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n.refCleanup=e.refCleanup,n}function gc(e,t){e.flags&=65011714;var n=e.alternate;return n===null?(e.childLanes=0,e.lanes=t,e.child=null,e.subtreeFlags=0,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null,e.stateNode=null):(e.childLanes=n.childLanes,e.lanes=n.lanes,e.child=n.child,e.subtreeFlags=0,e.deletions=null,e.memoizedProps=n.memoizedProps,e.memoizedState=n.memoizedState,e.updateQueue=n.updateQueue,e.type=n.type,t=n.dependencies,e.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext}),e}function Qi(e,t,n,o,i,a){var s=0;if(o=e,typeof e=="function")Ss(e)&&(s=1);else if(typeof e=="string")s=Vh(e,n,k.current)?26:e==="html"||e==="head"||e==="body"?27:5;else e:switch(e){case le:return e=lt(31,n,t,i),e.elementType=le,e.lanes=a,e;case fe:return Gn(n.children,i,a,t);case et:s=8,i|=24;break;case ze:return e=lt(12,n,t,i|2),e.elementType=ze,e.lanes=a,e;case tt:return e=lt(13,n,t,i),e.elementType=tt,e.lanes=a,e;case He:return e=lt(19,n,t,i),e.elementType=He,e.lanes=a,e;default:if(typeof e=="object"&&e!==null)switch(e.$$typeof){case ve:s=10;break e;case Oe:s=9;break e;case Qe:s=11;break e;case O:s=14;break e;case $:s=16,o=null;break e}s=29,n=Error(m(130,e===null?"null":typeof e,"")),o=null}return t=lt(s,n,t,i),t.elementType=e,t.type=o,t.lanes=a,t}function Gn(e,t,n,o){return e=lt(7,e,o,t),e.lanes=n,e}function xs(e,t,n){return e=lt(6,e,null,t),e.lanes=n,e}function hc(e){var t=lt(18,null,null,0);return t.stateNode=e,t}function As(e,t,n){return t=lt(4,e.children!==null?e.children:[],e.key,t),t.lanes=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}var mc=new WeakMap;function bt(e,t){if(typeof e=="object"&&e!==null){var n=mc.get(e);return n!==void 0?n:(t={value:e,source:t,stack:hl(t)},mc.set(e,t),t)}return{value:e,source:t,stack:hl(t)}}var so=[],ro=0,Hi=null,Ho=0,vt=[],wt=0,on=null,Pt=1,qt="";function Bt(e,t){so[ro++]=Ho,so[ro++]=Hi,Hi=e,Ho=t}function fc(e,t,n){vt[wt++]=Pt,vt[wt++]=qt,vt[wt++]=on,on=e;var o=Pt;e=qt;var i=32-st(o)-1;o&=~(1<<i),n+=1;var a=32-st(t)+i;if(30<a){var s=i-i%5;a=(o&(1<<s)-1).toString(32),o>>=s,i-=s,Pt=1<<32-st(t)+i|n<<i|o,qt=a+e}else Pt=1<<a|n<<i|o,qt=e}function ks(e){e.return!==null&&(Bt(e,1),fc(e,1,0))}function Es(e){for(;e===Hi;)Hi=so[--ro],so[ro]=null,Ho=so[--ro],so[ro]=null;for(;e===on;)on=vt[--wt],vt[wt]=null,qt=vt[--wt],vt[wt]=null,Pt=vt[--wt],vt[wt]=null}function yc(e,t){vt[wt++]=Pt,vt[wt++]=qt,vt[wt++]=on,Pt=t.id,qt=t.overflow,on=e}var Ue=null,he=null,X=!1,an=null,Ct=!1,Ts=Error(m(519));function sn(e){var t=Error(m(418,1<arguments.length&&arguments[1]!==void 0&&arguments[1]?"text":"HTML",""));throw Wo(bt(t,e)),Ts}function bc(e){var t=e.stateNode,n=e.type,o=e.memoizedProps;switch(t[Ve]=e,t[_e]=o,n){case"dialog":W("cancel",t),W("close",t);break;case"iframe":case"object":case"embed":W("load",t);break;case"video":case"audio":for(n=0;n<hi.length;n++)W(hi[n],t);break;case"source":W("error",t);break;case"img":case"image":case"link":W("error",t),W("load",t);break;case"details":W("toggle",t);break;case"input":W("invalid",t),zl(t,o.value,o.defaultValue,o.checked,o.defaultChecked,o.type,o.name,!0);break;case"select":W("invalid",t);break;case"textarea":W("invalid",t),Il(t,o.value,o.defaultValue,o.children)}n=o.children,typeof n!="string"&&typeof n!="number"&&typeof n!="bigint"||t.textContent===""+n||o.suppressHydrationWarning===!0||Vd(t.textContent,n)?(o.popover!=null&&(W("beforetoggle",t),W("toggle",t)),o.onScroll!=null&&W("scroll",t),o.onScrollEnd!=null&&W("scrollend",t),o.onClick!=null&&(t.onclick=jt),t=!0):t=!1,t||sn(e,!0)}function vc(e){for(Ue=e.return;Ue;)switch(Ue.tag){case 5:case 31:case 13:Ct=!1;return;case 27:case 3:Ct=!0;return;default:Ue=Ue.return}}function lo(e){if(e!==Ue)return!1;if(!X)return vc(e),X=!0,!1;var t=e.tag,n;if((n=t!==3&&t!==27)&&((n=t===5)&&(n=e.type,n=!(n!=="form"&&n!=="button")||Hr(e.type,e.memoizedProps)),n=!n),n&&he&&sn(e),vc(e),t===13){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(m(317));he=Qd(e)}else if(t===31){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(m(317));he=Qd(e)}else t===27?(t=he,wn(e.type)?(e=Xr,Xr=null,he=e):he=t):he=Ue?xt(e.stateNode.nextSibling):null;return!0}function Vn(){he=Ue=null,X=!1}function Ms(){var e=an;return e!==null&&(Je===null?Je=e:Je.push.apply(Je,e),an=null),e}function Wo(e){an===null?an=[e]:an.push(e)}var Ps=l(null),Un=null,Nt=null;function rn(e,t,n){S(Ps,t._currentValue),t._currentValue=n}function Rt(e){e._currentValue=Ps.current,f(Ps)}function qs(e,t,n){for(;e!==null;){var o=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,o!==null&&(o.childLanes|=t)):o!==null&&(o.childLanes&t)!==t&&(o.childLanes|=t),e===n)break;e=e.return}}function zs(e,t,n,o){var i=e.child;for(i!==null&&(i.return=e);i!==null;){var a=i.dependencies;if(a!==null){var s=i.child;a=a.firstContext;e:for(;a!==null;){var r=a;a=i;for(var c=0;c<t.length;c++)if(r.context===t[c]){a.lanes|=n,r=a.alternate,r!==null&&(r.lanes|=n),qs(a.return,n,e),o||(s=null);break e}a=r.next}}else if(i.tag===18){if(s=i.return,s===null)throw Error(m(341));s.lanes|=n,a=s.alternate,a!==null&&(a.lanes|=n),qs(s,n,e),s=null}else s=i.child;if(s!==null)s.return=i;else for(s=i;s!==null;){if(s===e){s=null;break}if(i=s.sibling,i!==null){i.return=s.return,s=i;break}s=s.return}i=s}}function co(e,t,n,o){e=null;for(var i=t,a=!1;i!==null;){if(!a){if((i.flags&524288)!==0)a=!0;else if((i.flags&262144)!==0)break}if(i.tag===10){var s=i.alternate;if(s===null)throw Error(m(387));if(s=s.memoizedProps,s!==null){var r=i.type;rt(i.pendingProps.value,s.value)||(e!==null?e.push(r):e=[r])}}else if(i===Q.current){if(s=i.alternate,s===null)throw Error(m(387));s.memoizedState.memoizedState!==i.memoizedState.memoizedState&&(e!==null?e.push(vi):e=[vi])}i=i.return}e!==null&&zs(t,e,n,o),t.flags|=262144}function Wi(e){for(e=e.firstContext;e!==null;){if(!rt(e.context._currentValue,e.memoizedValue))return!0;e=e.next}return!1}function jn(e){Un=e,Nt=null,e=e.dependencies,e!==null&&(e.firstContext=null)}function je(e){return wc(Un,e)}function _i(e,t){return Un===null&&jn(e),wc(e,t)}function wc(e,t){var n=t._currentValue;if(t={context:t,memoizedValue:n,next:null},Nt===null){if(e===null)throw Error(m(308));Nt=t,e.dependencies={lanes:0,firstContext:t},e.flags|=524288}else Nt=Nt.next=t;return n}var zg=typeof AbortController<"u"?AbortController:function(){var e=[],t=this.signal={aborted:!1,addEventListener:function(n,o){e.push(o)}};this.abort=function(){t.aborted=!0,e.forEach(function(n){return n()})}},Dg=P.unstable_scheduleCallback,Ig=P.unstable_NormalPriority,ke={$$typeof:ve,Consumer:null,Provider:null,_currentValue:null,_currentValue2:null,_threadCount:0};function Ds(){return{controller:new zg,data:new Map,refCount:0}}function _o(e){e.refCount--,e.refCount===0&&Dg(Ig,function(){e.controller.abort()})}var Ko=null,Is=0,uo=0,po=null;function Gg(e,t){if(Ko===null){var n=Ko=[];Is=0,uo=Ur(),po={status:"pending",value:void 0,then:function(o){n.push(o)}}}return Is++,t.then(Cc,Cc),t}function Cc(){if(--Is===0&&Ko!==null){po!==null&&(po.status="fulfilled");var e=Ko;Ko=null,uo=0,po=null;for(var t=0;t<e.length;t++)(0,e[t])()}}function Vg(e,t){var n=[],o={status:"pending",value:null,reason:null,then:function(i){n.push(i)}};return e.then(function(){o.status="fulfilled",o.value=t;for(var i=0;i<n.length;i++)(0,n[i])(t)},function(i){for(o.status="rejected",o.reason=i,i=0;i<n.length;i++)(0,n[i])(void 0)}),o}var Sc=v.S;v.S=function(e,t){ad=it(),typeof t=="object"&&t!==null&&typeof t.then=="function"&&Gg(e,t),Sc!==null&&Sc(e,t)};var Yn=l(null);function Gs(){var e=Yn.current;return e!==null?e:pe.pooledCache}function Ki(e,t){t===null?S(Yn,Yn.current):S(Yn,t.pool)}function xc(){var e=Gs();return e===null?null:{parent:ke._currentValue,pool:e}}var go=Error(m(460)),Vs=Error(m(474)),Fi=Error(m(542)),Xi={then:function(){}};function Ac(e){return e=e.status,e==="fulfilled"||e==="rejected"}function kc(e,t,n){switch(n=e[n],n===void 0?e.push(t):n!==t&&(t.then(jt,jt),t=n),t.status){case"fulfilled":return t.value;case"rejected":throw e=t.reason,Tc(e),e;default:if(typeof t.status=="string")t.then(jt,jt);else{if(e=pe,e!==null&&100<e.shellSuspendCounter)throw Error(m(482));e=t,e.status="pending",e.then(function(o){if(t.status==="pending"){var i=t;i.status="fulfilled",i.value=o}},function(o){if(t.status==="pending"){var i=t;i.status="rejected",i.reason=o}})}switch(t.status){case"fulfilled":return t.value;case"rejected":throw e=t.reason,Tc(e),e}throw Bn=t,go}}function Ln(e){try{var t=e._init;return t(e._payload)}catch(n){throw n!==null&&typeof n=="object"&&typeof n.then=="function"?(Bn=n,go):n}}var Bn=null;function Ec(){if(Bn===null)throw Error(m(459));var e=Bn;return Bn=null,e}function Tc(e){if(e===go||e===Fi)throw Error(m(483))}var ho=null,Fo=0;function Zi(e){var t=Fo;return Fo+=1,ho===null&&(ho=[]),kc(ho,e,t)}function Xo(e,t){t=t.props.ref,e.ref=t!==void 0?t:null}function Ji(e,t){throw t.$$typeof===ne?Error(m(525)):(e=Object.prototype.toString.call(t),Error(m(31,e==="[object Object]"?"object with keys {"+Object.keys(t).join(", ")+"}":e)))}function Mc(e){function t(d,u){if(e){var p=d.deletions;p===null?(d.deletions=[u],d.flags|=16):p.push(u)}}function n(d,u){if(!e)return null;for(;u!==null;)t(d,u),u=u.sibling;return null}function o(d){for(var u=new Map;d!==null;)d.key!==null?u.set(d.key,d):u.set(d.index,d),d=d.sibling;return u}function i(d,u){return d=Lt(d,u),d.index=0,d.sibling=null,d}function a(d,u,p){return d.index=p,e?(p=d.alternate,p!==null?(p=p.index,p<u?(d.flags|=67108866,u):p):(d.flags|=67108866,u)):(d.flags|=1048576,u)}function s(d){return e&&d.alternate===null&&(d.flags|=67108866),d}function r(d,u,p,w){return u===null||u.tag!==6?(u=xs(p,d.mode,w),u.return=d,u):(u=i(u,p),u.return=d,u)}function c(d,u,p,w){var G=p.type;return G===fe?b(d,u,p.props.children,w,p.key):u!==null&&(u.elementType===G||typeof G=="object"&&G!==null&&G.$$typeof===$&&Ln(G)===u.type)?(u=i(u,p.props),Xo(u,p),u.return=d,u):(u=Qi(p.type,p.key,p.props,null,d.mode,w),Xo(u,p),u.return=d,u)}function g(d,u,p,w){return u===null||u.tag!==4||u.stateNode.containerInfo!==p.containerInfo||u.stateNode.implementation!==p.implementation?(u=As(p,d.mode,w),u.return=d,u):(u=i(u,p.children||[]),u.return=d,u)}function b(d,u,p,w,G){return u===null||u.tag!==7?(u=Gn(p,d.mode,w,G),u.return=d,u):(u=i(u,p),u.return=d,u)}function C(d,u,p){if(typeof u=="string"&&u!==""||typeof u=="number"||typeof u=="bigint")return u=xs(""+u,d.mode,p),u.return=d,u;if(typeof u=="object"&&u!==null){switch(u.$$typeof){case Ne:return p=Qi(u.type,u.key,u.props,null,d.mode,p),Xo(p,u),p.return=d,p;case be:return u=As(u,d.mode,p),u.return=d,u;case $:return u=Ln(u),C(d,u,p)}if(ot(u)||Ge(u))return u=Gn(u,d.mode,p,null),u.return=d,u;if(typeof u.then=="function")return C(d,Zi(u),p);if(u.$$typeof===ve)return C(d,_i(d,u),p);Ji(d,u)}return null}function h(d,u,p,w){var G=u!==null?u.key:null;if(typeof p=="string"&&p!==""||typeof p=="number"||typeof p=="bigint")return G!==null?null:r(d,u,""+p,w);if(typeof p=="object"&&p!==null){switch(p.$$typeof){case Ne:return p.key===G?c(d,u,p,w):null;case be:return p.key===G?g(d,u,p,w):null;case $:return p=Ln(p),h(d,u,p,w)}if(ot(p)||Ge(p))return G!==null?null:b(d,u,p,w,null);if(typeof p.then=="function")return h(d,u,Zi(p),w);if(p.$$typeof===ve)return h(d,u,_i(d,p),w);Ji(d,p)}return null}function y(d,u,p,w,G){if(typeof w=="string"&&w!==""||typeof w=="number"||typeof w=="bigint")return d=d.get(p)||null,r(u,d,""+w,G);if(typeof w=="object"&&w!==null){switch(w.$$typeof){case Ne:return d=d.get(w.key===null?p:w.key)||null,c(u,d,w,G);case be:return d=d.get(w.key===null?p:w.key)||null,g(u,d,w,G);case $:return w=Ln(w),y(d,u,p,w,G)}if(ot(w)||Ge(w))return d=d.get(p)||null,b(u,d,w,G,null);if(typeof w.then=="function")return y(d,u,p,Zi(w),G);if(w.$$typeof===ve)return y(d,u,p,_i(u,w),G);Ji(u,w)}return null}function M(d,u,p,w){for(var G=null,ee=null,q=u,B=u=0,F=null;q!==null&&B<p.length;B++){q.index>B?(F=q,q=null):F=q.sibling;var te=h(d,q,p[B],w);if(te===null){q===null&&(q=F);break}e&&q&&te.alternate===null&&t(d,q),u=a(te,u,B),ee===null?G=te:ee.sibling=te,ee=te,q=F}if(B===p.length)return n(d,q),X&&Bt(d,B),G;if(q===null){for(;B<p.length;B++)q=C(d,p[B],w),q!==null&&(u=a(q,u,B),ee===null?G=q:ee.sibling=q,ee=q);return X&&Bt(d,B),G}for(q=o(q);B<p.length;B++)F=y(q,d,B,p[B],w),F!==null&&(e&&F.alternate!==null&&q.delete(F.key===null?B:F.key),u=a(F,u,B),ee===null?G=F:ee.sibling=F,ee=F);return e&&q.forEach(function(kn){return t(d,kn)}),X&&Bt(d,B),G}function U(d,u,p,w){if(p==null)throw Error(m(151));for(var G=null,ee=null,q=u,B=u=0,F=null,te=p.next();q!==null&&!te.done;B++,te=p.next()){q.index>B?(F=q,q=null):F=q.sibling;var kn=h(d,q,te.value,w);if(kn===null){q===null&&(q=F);break}e&&q&&kn.alternate===null&&t(d,q),u=a(kn,u,B),ee===null?G=kn:ee.sibling=kn,ee=kn,q=F}if(te.done)return n(d,q),X&&Bt(d,B),G;if(q===null){for(;!te.done;B++,te=p.next())te=C(d,te.value,w),te!==null&&(u=a(te,u,B),ee===null?G=te:ee.sibling=te,ee=te);return X&&Bt(d,B),G}for(q=o(q);!te.done;B++,te=p.next())te=y(q,d,B,te.value,w),te!==null&&(e&&te.alternate!==null&&q.delete(te.key===null?B:te.key),u=a(te,u,B),ee===null?G=te:ee.sibling=te,ee=te);return e&&q.forEach(function(Wh){return t(d,Wh)}),X&&Bt(d,B),G}function de(d,u,p,w){if(typeof p=="object"&&p!==null&&p.type===fe&&p.key===null&&(p=p.props.children),typeof p=="object"&&p!==null){switch(p.$$typeof){case Ne:e:{for(var G=p.key;u!==null;){if(u.key===G){if(G=p.type,G===fe){if(u.tag===7){n(d,u.sibling),w=i(u,p.props.children),w.return=d,d=w;break e}}else if(u.elementType===G||typeof G=="object"&&G!==null&&G.$$typeof===$&&Ln(G)===u.type){n(d,u.sibling),w=i(u,p.props),Xo(w,p),w.return=d,d=w;break e}n(d,u);break}else t(d,u);u=u.sibling}p.type===fe?(w=Gn(p.props.children,d.mode,w,p.key),w.return=d,d=w):(w=Qi(p.type,p.key,p.props,null,d.mode,w),Xo(w,p),w.return=d,d=w)}return s(d);case be:e:{for(G=p.key;u!==null;){if(u.key===G)if(u.tag===4&&u.stateNode.containerInfo===p.containerInfo&&u.stateNode.implementation===p.implementation){n(d,u.sibling),w=i(u,p.children||[]),w.return=d,d=w;break e}else{n(d,u);break}else t(d,u);u=u.sibling}w=As(p,d.mode,w),w.return=d,d=w}return s(d);case $:return p=Ln(p),de(d,u,p,w)}if(ot(p))return M(d,u,p,w);if(Ge(p)){if(G=Ge(p),typeof G!="function")throw Error(m(150));return p=G.call(p),U(d,u,p,w)}if(typeof p.then=="function")return de(d,u,Zi(p),w);if(p.$$typeof===ve)return de(d,u,_i(d,p),w);Ji(d,p)}return typeof p=="string"&&p!==""||typeof p=="number"||typeof p=="bigint"?(p=""+p,u!==null&&u.tag===6?(n(d,u.sibling),w=i(u,p),w.return=d,d=w):(n(d,u),w=xs(p,d.mode,w),w.return=d,d=w),s(d)):n(d,u)}return function(d,u,p,w){try{Fo=0;var G=de(d,u,p,w);return ho=null,G}catch(q){if(q===go||q===Fi)throw q;var ee=lt(29,q,null,d.mode);return ee.lanes=w,ee.return=d,ee}finally{}}}var Nn=Mc(!0),Pc=Mc(!1),ln=!1;function Us(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,lanes:0,hiddenCallbacks:null},callbacks:null}}function js(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,callbacks:null})}function cn(e){return{lane:e,tag:0,payload:null,callback:null,next:null}}function un(e,t,n){var o=e.updateQueue;if(o===null)return null;if(o=o.shared,(ie&2)!==0){var i=o.pending;return i===null?t.next=t:(t.next=i.next,i.next=t),o.pending=t,t=Oi(e),pc(e,null,n),t}return Ri(e,o,t,n),Oi(e)}function Zo(e,t,n){if(t=t.updateQueue,t!==null&&(t=t.shared,(n&4194048)!==0)){var o=t.lanes;o&=e.pendingLanes,n|=o,t.lanes=n,wl(e,n)}}function Ys(e,t){var n=e.updateQueue,o=e.alternate;if(o!==null&&(o=o.updateQueue,n===o)){var i=null,a=null;if(n=n.firstBaseUpdate,n!==null){do{var s={lane:n.lane,tag:n.tag,payload:n.payload,callback:null,next:null};a===null?i=a=s:a=a.next=s,n=n.next}while(n!==null);a===null?i=a=t:a=a.next=t}else i=a=t;n={baseState:o.baseState,firstBaseUpdate:i,lastBaseUpdate:a,shared:o.shared,callbacks:o.callbacks},e.updateQueue=n;return}e=n.lastBaseUpdate,e===null?n.firstBaseUpdate=t:e.next=t,n.lastBaseUpdate=t}var Ls=!1;function Jo(){if(Ls){var e=po;if(e!==null)throw e}}function $o(e,t,n,o){Ls=!1;var i=e.updateQueue;ln=!1;var a=i.firstBaseUpdate,s=i.lastBaseUpdate,r=i.shared.pending;if(r!==null){i.shared.pending=null;var c=r,g=c.next;c.next=null,s===null?a=g:s.next=g,s=c;var b=e.alternate;b!==null&&(b=b.updateQueue,r=b.lastBaseUpdate,r!==s&&(r===null?b.firstBaseUpdate=g:r.next=g,b.lastBaseUpdate=c))}if(a!==null){var C=i.baseState;s=0,b=g=c=null,r=a;do{var h=r.lane&-536870913,y=h!==r.lane;if(y?(K&h)===h:(o&h)===h){h!==0&&h===uo&&(Ls=!0),b!==null&&(b=b.next={lane:0,tag:r.tag,payload:r.payload,callback:null,next:null});e:{var M=e,U=r;h=t;var de=n;switch(U.tag){case 1:if(M=U.payload,typeof M=="function"){C=M.call(de,C,h);break e}C=M;break e;case 3:M.flags=M.flags&-65537|128;case 0:if(M=U.payload,h=typeof M=="function"?M.call(de,C,h):M,h==null)break e;C=V({},C,h);break e;case 2:ln=!0}}h=r.callback,h!==null&&(e.flags|=64,y&&(e.flags|=8192),y=i.callbacks,y===null?i.callbacks=[h]:y.push(h))}else y={lane:h,tag:r.tag,payload:r.payload,callback:r.callback,next:null},b===null?(g=b=y,c=C):b=b.next=y,s|=h;if(r=r.next,r===null){if(r=i.shared.pending,r===null)break;y=r,r=y.next,y.next=null,i.lastBaseUpdate=y,i.shared.pending=null}}while(!0);b===null&&(c=C),i.baseState=c,i.firstBaseUpdate=g,i.lastBaseUpdate=b,a===null&&(i.shared.lanes=0),mn|=s,e.lanes=s,e.memoizedState=C}}function qc(e,t){if(typeof e!="function")throw Error(m(191,e));e.call(t)}function zc(e,t){var n=e.callbacks;if(n!==null)for(e.callbacks=null,e=0;e<n.length;e++)qc(n[e],t)}var mo=l(null),$i=l(0);function Dc(e,t){e=Zt,S($i,e),S(mo,t),Zt=e|t.baseLanes}function Bs(){S($i,Zt),S(mo,mo.current)}function Ns(){Zt=$i.current,f(mo),f($i)}var ct=l(null),St=null;function dn(e){var t=e.alternate;S(xe,xe.current&1),S(ct,e),St===null&&(t===null||mo.current!==null||t.memoizedState!==null)&&(St=e)}function Rs(e){S(xe,xe.current),S(ct,e),St===null&&(St=e)}function Ic(e){e.tag===22?(S(xe,xe.current),S(ct,e),St===null&&(St=e)):pn()}function pn(){S(xe,xe.current),S(ct,ct.current)}function ut(e){f(ct),St===e&&(St=null),f(xe)}var xe=l(0);function ea(e){for(var t=e;t!==null;){if(t.tag===13){var n=t.memoizedState;if(n!==null&&(n=n.dehydrated,n===null||Kr(n)||Fr(n)))return t}else if(t.tag===19&&(t.memoizedProps.revealOrder==="forwards"||t.memoizedProps.revealOrder==="backwards"||t.memoizedProps.revealOrder==="unstable_legacy-backwards"||t.memoizedProps.revealOrder==="together")){if((t.flags&128)!==0)return t}else if(t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var Ot=0,L=null,ce=null,Ee=null,ta=!1,fo=!1,Rn=!1,na=0,ei=0,yo=null,Ug=0;function we(){throw Error(m(321))}function Os(e,t){if(t===null)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!rt(e[n],t[n]))return!1;return!0}function Qs(e,t,n,o,i,a){return Ot=a,L=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,v.H=e===null||e.memoizedState===null?fu:ar,Rn=!1,a=n(o,i),Rn=!1,fo&&(a=Vc(t,n,o,i)),Gc(e),a}function Gc(e){v.H=oi;var t=ce!==null&&ce.next!==null;if(Ot=0,Ee=ce=L=null,ta=!1,ei=0,yo=null,t)throw Error(m(300));e===null||Te||(e=e.dependencies,e!==null&&Wi(e)&&(Te=!0))}function Vc(e,t,n,o){L=e;var i=0;do{if(fo&&(yo=null),ei=0,fo=!1,25<=i)throw Error(m(301));if(i+=1,Ee=ce=null,e.updateQueue!=null){var a=e.updateQueue;a.lastEffect=null,a.events=null,a.stores=null,a.memoCache!=null&&(a.memoCache.index=0)}v.H=yu,a=t(n,o)}while(fo);return a}function jg(){var e=v.H,t=e.useState()[0];return t=typeof t.then=="function"?ti(t):t,e=e.useState()[0],(ce!==null?ce.memoizedState:null)!==e&&(L.flags|=1024),t}function Hs(){var e=na!==0;return na=0,e}function Ws(e,t,n){t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~n}function _s(e){if(ta){for(e=e.memoizedState;e!==null;){var t=e.queue;t!==null&&(t.pending=null),e=e.next}ta=!1}Ot=0,Ee=ce=L=null,fo=!1,ei=na=0,yo=null}function We(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return Ee===null?L.memoizedState=Ee=e:Ee=Ee.next=e,Ee}function Ae(){if(ce===null){var e=L.alternate;e=e!==null?e.memoizedState:null}else e=ce.next;var t=Ee===null?L.memoizedState:Ee.next;if(t!==null)Ee=t,ce=e;else{if(e===null)throw L.alternate===null?Error(m(467)):Error(m(310));ce=e,e={memoizedState:ce.memoizedState,baseState:ce.baseState,baseQueue:ce.baseQueue,queue:ce.queue,next:null},Ee===null?L.memoizedState=Ee=e:Ee=Ee.next=e}return Ee}function oa(){return{lastEffect:null,events:null,stores:null,memoCache:null}}function ti(e){var t=ei;return ei+=1,yo===null&&(yo=[]),e=kc(yo,e,t),t=L,(Ee===null?t.memoizedState:Ee.next)===null&&(t=t.alternate,v.H=t===null||t.memoizedState===null?fu:ar),e}function ia(e){if(e!==null&&typeof e=="object"){if(typeof e.then=="function")return ti(e);if(e.$$typeof===ve)return je(e)}throw Error(m(438,String(e)))}function Ks(e){var t=null,n=L.updateQueue;if(n!==null&&(t=n.memoCache),t==null){var o=L.alternate;o!==null&&(o=o.updateQueue,o!==null&&(o=o.memoCache,o!=null&&(t={data:o.data.map(function(i){return i.slice()}),index:0})))}if(t==null&&(t={data:[],index:0}),n===null&&(n=oa(),L.updateQueue=n),n.memoCache=t,n=t.data[t.index],n===void 0)for(n=t.data[t.index]=Array(e),o=0;o<e;o++)n[o]=It;return t.index++,n}function Qt(e,t){return typeof t=="function"?t(e):t}function aa(e){var t=Ae();return Fs(t,ce,e)}function Fs(e,t,n){var o=e.queue;if(o===null)throw Error(m(311));o.lastRenderedReducer=n;var i=e.baseQueue,a=o.pending;if(a!==null){if(i!==null){var s=i.next;i.next=a.next,a.next=s}t.baseQueue=i=a,o.pending=null}if(a=e.baseState,i===null)e.memoizedState=a;else{t=i.next;var r=s=null,c=null,g=t,b=!1;do{var C=g.lane&-536870913;if(C!==g.lane?(K&C)===C:(Ot&C)===C){var h=g.revertLane;if(h===0)c!==null&&(c=c.next={lane:0,revertLane:0,gesture:null,action:g.action,hasEagerState:g.hasEagerState,eagerState:g.eagerState,next:null}),C===uo&&(b=!0);else if((Ot&h)===h){g=g.next,h===uo&&(b=!0);continue}else C={lane:0,revertLane:g.revertLane,gesture:null,action:g.action,hasEagerState:g.hasEagerState,eagerState:g.eagerState,next:null},c===null?(r=c=C,s=a):c=c.next=C,L.lanes|=h,mn|=h;C=g.action,Rn&&n(a,C),a=g.hasEagerState?g.eagerState:n(a,C)}else h={lane:C,revertLane:g.revertLane,gesture:g.gesture,action:g.action,hasEagerState:g.hasEagerState,eagerState:g.eagerState,next:null},c===null?(r=c=h,s=a):c=c.next=h,L.lanes|=C,mn|=C;g=g.next}while(g!==null&&g!==t);if(c===null?s=a:c.next=r,!rt(a,e.memoizedState)&&(Te=!0,b&&(n=po,n!==null)))throw n;e.memoizedState=a,e.baseState=s,e.baseQueue=c,o.lastRenderedState=a}return i===null&&(o.lanes=0),[e.memoizedState,o.dispatch]}function Xs(e){var t=Ae(),n=t.queue;if(n===null)throw Error(m(311));n.lastRenderedReducer=e;var o=n.dispatch,i=n.pending,a=t.memoizedState;if(i!==null){n.pending=null;var s=i=i.next;do a=e(a,s.action),s=s.next;while(s!==i);rt(a,t.memoizedState)||(Te=!0),t.memoizedState=a,t.baseQueue===null&&(t.baseState=a),n.lastRenderedState=a}return[a,o]}function Uc(e,t,n){var o=L,i=Ae(),a=X;if(a){if(n===void 0)throw Error(m(407));n=n()}else n=t();var s=!rt((ce||i).memoizedState,n);if(s&&(i.memoizedState=n,Te=!0),i=i.queue,$s(Lc.bind(null,o,i,e),[e]),i.getSnapshot!==t||s||Ee!==null&&Ee.memoizedState.tag&1){if(o.flags|=2048,bo(9,{destroy:void 0},Yc.bind(null,o,i,n,t),null),pe===null)throw Error(m(349));a||(Ot&127)!==0||jc(o,t,n)}return n}function jc(e,t,n){e.flags|=16384,e={getSnapshot:t,value:n},t=L.updateQueue,t===null?(t=oa(),L.updateQueue=t,t.stores=[e]):(n=t.stores,n===null?t.stores=[e]:n.push(e))}function Yc(e,t,n,o){t.value=n,t.getSnapshot=o,Bc(t)&&Nc(e)}function Lc(e,t,n){return n(function(){Bc(t)&&Nc(e)})}function Bc(e){var t=e.getSnapshot;e=e.value;try{var n=t();return!rt(e,n)}catch{return!0}}function Nc(e){var t=In(e,2);t!==null&&$e(t,e,2)}function Zs(e){var t=We();if(typeof e=="function"){var n=e;if(e=n(),Rn){en(!0);try{n()}finally{en(!1)}}}return t.memoizedState=t.baseState=e,t.queue={pending:null,lanes:0,dispatch:null,lastRenderedReducer:Qt,lastRenderedState:e},t}function Rc(e,t,n,o){return e.baseState=n,Fs(e,ce,typeof o=="function"?o:Qt)}function Yg(e,t,n,o,i){if(la(e))throw Error(m(485));if(e=t.action,e!==null){var a={payload:i,action:e,next:null,isTransition:!0,status:"pending",value:null,reason:null,listeners:[],then:function(s){a.listeners.push(s)}};v.T!==null?n(!0):a.isTransition=!1,o(a),n=t.pending,n===null?(a.next=t.pending=a,Oc(t,a)):(a.next=n.next,t.pending=n.next=a)}}function Oc(e,t){var n=t.action,o=t.payload,i=e.state;if(t.isTransition){var a=v.T,s={};v.T=s;try{var r=n(i,o),c=v.S;c!==null&&c(s,r),Qc(e,t,r)}catch(g){Js(e,t,g)}finally{a!==null&&s.types!==null&&(a.types=s.types),v.T=a}}else try{a=n(i,o),Qc(e,t,a)}catch(g){Js(e,t,g)}}function Qc(e,t,n){n!==null&&typeof n=="object"&&typeof n.then=="function"?n.then(function(o){Hc(e,t,o)},function(o){return Js(e,t,o)}):Hc(e,t,n)}function Hc(e,t,n){t.status="fulfilled",t.value=n,Wc(t),e.state=n,t=e.pending,t!==null&&(n=t.next,n===t?e.pending=null:(n=n.next,t.next=n,Oc(e,n)))}function Js(e,t,n){var o=e.pending;if(e.pending=null,o!==null){o=o.next;do t.status="rejected",t.reason=n,Wc(t),t=t.next;while(t!==o)}e.action=null}function Wc(e){e=e.listeners;for(var t=0;t<e.length;t++)(0,e[t])()}function _c(e,t){return t}function Kc(e,t){if(X){var n=pe.formState;if(n!==null){e:{var o=L;if(X){if(he){t:{for(var i=he,a=Ct;i.nodeType!==8;){if(!a){i=null;break t}if(i=xt(i.nextSibling),i===null){i=null;break t}}a=i.data,i=a==="F!"||a==="F"?i:null}if(i){he=xt(i.nextSibling),o=i.data==="F!";break e}}sn(o)}o=!1}o&&(t=n[0])}}return n=We(),n.memoizedState=n.baseState=t,o={pending:null,lanes:0,dispatch:null,lastRenderedReducer:_c,lastRenderedState:t},n.queue=o,n=gu.bind(null,L,o),o.dispatch=n,o=Zs(!1),a=ir.bind(null,L,!1,o.queue),o=We(),i={state:t,dispatch:null,action:e,pending:null},o.queue=i,n=Yg.bind(null,L,i,a,n),i.dispatch=n,o.memoizedState=e,[t,n,!1]}function Fc(e){var t=Ae();return Xc(t,ce,e)}function Xc(e,t,n){if(t=Fs(e,t,_c)[0],e=aa(Qt)[0],typeof t=="object"&&t!==null&&typeof t.then=="function")try{var o=ti(t)}catch(s){throw s===go?Fi:s}else o=t;t=Ae();var i=t.queue,a=i.dispatch;return n!==t.memoizedState&&(L.flags|=2048,bo(9,{destroy:void 0},Lg.bind(null,i,n),null)),[o,a,e]}function Lg(e,t){e.action=t}function Zc(e){var t=Ae(),n=ce;if(n!==null)return Xc(t,n,e);Ae(),t=t.memoizedState,n=Ae();var o=n.queue.dispatch;return n.memoizedState=e,[t,o,!1]}function bo(e,t,n,o){return e={tag:e,create:n,deps:o,inst:t,next:null},t=L.updateQueue,t===null&&(t=oa(),L.updateQueue=t),n=t.lastEffect,n===null?t.lastEffect=e.next=e:(o=n.next,n.next=e,e.next=o,t.lastEffect=e),e}function Jc(){return Ae().memoizedState}function sa(e,t,n,o){var i=We();L.flags|=e,i.memoizedState=bo(1|t,{destroy:void 0},n,o===void 0?null:o)}function ra(e,t,n,o){var i=Ae();o=o===void 0?null:o;var a=i.memoizedState.inst;ce!==null&&o!==null&&Os(o,ce.memoizedState.deps)?i.memoizedState=bo(t,a,n,o):(L.flags|=e,i.memoizedState=bo(1|t,a,n,o))}function $c(e,t){sa(8390656,8,e,t)}function $s(e,t){ra(2048,8,e,t)}function Bg(e){L.flags|=4;var t=L.updateQueue;if(t===null)t=oa(),L.updateQueue=t,t.events=[e];else{var n=t.events;n===null?t.events=[e]:n.push(e)}}function eu(e){var t=Ae().memoizedState;return Bg({ref:t,nextImpl:e}),function(){if((ie&2)!==0)throw Error(m(440));return t.impl.apply(void 0,arguments)}}function tu(e,t){return ra(4,2,e,t)}function nu(e,t){return ra(4,4,e,t)}function ou(e,t){if(typeof t=="function"){e=e();var n=t(e);return function(){typeof n=="function"?n():t(null)}}if(t!=null)return e=e(),t.current=e,function(){t.current=null}}function iu(e,t,n){n=n!=null?n.concat([e]):null,ra(4,4,ou.bind(null,t,e),n)}function er(){}function au(e,t){var n=Ae();t=t===void 0?null:t;var o=n.memoizedState;return t!==null&&Os(t,o[1])?o[0]:(n.memoizedState=[e,t],e)}function su(e,t){var n=Ae();t=t===void 0?null:t;var o=n.memoizedState;if(t!==null&&Os(t,o[1]))return o[0];if(o=e(),Rn){en(!0);try{e()}finally{en(!1)}}return n.memoizedState=[o,t],o}function tr(e,t,n){return n===void 0||(Ot&1073741824)!==0&&(K&261930)===0?e.memoizedState=t:(e.memoizedState=n,e=rd(),L.lanes|=e,mn|=e,n)}function ru(e,t,n,o){return rt(n,t)?n:mo.current!==null?(e=tr(e,n,o),rt(e,t)||(Te=!0),e):(Ot&42)===0||(Ot&1073741824)!==0&&(K&261930)===0?(Te=!0,e.memoizedState=n):(e=rd(),L.lanes|=e,mn|=e,t)}function lu(e,t,n,o,i){var a=T.p;T.p=a!==0&&8>a?a:8;var s=v.T,r={};v.T=r,ir(e,!1,t,n);try{var c=i(),g=v.S;if(g!==null&&g(r,c),c!==null&&typeof c=="object"&&typeof c.then=="function"){var b=Vg(c,o);ni(e,t,b,gt(e))}else ni(e,t,o,gt(e))}catch(C){ni(e,t,{then:function(){},status:"rejected",reason:C},gt())}finally{T.p=a,s!==null&&r.types!==null&&(s.types=r.types),v.T=s}}function Ng(){}function nr(e,t,n,o){if(e.tag!==5)throw Error(m(476));var i=cu(e).queue;lu(e,i,t,j,n===null?Ng:function(){return uu(e),n(o)})}function cu(e){var t=e.memoizedState;if(t!==null)return t;t={memoizedState:j,baseState:j,baseQueue:null,queue:{pending:null,lanes:0,dispatch:null,lastRenderedReducer:Qt,lastRenderedState:j},next:null};var n={};return t.next={memoizedState:n,baseState:n,baseQueue:null,queue:{pending:null,lanes:0,dispatch:null,lastRenderedReducer:Qt,lastRenderedState:n},next:null},e.memoizedState=t,e=e.alternate,e!==null&&(e.memoizedState=t),t}function uu(e){var t=cu(e);t.next===null&&(t=e.alternate.memoizedState),ni(e,t.next.queue,{},gt())}function or(){return je(vi)}function du(){return Ae().memoizedState}function pu(){return Ae().memoizedState}function Rg(e){for(var t=e.return;t!==null;){switch(t.tag){case 24:case 3:var n=gt();e=cn(n);var o=un(t,e,n);o!==null&&($e(o,t,n),Zo(o,t,n)),t={cache:Ds()},e.payload=t;return}t=t.return}}function Og(e,t,n){var o=gt();n={lane:o,revertLane:0,gesture:null,action:n,hasEagerState:!1,eagerState:null,next:null},la(e)?hu(t,n):(n=Cs(e,t,n,o),n!==null&&($e(n,e,o),mu(n,t,o)))}function gu(e,t,n){var o=gt();ni(e,t,n,o)}function ni(e,t,n,o){var i={lane:o,revertLane:0,gesture:null,action:n,hasEagerState:!1,eagerState:null,next:null};if(la(e))hu(t,i);else{var a=e.alternate;if(e.lanes===0&&(a===null||a.lanes===0)&&(a=t.lastRenderedReducer,a!==null))try{var s=t.lastRenderedState,r=a(s,n);if(i.hasEagerState=!0,i.eagerState=r,rt(r,s))return Ri(e,t,i,0),pe===null&&Ni(),!1}catch{}finally{}if(n=Cs(e,t,i,o),n!==null)return $e(n,e,o),mu(n,t,o),!0}return!1}function ir(e,t,n,o){if(o={lane:2,revertLane:Ur(),gesture:null,action:o,hasEagerState:!1,eagerState:null,next:null},la(e)){if(t)throw Error(m(479))}else t=Cs(e,n,o,2),t!==null&&$e(t,e,2)}function la(e){var t=e.alternate;return e===L||t!==null&&t===L}function hu(e,t){fo=ta=!0;var n=e.pending;n===null?t.next=t:(t.next=n.next,n.next=t),e.pending=t}function mu(e,t,n){if((n&4194048)!==0){var o=t.lanes;o&=e.pendingLanes,n|=o,t.lanes=n,wl(e,n)}}var oi={readContext:je,use:ia,useCallback:we,useContext:we,useEffect:we,useImperativeHandle:we,useLayoutEffect:we,useInsertionEffect:we,useMemo:we,useReducer:we,useRef:we,useState:we,useDebugValue:we,useDeferredValue:we,useTransition:we,useSyncExternalStore:we,useId:we,useHostTransitionStatus:we,useFormState:we,useActionState:we,useOptimistic:we,useMemoCache:we,useCacheRefresh:we};oi.useEffectEvent=we;var fu={readContext:je,use:ia,useCallback:function(e,t){return We().memoizedState=[e,t===void 0?null:t],e},useContext:je,useEffect:$c,useImperativeHandle:function(e,t,n){n=n!=null?n.concat([e]):null,sa(4194308,4,ou.bind(null,t,e),n)},useLayoutEffect:function(e,t){return sa(4194308,4,e,t)},useInsertionEffect:function(e,t){sa(4,2,e,t)},useMemo:function(e,t){var n=We();t=t===void 0?null:t;var o=e();if(Rn){en(!0);try{e()}finally{en(!1)}}return n.memoizedState=[o,t],o},useReducer:function(e,t,n){var o=We();if(n!==void 0){var i=n(t);if(Rn){en(!0);try{n(t)}finally{en(!1)}}}else i=t;return o.memoizedState=o.baseState=i,e={pending:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:i},o.queue=e,e=e.dispatch=Og.bind(null,L,e),[o.memoizedState,e]},useRef:function(e){var t=We();return e={current:e},t.memoizedState=e},useState:function(e){e=Zs(e);var t=e.queue,n=gu.bind(null,L,t);return t.dispatch=n,[e.memoizedState,n]},useDebugValue:er,useDeferredValue:function(e,t){var n=We();return tr(n,e,t)},useTransition:function(){var e=Zs(!1);return e=lu.bind(null,L,e.queue,!0,!1),We().memoizedState=e,[!1,e]},useSyncExternalStore:function(e,t,n){var o=L,i=We();if(X){if(n===void 0)throw Error(m(407));n=n()}else{if(n=t(),pe===null)throw Error(m(349));(K&127)!==0||jc(o,t,n)}i.memoizedState=n;var a={value:n,getSnapshot:t};return i.queue=a,$c(Lc.bind(null,o,a,e),[e]),o.flags|=2048,bo(9,{destroy:void 0},Yc.bind(null,o,a,n,t),null),n},useId:function(){var e=We(),t=pe.identifierPrefix;if(X){var n=qt,o=Pt;n=(o&~(1<<32-st(o)-1)).toString(32)+n,t="_"+t+"R_"+n,n=na++,0<n&&(t+="H"+n.toString(32)),t+="_"}else n=Ug++,t="_"+t+"r_"+n.toString(32)+"_";return e.memoizedState=t},useHostTransitionStatus:or,useFormState:Kc,useActionState:Kc,useOptimistic:function(e){var t=We();t.memoizedState=t.baseState=e;var n={pending:null,lanes:0,dispatch:null,lastRenderedReducer:null,lastRenderedState:null};return t.queue=n,t=ir.bind(null,L,!0,n),n.dispatch=t,[e,t]},useMemoCache:Ks,useCacheRefresh:function(){return We().memoizedState=Rg.bind(null,L)},useEffectEvent:function(e){var t=We(),n={impl:e};return t.memoizedState=n,function(){if((ie&2)!==0)throw Error(m(440));return n.impl.apply(void 0,arguments)}}},ar={readContext:je,use:ia,useCallback:au,useContext:je,useEffect:$s,useImperativeHandle:iu,useInsertionEffect:tu,useLayoutEffect:nu,useMemo:su,useReducer:aa,useRef:Jc,useState:function(){return aa(Qt)},useDebugValue:er,useDeferredValue:function(e,t){var n=Ae();return ru(n,ce.memoizedState,e,t)},useTransition:function(){var e=aa(Qt)[0],t=Ae().memoizedState;return[typeof e=="boolean"?e:ti(e),t]},useSyncExternalStore:Uc,useId:du,useHostTransitionStatus:or,useFormState:Fc,useActionState:Fc,useOptimistic:function(e,t){var n=Ae();return Rc(n,ce,e,t)},useMemoCache:Ks,useCacheRefresh:pu};ar.useEffectEvent=eu;var yu={readContext:je,use:ia,useCallback:au,useContext:je,useEffect:$s,useImperativeHandle:iu,useInsertionEffect:tu,useLayoutEffect:nu,useMemo:su,useReducer:Xs,useRef:Jc,useState:function(){return Xs(Qt)},useDebugValue:er,useDeferredValue:function(e,t){var n=Ae();return ce===null?tr(n,e,t):ru(n,ce.memoizedState,e,t)},useTransition:function(){var e=Xs(Qt)[0],t=Ae().memoizedState;return[typeof e=="boolean"?e:ti(e),t]},useSyncExternalStore:Uc,useId:du,useHostTransitionStatus:or,useFormState:Zc,useActionState:Zc,useOptimistic:function(e,t){var n=Ae();return ce!==null?Rc(n,ce,e,t):(n.baseState=e,[e,n.queue.dispatch])},useMemoCache:Ks,useCacheRefresh:pu};yu.useEffectEvent=eu;function sr(e,t,n,o){t=e.memoizedState,n=n(o,t),n=n==null?t:V({},t,n),e.memoizedState=n,e.lanes===0&&(e.updateQueue.baseState=n)}var rr={enqueueSetState:function(e,t,n){e=e._reactInternals;var o=gt(),i=cn(o);i.payload=t,n!=null&&(i.callback=n),t=un(e,i,o),t!==null&&($e(t,e,o),Zo(t,e,o))},enqueueReplaceState:function(e,t,n){e=e._reactInternals;var o=gt(),i=cn(o);i.tag=1,i.payload=t,n!=null&&(i.callback=n),t=un(e,i,o),t!==null&&($e(t,e,o),Zo(t,e,o))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var n=gt(),o=cn(n);o.tag=2,t!=null&&(o.callback=t),t=un(e,o,n),t!==null&&($e(t,e,n),Zo(t,e,n))}};function bu(e,t,n,o,i,a,s){return e=e.stateNode,typeof e.shouldComponentUpdate=="function"?e.shouldComponentUpdate(o,a,s):t.prototype&&t.prototype.isPureReactComponent?!Oo(n,o)||!Oo(i,a):!0}function vu(e,t,n,o){e=t.state,typeof t.componentWillReceiveProps=="function"&&t.componentWillReceiveProps(n,o),typeof t.UNSAFE_componentWillReceiveProps=="function"&&t.UNSAFE_componentWillReceiveProps(n,o),t.state!==e&&rr.enqueueReplaceState(t,t.state,null)}function On(e,t){var n=t;if("ref"in t){n={};for(var o in t)o!=="ref"&&(n[o]=t[o])}if(e=e.defaultProps){n===t&&(n=V({},n));for(var i in e)n[i]===void 0&&(n[i]=e[i])}return n}function wu(e){Bi(e)}function Cu(e){console.error(e)}function Su(e){Bi(e)}function ca(e,t){try{var n=e.onUncaughtError;n(t.value,{componentStack:t.stack})}catch(o){setTimeout(function(){throw o})}}function xu(e,t,n){try{var o=e.onCaughtError;o(n.value,{componentStack:n.stack,errorBoundary:t.tag===1?t.stateNode:null})}catch(i){setTimeout(function(){throw i})}}function lr(e,t,n){return n=cn(n),n.tag=3,n.payload={element:null},n.callback=function(){ca(e,t)},n}function Au(e){return e=cn(e),e.tag=3,e}function ku(e,t,n,o){var i=n.type.getDerivedStateFromError;if(typeof i=="function"){var a=o.value;e.payload=function(){return i(a)},e.callback=function(){xu(t,n,o)}}var s=n.stateNode;s!==null&&typeof s.componentDidCatch=="function"&&(e.callback=function(){xu(t,n,o),typeof i!="function"&&(fn===null?fn=new Set([this]):fn.add(this));var r=o.stack;this.componentDidCatch(o.value,{componentStack:r!==null?r:""})})}function Qg(e,t,n,o,i){if(n.flags|=32768,o!==null&&typeof o=="object"&&typeof o.then=="function"){if(t=n.alternate,t!==null&&co(t,n,i,!0),n=ct.current,n!==null){switch(n.tag){case 31:case 13:return St===null?Ca():n.alternate===null&&Ce===0&&(Ce=3),n.flags&=-257,n.flags|=65536,n.lanes=i,o===Xi?n.flags|=16384:(t=n.updateQueue,t===null?n.updateQueue=new Set([o]):t.add(o),Ir(e,o,i)),!1;case 22:return n.flags|=65536,o===Xi?n.flags|=16384:(t=n.updateQueue,t===null?(t={transitions:null,markerInstances:null,retryQueue:new Set([o])},n.updateQueue=t):(n=t.retryQueue,n===null?t.retryQueue=new Set([o]):n.add(o)),Ir(e,o,i)),!1}throw Error(m(435,n.tag))}return Ir(e,o,i),Ca(),!1}if(X)return t=ct.current,t!==null?((t.flags&65536)===0&&(t.flags|=256),t.flags|=65536,t.lanes=i,o!==Ts&&(e=Error(m(422),{cause:o}),Wo(bt(e,n)))):(o!==Ts&&(t=Error(m(423),{cause:o}),Wo(bt(t,n))),e=e.current.alternate,e.flags|=65536,i&=-i,e.lanes|=i,o=bt(o,n),i=lr(e.stateNode,o,i),Ys(e,i),Ce!==4&&(Ce=2)),!1;var a=Error(m(520),{cause:o});if(a=bt(a,n),di===null?di=[a]:di.push(a),Ce!==4&&(Ce=2),t===null)return!0;o=bt(o,n),n=t;do{switch(n.tag){case 3:return n.flags|=65536,e=i&-i,n.lanes|=e,e=lr(n.stateNode,o,e),Ys(n,e),!1;case 1:if(t=n.type,a=n.stateNode,(n.flags&128)===0&&(typeof t.getDerivedStateFromError=="function"||a!==null&&typeof a.componentDidCatch=="function"&&(fn===null||!fn.has(a))))return n.flags|=65536,i&=-i,n.lanes|=i,i=Au(i),ku(i,e,n,o),Ys(n,i),!1}n=n.return}while(n!==null);return!1}var cr=Error(m(461)),Te=!1;function Ye(e,t,n,o){t.child=e===null?Pc(t,null,n,o):Nn(t,e.child,n,o)}function Eu(e,t,n,o,i){n=n.render;var a=t.ref;if("ref"in o){var s={};for(var r in o)r!=="ref"&&(s[r]=o[r])}else s=o;return jn(t),o=Qs(e,t,n,s,a,i),r=Hs(),e!==null&&!Te?(Ws(e,t,i),Ht(e,t,i)):(X&&r&&ks(t),t.flags|=1,Ye(e,t,o,i),t.child)}function Tu(e,t,n,o,i){if(e===null){var a=n.type;return typeof a=="function"&&!Ss(a)&&a.defaultProps===void 0&&n.compare===null?(t.tag=15,t.type=a,Mu(e,t,a,o,i)):(e=Qi(n.type,null,o,t,t.mode,i),e.ref=t.ref,e.return=t,t.child=e)}if(a=e.child,!yr(e,i)){var s=a.memoizedProps;if(n=n.compare,n=n!==null?n:Oo,n(s,o)&&e.ref===t.ref)return Ht(e,t,i)}return t.flags|=1,e=Lt(a,o),e.ref=t.ref,e.return=t,t.child=e}function Mu(e,t,n,o,i){if(e!==null){var a=e.memoizedProps;if(Oo(a,o)&&e.ref===t.ref)if(Te=!1,t.pendingProps=o=a,yr(e,i))(e.flags&131072)!==0&&(Te=!0);else return t.lanes=e.lanes,Ht(e,t,i)}return ur(e,t,n,o,i)}function Pu(e,t,n,o){var i=o.children,a=e!==null?e.memoizedState:null;if(e===null&&t.stateNode===null&&(t.stateNode={_visibility:1,_pendingMarkers:null,_retryCache:null,_transitions:null}),o.mode==="hidden"){if((t.flags&128)!==0){if(a=a!==null?a.baseLanes|n:n,e!==null){for(o=t.child=e.child,i=0;o!==null;)i=i|o.lanes|o.childLanes,o=o.sibling;o=i&~a}else o=0,t.child=null;return qu(e,t,a,n,o)}if((n&536870912)!==0)t.memoizedState={baseLanes:0,cachePool:null},e!==null&&Ki(t,a!==null?a.cachePool:null),a!==null?Dc(t,a):Bs(),Ic(t);else return o=t.lanes=536870912,qu(e,t,a!==null?a.baseLanes|n:n,n,o)}else a!==null?(Ki(t,a.cachePool),Dc(t,a),pn(),t.memoizedState=null):(e!==null&&Ki(t,null),Bs(),pn());return Ye(e,t,i,n),t.child}function ii(e,t){return e!==null&&e.tag===22||t.stateNode!==null||(t.stateNode={_visibility:1,_pendingMarkers:null,_retryCache:null,_transitions:null}),t.sibling}function qu(e,t,n,o,i){var a=Gs();return a=a===null?null:{parent:ke._currentValue,pool:a},t.memoizedState={baseLanes:n,cachePool:a},e!==null&&Ki(t,null),Bs(),Ic(t),e!==null&&co(e,t,o,!0),t.childLanes=i,null}function ua(e,t){return t=pa({mode:t.mode,children:t.children},e.mode),t.ref=e.ref,e.child=t,t.return=e,t}function zu(e,t,n){return Nn(t,e.child,null,n),e=ua(t,t.pendingProps),e.flags|=2,ut(t),t.memoizedState=null,e}function Hg(e,t,n){var o=t.pendingProps,i=(t.flags&128)!==0;if(t.flags&=-129,e===null){if(X){if(o.mode==="hidden")return e=ua(t,o),t.lanes=536870912,ii(null,e);if(Rs(t),(e=he)?(e=Od(e,Ct),e=e!==null&&e.data==="&"?e:null,e!==null&&(t.memoizedState={dehydrated:e,treeContext:on!==null?{id:Pt,overflow:qt}:null,retryLane:536870912,hydrationErrors:null},n=hc(e),n.return=t,t.child=n,Ue=t,he=null)):e=null,e===null)throw sn(t);return t.lanes=536870912,null}return ua(t,o)}var a=e.memoizedState;if(a!==null){var s=a.dehydrated;if(Rs(t),i)if(t.flags&256)t.flags&=-257,t=zu(e,t,n);else if(t.memoizedState!==null)t.child=e.child,t.flags|=128,t=null;else throw Error(m(558));else if(Te||co(e,t,n,!1),i=(n&e.childLanes)!==0,Te||i){if(o=pe,o!==null&&(s=Cl(o,n),s!==0&&s!==a.retryLane))throw a.retryLane=s,In(e,s),$e(o,e,s),cr;Ca(),t=zu(e,t,n)}else e=a.treeContext,he=xt(s.nextSibling),Ue=t,X=!0,an=null,Ct=!1,e!==null&&yc(t,e),t=ua(t,o),t.flags|=4096;return t}return e=Lt(e.child,{mode:o.mode,children:o.children}),e.ref=t.ref,t.child=e,e.return=t,e}function da(e,t){var n=t.ref;if(n===null)e!==null&&e.ref!==null&&(t.flags|=4194816);else{if(typeof n!="function"&&typeof n!="object")throw Error(m(284));(e===null||e.ref!==n)&&(t.flags|=4194816)}}function ur(e,t,n,o,i){return jn(t),n=Qs(e,t,n,o,void 0,i),o=Hs(),e!==null&&!Te?(Ws(e,t,i),Ht(e,t,i)):(X&&o&&ks(t),t.flags|=1,Ye(e,t,n,i),t.child)}function Du(e,t,n,o,i,a){return jn(t),t.updateQueue=null,n=Vc(t,o,n,i),Gc(e),o=Hs(),e!==null&&!Te?(Ws(e,t,a),Ht(e,t,a)):(X&&o&&ks(t),t.flags|=1,Ye(e,t,n,a),t.child)}function Iu(e,t,n,o,i){if(jn(t),t.stateNode===null){var a=ao,s=n.contextType;typeof s=="object"&&s!==null&&(a=je(s)),a=new n(o,a),t.memoizedState=a.state!==null&&a.state!==void 0?a.state:null,a.updater=rr,t.stateNode=a,a._reactInternals=t,a=t.stateNode,a.props=o,a.state=t.memoizedState,a.refs={},Us(t),s=n.contextType,a.context=typeof s=="object"&&s!==null?je(s):ao,a.state=t.memoizedState,s=n.getDerivedStateFromProps,typeof s=="function"&&(sr(t,n,s,o),a.state=t.memoizedState),typeof n.getDerivedStateFromProps=="function"||typeof a.getSnapshotBeforeUpdate=="function"||typeof a.UNSAFE_componentWillMount!="function"&&typeof a.componentWillMount!="function"||(s=a.state,typeof a.componentWillMount=="function"&&a.componentWillMount(),typeof a.UNSAFE_componentWillMount=="function"&&a.UNSAFE_componentWillMount(),s!==a.state&&rr.enqueueReplaceState(a,a.state,null),$o(t,o,a,i),Jo(),a.state=t.memoizedState),typeof a.componentDidMount=="function"&&(t.flags|=4194308),o=!0}else if(e===null){a=t.stateNode;var r=t.memoizedProps,c=On(n,r);a.props=c;var g=a.context,b=n.contextType;s=ao,typeof b=="object"&&b!==null&&(s=je(b));var C=n.getDerivedStateFromProps;b=typeof C=="function"||typeof a.getSnapshotBeforeUpdate=="function",r=t.pendingProps!==r,b||typeof a.UNSAFE_componentWillReceiveProps!="function"&&typeof a.componentWillReceiveProps!="function"||(r||g!==s)&&vu(t,a,o,s),ln=!1;var h=t.memoizedState;a.state=h,$o(t,o,a,i),Jo(),g=t.memoizedState,r||h!==g||ln?(typeof C=="function"&&(sr(t,n,C,o),g=t.memoizedState),(c=ln||bu(t,n,c,o,h,g,s))?(b||typeof a.UNSAFE_componentWillMount!="function"&&typeof a.componentWillMount!="function"||(typeof a.componentWillMount=="function"&&a.componentWillMount(),typeof a.UNSAFE_componentWillMount=="function"&&a.UNSAFE_componentWillMount()),typeof a.componentDidMount=="function"&&(t.flags|=4194308)):(typeof a.componentDidMount=="function"&&(t.flags|=4194308),t.memoizedProps=o,t.memoizedState=g),a.props=o,a.state=g,a.context=s,o=c):(typeof a.componentDidMount=="function"&&(t.flags|=4194308),o=!1)}else{a=t.stateNode,js(e,t),s=t.memoizedProps,b=On(n,s),a.props=b,C=t.pendingProps,h=a.context,g=n.contextType,c=ao,typeof g=="object"&&g!==null&&(c=je(g)),r=n.getDerivedStateFromProps,(g=typeof r=="function"||typeof a.getSnapshotBeforeUpdate=="function")||typeof a.UNSAFE_componentWillReceiveProps!="function"&&typeof a.componentWillReceiveProps!="function"||(s!==C||h!==c)&&vu(t,a,o,c),ln=!1,h=t.memoizedState,a.state=h,$o(t,o,a,i),Jo();var y=t.memoizedState;s!==C||h!==y||ln||e!==null&&e.dependencies!==null&&Wi(e.dependencies)?(typeof r=="function"&&(sr(t,n,r,o),y=t.memoizedState),(b=ln||bu(t,n,b,o,h,y,c)||e!==null&&e.dependencies!==null&&Wi(e.dependencies))?(g||typeof a.UNSAFE_componentWillUpdate!="function"&&typeof a.componentWillUpdate!="function"||(typeof a.componentWillUpdate=="function"&&a.componentWillUpdate(o,y,c),typeof a.UNSAFE_componentWillUpdate=="function"&&a.UNSAFE_componentWillUpdate(o,y,c)),typeof a.componentDidUpdate=="function"&&(t.flags|=4),typeof a.getSnapshotBeforeUpdate=="function"&&(t.flags|=1024)):(typeof a.componentDidUpdate!="function"||s===e.memoizedProps&&h===e.memoizedState||(t.flags|=4),typeof a.getSnapshotBeforeUpdate!="function"||s===e.memoizedProps&&h===e.memoizedState||(t.flags|=1024),t.memoizedProps=o,t.memoizedState=y),a.props=o,a.state=y,a.context=c,o=b):(typeof a.componentDidUpdate!="function"||s===e.memoizedProps&&h===e.memoizedState||(t.flags|=4),typeof a.getSnapshotBeforeUpdate!="function"||s===e.memoizedProps&&h===e.memoizedState||(t.flags|=1024),o=!1)}return a=o,da(e,t),o=(t.flags&128)!==0,a||o?(a=t.stateNode,n=o&&typeof n.getDerivedStateFromError!="function"?null:a.render(),t.flags|=1,e!==null&&o?(t.child=Nn(t,e.child,null,i),t.child=Nn(t,null,n,i)):Ye(e,t,n,i),t.memoizedState=a.state,e=t.child):e=Ht(e,t,i),e}function Gu(e,t,n,o){return Vn(),t.flags|=256,Ye(e,t,n,o),t.child}var dr={dehydrated:null,treeContext:null,retryLane:0,hydrationErrors:null};function pr(e){return{baseLanes:e,cachePool:xc()}}function gr(e,t,n){return e=e!==null?e.childLanes&~n:0,t&&(e|=pt),e}function Vu(e,t,n){var o=t.pendingProps,i=!1,a=(t.flags&128)!==0,s;if((s=a)||(s=e!==null&&e.memoizedState===null?!1:(xe.current&2)!==0),s&&(i=!0,t.flags&=-129),s=(t.flags&32)!==0,t.flags&=-33,e===null){if(X){if(i?dn(t):pn(),(e=he)?(e=Od(e,Ct),e=e!==null&&e.data!=="&"?e:null,e!==null&&(t.memoizedState={dehydrated:e,treeContext:on!==null?{id:Pt,overflow:qt}:null,retryLane:536870912,hydrationErrors:null},n=hc(e),n.return=t,t.child=n,Ue=t,he=null)):e=null,e===null)throw sn(t);return Fr(e)?t.lanes=32:t.lanes=536870912,null}var r=o.children;return o=o.fallback,i?(pn(),i=t.mode,r=pa({mode:"hidden",children:r},i),o=Gn(o,i,n,null),r.return=t,o.return=t,r.sibling=o,t.child=r,o=t.child,o.memoizedState=pr(n),o.childLanes=gr(e,s,n),t.memoizedState=dr,ii(null,o)):(dn(t),hr(t,r))}var c=e.memoizedState;if(c!==null&&(r=c.dehydrated,r!==null)){if(a)t.flags&256?(dn(t),t.flags&=-257,t=mr(e,t,n)):t.memoizedState!==null?(pn(),t.child=e.child,t.flags|=128,t=null):(pn(),r=o.fallback,i=t.mode,o=pa({mode:"visible",children:o.children},i),r=Gn(r,i,n,null),r.flags|=2,o.return=t,r.return=t,o.sibling=r,t.child=o,Nn(t,e.child,null,n),o=t.child,o.memoizedState=pr(n),o.childLanes=gr(e,s,n),t.memoizedState=dr,t=ii(null,o));else if(dn(t),Fr(r)){if(s=r.nextSibling&&r.nextSibling.dataset,s)var g=s.dgst;s=g,o=Error(m(419)),o.stack="",o.digest=s,Wo({value:o,source:null,stack:null}),t=mr(e,t,n)}else if(Te||co(e,t,n,!1),s=(n&e.childLanes)!==0,Te||s){if(s=pe,s!==null&&(o=Cl(s,n),o!==0&&o!==c.retryLane))throw c.retryLane=o,In(e,o),$e(s,e,o),cr;Kr(r)||Ca(),t=mr(e,t,n)}else Kr(r)?(t.flags|=192,t.child=e.child,t=null):(e=c.treeContext,he=xt(r.nextSibling),Ue=t,X=!0,an=null,Ct=!1,e!==null&&yc(t,e),t=hr(t,o.children),t.flags|=4096);return t}return i?(pn(),r=o.fallback,i=t.mode,c=e.child,g=c.sibling,o=Lt(c,{mode:"hidden",children:o.children}),o.subtreeFlags=c.subtreeFlags&65011712,g!==null?r=Lt(g,r):(r=Gn(r,i,n,null),r.flags|=2),r.return=t,o.return=t,o.sibling=r,t.child=o,ii(null,o),o=t.child,r=e.child.memoizedState,r===null?r=pr(n):(i=r.cachePool,i!==null?(c=ke._currentValue,i=i.parent!==c?{parent:c,pool:c}:i):i=xc(),r={baseLanes:r.baseLanes|n,cachePool:i}),o.memoizedState=r,o.childLanes=gr(e,s,n),t.memoizedState=dr,ii(e.child,o)):(dn(t),n=e.child,e=n.sibling,n=Lt(n,{mode:"visible",children:o.children}),n.return=t,n.sibling=null,e!==null&&(s=t.deletions,s===null?(t.deletions=[e],t.flags|=16):s.push(e)),t.child=n,t.memoizedState=null,n)}function hr(e,t){return t=pa({mode:"visible",children:t},e.mode),t.return=e,e.child=t}function pa(e,t){return e=lt(22,e,null,t),e.lanes=0,e}function mr(e,t,n){return Nn(t,e.child,null,n),e=hr(t,t.pendingProps.children),e.flags|=2,t.memoizedState=null,e}function Uu(e,t,n){e.lanes|=t;var o=e.alternate;o!==null&&(o.lanes|=t),qs(e.return,t,n)}function fr(e,t,n,o,i,a){var s=e.memoizedState;s===null?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:o,tail:n,tailMode:i,treeForkCount:a}:(s.isBackwards=t,s.rendering=null,s.renderingStartTime=0,s.last=o,s.tail=n,s.tailMode=i,s.treeForkCount=a)}function ju(e,t,n){var o=t.pendingProps,i=o.revealOrder,a=o.tail;o=o.children;var s=xe.current,r=(s&2)!==0;if(r?(s=s&1|2,t.flags|=128):s&=1,S(xe,s),Ye(e,t,o,n),o=X?Ho:0,!r&&e!==null&&(e.flags&128)!==0)e:for(e=t.child;e!==null;){if(e.tag===13)e.memoizedState!==null&&Uu(e,n,t);else if(e.tag===19)Uu(e,n,t);else if(e.child!==null){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;e.sibling===null;){if(e.return===null||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}switch(i){case"forwards":for(n=t.child,i=null;n!==null;)e=n.alternate,e!==null&&ea(e)===null&&(i=n),n=n.sibling;n=i,n===null?(i=t.child,t.child=null):(i=n.sibling,n.sibling=null),fr(t,!1,i,n,a,o);break;case"backwards":case"unstable_legacy-backwards":for(n=null,i=t.child,t.child=null;i!==null;){if(e=i.alternate,e!==null&&ea(e)===null){t.child=i;break}e=i.sibling,i.sibling=n,n=i,i=e}fr(t,!0,n,null,a,o);break;case"together":fr(t,!1,null,null,void 0,o);break;default:t.memoizedState=null}return t.child}function Ht(e,t,n){if(e!==null&&(t.dependencies=e.dependencies),mn|=t.lanes,(n&t.childLanes)===0)if(e!==null){if(co(e,t,n,!1),(n&t.childLanes)===0)return null}else return null;if(e!==null&&t.child!==e.child)throw Error(m(153));if(t.child!==null){for(e=t.child,n=Lt(e,e.pendingProps),t.child=n,n.return=t;e.sibling!==null;)e=e.sibling,n=n.sibling=Lt(e,e.pendingProps),n.return=t;n.sibling=null}return t.child}function yr(e,t){return(e.lanes&t)!==0?!0:(e=e.dependencies,!!(e!==null&&Wi(e)))}function Wg(e,t,n){switch(t.tag){case 3:Pe(t,t.stateNode.containerInfo),rn(t,ke,e.memoizedState.cache),Vn();break;case 27:case 5:Gt(t);break;case 4:Pe(t,t.stateNode.containerInfo);break;case 10:rn(t,t.type,t.memoizedProps.value);break;case 31:if(t.memoizedState!==null)return t.flags|=128,Rs(t),null;break;case 13:var o=t.memoizedState;if(o!==null)return o.dehydrated!==null?(dn(t),t.flags|=128,null):(n&t.child.childLanes)!==0?Vu(e,t,n):(dn(t),e=Ht(e,t,n),e!==null?e.sibling:null);dn(t);break;case 19:var i=(e.flags&128)!==0;if(o=(n&t.childLanes)!==0,o||(co(e,t,n,!1),o=(n&t.childLanes)!==0),i){if(o)return ju(e,t,n);t.flags|=128}if(i=t.memoizedState,i!==null&&(i.rendering=null,i.tail=null,i.lastEffect=null),S(xe,xe.current),o)break;return null;case 22:return t.lanes=0,Pu(e,t,n,t.pendingProps);case 24:rn(t,ke,e.memoizedState.cache)}return Ht(e,t,n)}function Yu(e,t,n){if(e!==null)if(e.memoizedProps!==t.pendingProps)Te=!0;else{if(!yr(e,n)&&(t.flags&128)===0)return Te=!1,Wg(e,t,n);Te=(e.flags&131072)!==0}else Te=!1,X&&(t.flags&1048576)!==0&&fc(t,Ho,t.index);switch(t.lanes=0,t.tag){case 16:e:{var o=t.pendingProps;if(e=Ln(t.elementType),t.type=e,typeof e=="function")Ss(e)?(o=On(e,o),t.tag=1,t=Iu(null,t,e,o,n)):(t.tag=0,t=ur(null,t,e,o,n));else{if(e!=null){var i=e.$$typeof;if(i===Qe){t.tag=11,t=Eu(null,t,e,o,n);break e}else if(i===O){t.tag=14,t=Tu(null,t,e,o,n);break e}}throw t=ht(e)||e,Error(m(306,t,""))}}return t;case 0:return ur(e,t,t.type,t.pendingProps,n);case 1:return o=t.type,i=On(o,t.pendingProps),Iu(e,t,o,i,n);case 3:e:{if(Pe(t,t.stateNode.containerInfo),e===null)throw Error(m(387));o=t.pendingProps;var a=t.memoizedState;i=a.element,js(e,t),$o(t,o,null,n);var s=t.memoizedState;if(o=s.cache,rn(t,ke,o),o!==a.cache&&zs(t,[ke],n,!0),Jo(),o=s.element,a.isDehydrated)if(a={element:o,isDehydrated:!1,cache:s.cache},t.updateQueue.baseState=a,t.memoizedState=a,t.flags&256){t=Gu(e,t,o,n);break e}else if(o!==i){i=bt(Error(m(424)),t),Wo(i),t=Gu(e,t,o,n);break e}else{switch(e=t.stateNode.containerInfo,e.nodeType){case 9:e=e.body;break;default:e=e.nodeName==="HTML"?e.ownerDocument.body:e}for(he=xt(e.firstChild),Ue=t,X=!0,an=null,Ct=!0,n=Pc(t,null,o,n),t.child=n;n;)n.flags=n.flags&-3|4096,n=n.sibling}else{if(Vn(),o===i){t=Ht(e,t,n);break e}Ye(e,t,o,n)}t=t.child}return t;case 26:return da(e,t),e===null?(n=Fd(t.type,null,t.pendingProps,null))?t.memoizedState=n:X||(n=t.type,e=t.pendingProps,o=Ma(I.current).createElement(n),o[Ve]=t,o[_e]=e,Le(o,n,e),De(o),t.stateNode=o):t.memoizedState=Fd(t.type,e.memoizedProps,t.pendingProps,e.memoizedState),null;case 27:return Gt(t),e===null&&X&&(o=t.stateNode=Wd(t.type,t.pendingProps,I.current),Ue=t,Ct=!0,i=he,wn(t.type)?(Xr=i,he=xt(o.firstChild)):he=i),Ye(e,t,t.pendingProps.children,n),da(e,t),e===null&&(t.flags|=4194304),t.child;case 5:return e===null&&X&&((i=o=he)&&(o=Sh(o,t.type,t.pendingProps,Ct),o!==null?(t.stateNode=o,Ue=t,he=xt(o.firstChild),Ct=!1,i=!0):i=!1),i||sn(t)),Gt(t),i=t.type,a=t.pendingProps,s=e!==null?e.memoizedProps:null,o=a.children,Hr(i,a)?o=null:s!==null&&Hr(i,s)&&(t.flags|=32),t.memoizedState!==null&&(i=Qs(e,t,jg,null,null,n),vi._currentValue=i),da(e,t),Ye(e,t,o,n),t.child;case 6:return e===null&&X&&((e=n=he)&&(n=xh(n,t.pendingProps,Ct),n!==null?(t.stateNode=n,Ue=t,he=null,e=!0):e=!1),e||sn(t)),null;case 13:return Vu(e,t,n);case 4:return Pe(t,t.stateNode.containerInfo),o=t.pendingProps,e===null?t.child=Nn(t,null,o,n):Ye(e,t,o,n),t.child;case 11:return Eu(e,t,t.type,t.pendingProps,n);case 7:return Ye(e,t,t.pendingProps,n),t.child;case 8:return Ye(e,t,t.pendingProps.children,n),t.child;case 12:return Ye(e,t,t.pendingProps.children,n),t.child;case 10:return o=t.pendingProps,rn(t,t.type,o.value),Ye(e,t,o.children,n),t.child;case 9:return i=t.type._context,o=t.pendingProps.children,jn(t),i=je(i),o=o(i),t.flags|=1,Ye(e,t,o,n),t.child;case 14:return Tu(e,t,t.type,t.pendingProps,n);case 15:return Mu(e,t,t.type,t.pendingProps,n);case 19:return ju(e,t,n);case 31:return Hg(e,t,n);case 22:return Pu(e,t,n,t.pendingProps);case 24:return jn(t),o=je(ke),e===null?(i=Gs(),i===null&&(i=pe,a=Ds(),i.pooledCache=a,a.refCount++,a!==null&&(i.pooledCacheLanes|=n),i=a),t.memoizedState={parent:o,cache:i},Us(t),rn(t,ke,i)):((e.lanes&n)!==0&&(js(e,t),$o(t,null,null,n),Jo()),i=e.memoizedState,a=t.memoizedState,i.parent!==o?(i={parent:o,cache:o},t.memoizedState=i,t.lanes===0&&(t.memoizedState=t.updateQueue.baseState=i),rn(t,ke,o)):(o=a.cache,rn(t,ke,o),o!==i.cache&&zs(t,[ke],n,!0))),Ye(e,t,t.pendingProps.children,n),t.child;case 29:throw t.pendingProps}throw Error(m(156,t.tag))}function Wt(e){e.flags|=4}function br(e,t,n,o,i){if((t=(e.mode&32)!==0)&&(t=!1),t){if(e.flags|=16777216,(i&335544128)===i)if(e.stateNode.complete)e.flags|=8192;else if(dd())e.flags|=8192;else throw Bn=Xi,Vs}else e.flags&=-16777217}function Lu(e,t){if(t.type!=="stylesheet"||(t.state.loading&4)!==0)e.flags&=-16777217;else if(e.flags|=16777216,!ep(t))if(dd())e.flags|=8192;else throw Bn=Xi,Vs}function ga(e,t){t!==null&&(e.flags|=4),e.flags&16384&&(t=e.tag!==22?bl():536870912,e.lanes|=t,So|=t)}function ai(e,t){if(!X)switch(e.tailMode){case"hidden":t=e.tail;for(var n=null;t!==null;)t.alternate!==null&&(n=t),t=t.sibling;n===null?e.tail=null:n.sibling=null;break;case"collapsed":n=e.tail;for(var o=null;n!==null;)n.alternate!==null&&(o=n),n=n.sibling;o===null?t||e.tail===null?e.tail=null:e.tail.sibling=null:o.sibling=null}}function me(e){var t=e.alternate!==null&&e.alternate.child===e.child,n=0,o=0;if(t)for(var i=e.child;i!==null;)n|=i.lanes|i.childLanes,o|=i.subtreeFlags&65011712,o|=i.flags&65011712,i.return=e,i=i.sibling;else for(i=e.child;i!==null;)n|=i.lanes|i.childLanes,o|=i.subtreeFlags,o|=i.flags,i.return=e,i=i.sibling;return e.subtreeFlags|=o,e.childLanes=n,t}function _g(e,t,n){var o=t.pendingProps;switch(Es(t),t.tag){case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return me(t),null;case 1:return me(t),null;case 3:return n=t.stateNode,o=null,e!==null&&(o=e.memoizedState.cache),t.memoizedState.cache!==o&&(t.flags|=2048),Rt(ke),ge(),n.pendingContext&&(n.context=n.pendingContext,n.pendingContext=null),(e===null||e.child===null)&&(lo(t)?Wt(t):e===null||e.memoizedState.isDehydrated&&(t.flags&256)===0||(t.flags|=1024,Ms())),me(t),null;case 26:var i=t.type,a=t.memoizedState;return e===null?(Wt(t),a!==null?(me(t),Lu(t,a)):(me(t),br(t,i,null,o,n))):a?a!==e.memoizedState?(Wt(t),me(t),Lu(t,a)):(me(t),t.flags&=-16777217):(e=e.memoizedProps,e!==o&&Wt(t),me(t),br(t,i,e,o,n)),null;case 27:if(Vt(t),n=I.current,i=t.type,e!==null&&t.stateNode!=null)e.memoizedProps!==o&&Wt(t);else{if(!o){if(t.stateNode===null)throw Error(m(166));return me(t),null}e=k.current,lo(t)?bc(t):(e=Wd(i,o,n),t.stateNode=e,Wt(t))}return me(t),null;case 5:if(Vt(t),i=t.type,e!==null&&t.stateNode!=null)e.memoizedProps!==o&&Wt(t);else{if(!o){if(t.stateNode===null)throw Error(m(166));return me(t),null}if(a=k.current,lo(t))bc(t);else{var s=Ma(I.current);switch(a){case 1:a=s.createElementNS("http://www.w3.org/2000/svg",i);break;case 2:a=s.createElementNS("http://www.w3.org/1998/Math/MathML",i);break;default:switch(i){case"svg":a=s.createElementNS("http://www.w3.org/2000/svg",i);break;case"math":a=s.createElementNS("http://www.w3.org/1998/Math/MathML",i);break;case"script":a=s.createElement("div"),a.innerHTML="<script><\/script>",a=a.removeChild(a.firstChild);break;case"select":a=typeof o.is=="string"?s.createElement("select",{is:o.is}):s.createElement("select"),o.multiple?a.multiple=!0:o.size&&(a.size=o.size);break;default:a=typeof o.is=="string"?s.createElement(i,{is:o.is}):s.createElement(i)}}a[Ve]=t,a[_e]=o;e:for(s=t.child;s!==null;){if(s.tag===5||s.tag===6)a.appendChild(s.stateNode);else if(s.tag!==4&&s.tag!==27&&s.child!==null){s.child.return=s,s=s.child;continue}if(s===t)break e;for(;s.sibling===null;){if(s.return===null||s.return===t)break e;s=s.return}s.sibling.return=s.return,s=s.sibling}t.stateNode=a;e:switch(Le(a,i,o),i){case"button":case"input":case"select":case"textarea":o=!!o.autoFocus;break e;case"img":o=!0;break e;default:o=!1}o&&Wt(t)}}return me(t),br(t,t.type,e===null?null:e.memoizedProps,t.pendingProps,n),null;case 6:if(e&&t.stateNode!=null)e.memoizedProps!==o&&Wt(t);else{if(typeof o!="string"&&t.stateNode===null)throw Error(m(166));if(e=I.current,lo(t)){if(e=t.stateNode,n=t.memoizedProps,o=null,i=Ue,i!==null)switch(i.tag){case 27:case 5:o=i.memoizedProps}e[Ve]=t,e=!!(e.nodeValue===n||o!==null&&o.suppressHydrationWarning===!0||Vd(e.nodeValue,n)),e||sn(t,!0)}else e=Ma(e).createTextNode(o),e[Ve]=t,t.stateNode=e}return me(t),null;case 31:if(n=t.memoizedState,e===null||e.memoizedState!==null){if(o=lo(t),n!==null){if(e===null){if(!o)throw Error(m(318));if(e=t.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(m(557));e[Ve]=t}else Vn(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;me(t),e=!1}else n=Ms(),e!==null&&e.memoizedState!==null&&(e.memoizedState.hydrationErrors=n),e=!0;if(!e)return t.flags&256?(ut(t),t):(ut(t),null);if((t.flags&128)!==0)throw Error(m(558))}return me(t),null;case 13:if(o=t.memoizedState,e===null||e.memoizedState!==null&&e.memoizedState.dehydrated!==null){if(i=lo(t),o!==null&&o.dehydrated!==null){if(e===null){if(!i)throw Error(m(318));if(i=t.memoizedState,i=i!==null?i.dehydrated:null,!i)throw Error(m(317));i[Ve]=t}else Vn(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;me(t),i=!1}else i=Ms(),e!==null&&e.memoizedState!==null&&(e.memoizedState.hydrationErrors=i),i=!0;if(!i)return t.flags&256?(ut(t),t):(ut(t),null)}return ut(t),(t.flags&128)!==0?(t.lanes=n,t):(n=o!==null,e=e!==null&&e.memoizedState!==null,n&&(o=t.child,i=null,o.alternate!==null&&o.alternate.memoizedState!==null&&o.alternate.memoizedState.cachePool!==null&&(i=o.alternate.memoizedState.cachePool.pool),a=null,o.memoizedState!==null&&o.memoizedState.cachePool!==null&&(a=o.memoizedState.cachePool.pool),a!==i&&(o.flags|=2048)),n!==e&&n&&(t.child.flags|=8192),ga(t,t.updateQueue),me(t),null);case 4:return ge(),e===null&&Br(t.stateNode.containerInfo),me(t),null;case 10:return Rt(t.type),me(t),null;case 19:if(f(xe),o=t.memoizedState,o===null)return me(t),null;if(i=(t.flags&128)!==0,a=o.rendering,a===null)if(i)ai(o,!1);else{if(Ce!==0||e!==null&&(e.flags&128)!==0)for(e=t.child;e!==null;){if(a=ea(e),a!==null){for(t.flags|=128,ai(o,!1),e=a.updateQueue,t.updateQueue=e,ga(t,e),t.subtreeFlags=0,e=n,n=t.child;n!==null;)gc(n,e),n=n.sibling;return S(xe,xe.current&1|2),X&&Bt(t,o.treeForkCount),t.child}e=e.sibling}o.tail!==null&&it()>ba&&(t.flags|=128,i=!0,ai(o,!1),t.lanes=4194304)}else{if(!i)if(e=ea(a),e!==null){if(t.flags|=128,i=!0,e=e.updateQueue,t.updateQueue=e,ga(t,e),ai(o,!0),o.tail===null&&o.tailMode==="hidden"&&!a.alternate&&!X)return me(t),null}else 2*it()-o.renderingStartTime>ba&&n!==536870912&&(t.flags|=128,i=!0,ai(o,!1),t.lanes=4194304);o.isBackwards?(a.sibling=t.child,t.child=a):(e=o.last,e!==null?e.sibling=a:t.child=a,o.last=a)}return o.tail!==null?(e=o.tail,o.rendering=e,o.tail=e.sibling,o.renderingStartTime=it(),e.sibling=null,n=xe.current,S(xe,i?n&1|2:n&1),X&&Bt(t,o.treeForkCount),e):(me(t),null);case 22:case 23:return ut(t),Ns(),o=t.memoizedState!==null,e!==null?e.memoizedState!==null!==o&&(t.flags|=8192):o&&(t.flags|=8192),o?(n&536870912)!==0&&(t.flags&128)===0&&(me(t),t.subtreeFlags&6&&(t.flags|=8192)):me(t),n=t.updateQueue,n!==null&&ga(t,n.retryQueue),n=null,e!==null&&e.memoizedState!==null&&e.memoizedState.cachePool!==null&&(n=e.memoizedState.cachePool.pool),o=null,t.memoizedState!==null&&t.memoizedState.cachePool!==null&&(o=t.memoizedState.cachePool.pool),o!==n&&(t.flags|=2048),e!==null&&f(Yn),null;case 24:return n=null,e!==null&&(n=e.memoizedState.cache),t.memoizedState.cache!==n&&(t.flags|=2048),Rt(ke),me(t),null;case 25:return null;case 30:return null}throw Error(m(156,t.tag))}function Kg(e,t){switch(Es(t),t.tag){case 1:return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 3:return Rt(ke),ge(),e=t.flags,(e&65536)!==0&&(e&128)===0?(t.flags=e&-65537|128,t):null;case 26:case 27:case 5:return Vt(t),null;case 31:if(t.memoizedState!==null){if(ut(t),t.alternate===null)throw Error(m(340));Vn()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 13:if(ut(t),e=t.memoizedState,e!==null&&e.dehydrated!==null){if(t.alternate===null)throw Error(m(340));Vn()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 19:return f(xe),null;case 4:return ge(),null;case 10:return Rt(t.type),null;case 22:case 23:return ut(t),Ns(),e!==null&&f(Yn),e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 24:return Rt(ke),null;case 25:return null;default:return null}}function Bu(e,t){switch(Es(t),t.tag){case 3:Rt(ke),ge();break;case 26:case 27:case 5:Vt(t);break;case 4:ge();break;case 31:t.memoizedState!==null&&ut(t);break;case 13:ut(t);break;case 19:f(xe);break;case 10:Rt(t.type);break;case 22:case 23:ut(t),Ns(),e!==null&&f(Yn);break;case 24:Rt(ke)}}function si(e,t){try{var n=t.updateQueue,o=n!==null?n.lastEffect:null;if(o!==null){var i=o.next;n=i;do{if((n.tag&e)===e){o=void 0;var a=n.create,s=n.inst;o=a(),s.destroy=o}n=n.next}while(n!==i)}}catch(r){re(t,t.return,r)}}function gn(e,t,n){try{var o=t.updateQueue,i=o!==null?o.lastEffect:null;if(i!==null){var a=i.next;o=a;do{if((o.tag&e)===e){var s=o.inst,r=s.destroy;if(r!==void 0){s.destroy=void 0,i=t;var c=n,g=r;try{g()}catch(b){re(i,c,b)}}}o=o.next}while(o!==a)}}catch(b){re(t,t.return,b)}}function Nu(e){var t=e.updateQueue;if(t!==null){var n=e.stateNode;try{zc(t,n)}catch(o){re(e,e.return,o)}}}function Ru(e,t,n){n.props=On(e.type,e.memoizedProps),n.state=e.memoizedState;try{n.componentWillUnmount()}catch(o){re(e,t,o)}}function ri(e,t){try{var n=e.ref;if(n!==null){switch(e.tag){case 26:case 27:case 5:var o=e.stateNode;break;case 30:o=e.stateNode;break;default:o=e.stateNode}typeof n=="function"?e.refCleanup=n(o):n.current=o}}catch(i){re(e,t,i)}}function zt(e,t){var n=e.ref,o=e.refCleanup;if(n!==null)if(typeof o=="function")try{o()}catch(i){re(e,t,i)}finally{e.refCleanup=null,e=e.alternate,e!=null&&(e.refCleanup=null)}else if(typeof n=="function")try{n(null)}catch(i){re(e,t,i)}else n.current=null}function Ou(e){var t=e.type,n=e.memoizedProps,o=e.stateNode;try{e:switch(t){case"button":case"input":case"select":case"textarea":n.autoFocus&&o.focus();break e;case"img":n.src?o.src=n.src:n.srcSet&&(o.srcset=n.srcSet)}}catch(i){re(e,e.return,i)}}function vr(e,t,n){try{var o=e.stateNode;fh(o,e.type,n,t),o[_e]=t}catch(i){re(e,e.return,i)}}function Qu(e){return e.tag===5||e.tag===3||e.tag===26||e.tag===27&&wn(e.type)||e.tag===4}function wr(e){e:for(;;){for(;e.sibling===null;){if(e.return===null||Qu(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;e.tag!==5&&e.tag!==6&&e.tag!==18;){if(e.tag===27&&wn(e.type)||e.flags&2||e.child===null||e.tag===4)continue e;e.child.return=e,e=e.child}if(!(e.flags&2))return e.stateNode}}function Cr(e,t,n){var o=e.tag;if(o===5||o===6)e=e.stateNode,t?(n.nodeType===9?n.body:n.nodeName==="HTML"?n.ownerDocument.body:n).insertBefore(e,t):(t=n.nodeType===9?n.body:n.nodeName==="HTML"?n.ownerDocument.body:n,t.appendChild(e),n=n._reactRootContainer,n!=null||t.onclick!==null||(t.onclick=jt));else if(o!==4&&(o===27&&wn(e.type)&&(n=e.stateNode,t=null),e=e.child,e!==null))for(Cr(e,t,n),e=e.sibling;e!==null;)Cr(e,t,n),e=e.sibling}function ha(e,t,n){var o=e.tag;if(o===5||o===6)e=e.stateNode,t?n.insertBefore(e,t):n.appendChild(e);else if(o!==4&&(o===27&&wn(e.type)&&(n=e.stateNode),e=e.child,e!==null))for(ha(e,t,n),e=e.sibling;e!==null;)ha(e,t,n),e=e.sibling}function Hu(e){var t=e.stateNode,n=e.memoizedProps;try{for(var o=e.type,i=t.attributes;i.length;)t.removeAttributeNode(i[0]);Le(t,o,n),t[Ve]=e,t[_e]=n}catch(a){re(e,e.return,a)}}var _t=!1,Me=!1,Sr=!1,Wu=typeof WeakSet=="function"?WeakSet:Set,Ie=null;function Fg(e,t){if(e=e.containerInfo,Or=Va,e=ic(e),ms(e)){if("selectionStart"in e)var n={start:e.selectionStart,end:e.selectionEnd};else e:{n=(n=e.ownerDocument)&&n.defaultView||window;var o=n.getSelection&&n.getSelection();if(o&&o.rangeCount!==0){n=o.anchorNode;var i=o.anchorOffset,a=o.focusNode;o=o.focusOffset;try{n.nodeType,a.nodeType}catch{n=null;break e}var s=0,r=-1,c=-1,g=0,b=0,C=e,h=null;t:for(;;){for(var y;C!==n||i!==0&&C.nodeType!==3||(r=s+i),C!==a||o!==0&&C.nodeType!==3||(c=s+o),C.nodeType===3&&(s+=C.nodeValue.length),(y=C.firstChild)!==null;)h=C,C=y;for(;;){if(C===e)break t;if(h===n&&++g===i&&(r=s),h===a&&++b===o&&(c=s),(y=C.nextSibling)!==null)break;C=h,h=C.parentNode}C=y}n=r===-1||c===-1?null:{start:r,end:c}}else n=null}n=n||{start:0,end:0}}else n=null;for(Qr={focusedElem:e,selectionRange:n},Va=!1,Ie=t;Ie!==null;)if(t=Ie,e=t.child,(t.subtreeFlags&1028)!==0&&e!==null)e.return=t,Ie=e;else for(;Ie!==null;){switch(t=Ie,a=t.alternate,e=t.flags,t.tag){case 0:if((e&4)!==0&&(e=t.updateQueue,e=e!==null?e.events:null,e!==null))for(n=0;n<e.length;n++)i=e[n],i.ref.impl=i.nextImpl;break;case 11:case 15:break;case 1:if((e&1024)!==0&&a!==null){e=void 0,n=t,i=a.memoizedProps,a=a.memoizedState,o=n.stateNode;try{var M=On(n.type,i);e=o.getSnapshotBeforeUpdate(M,a),o.__reactInternalSnapshotBeforeUpdate=e}catch(U){re(n,n.return,U)}}break;case 3:if((e&1024)!==0){if(e=t.stateNode.containerInfo,n=e.nodeType,n===9)_r(e);else if(n===1)switch(e.nodeName){case"HEAD":case"HTML":case"BODY":_r(e);break;default:e.textContent=""}}break;case 5:case 26:case 27:case 6:case 4:case 17:break;default:if((e&1024)!==0)throw Error(m(163))}if(e=t.sibling,e!==null){e.return=t.return,Ie=e;break}Ie=t.return}}function _u(e,t,n){var o=n.flags;switch(n.tag){case 0:case 11:case 15:Ft(e,n),o&4&&si(5,n);break;case 1:if(Ft(e,n),o&4)if(e=n.stateNode,t===null)try{e.componentDidMount()}catch(s){re(n,n.return,s)}else{var i=On(n.type,t.memoizedProps);t=t.memoizedState;try{e.componentDidUpdate(i,t,e.__reactInternalSnapshotBeforeUpdate)}catch(s){re(n,n.return,s)}}o&64&&Nu(n),o&512&&ri(n,n.return);break;case 3:if(Ft(e,n),o&64&&(e=n.updateQueue,e!==null)){if(t=null,n.child!==null)switch(n.child.tag){case 27:case 5:t=n.child.stateNode;break;case 1:t=n.child.stateNode}try{zc(e,t)}catch(s){re(n,n.return,s)}}break;case 27:t===null&&o&4&&Hu(n);case 26:case 5:Ft(e,n),t===null&&o&4&&Ou(n),o&512&&ri(n,n.return);break;case 12:Ft(e,n);break;case 31:Ft(e,n),o&4&&Xu(e,n);break;case 13:Ft(e,n),o&4&&Zu(e,n),o&64&&(e=n.memoizedState,e!==null&&(e=e.dehydrated,e!==null&&(n=ih.bind(null,n),Ah(e,n))));break;case 22:if(o=n.memoizedState!==null||_t,!o){t=t!==null&&t.memoizedState!==null||Me,i=_t;var a=Me;_t=o,(Me=t)&&!a?Xt(e,n,(n.subtreeFlags&8772)!==0):Ft(e,n),_t=i,Me=a}break;case 30:break;default:Ft(e,n)}}function Ku(e){var t=e.alternate;t!==null&&(e.alternate=null,Ku(t)),e.child=null,e.deletions=null,e.sibling=null,e.tag===5&&(t=e.stateNode,t!==null&&Ja(t)),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}var ye=null,Fe=!1;function Kt(e,t,n){for(n=n.child;n!==null;)Fu(e,t,n),n=n.sibling}function Fu(e,t,n){if(at&&typeof at.onCommitFiberUnmount=="function")try{at.onCommitFiberUnmount(zo,n)}catch{}switch(n.tag){case 26:Me||zt(n,t),Kt(e,t,n),n.memoizedState?n.memoizedState.count--:n.stateNode&&(n=n.stateNode,n.parentNode.removeChild(n));break;case 27:Me||zt(n,t);var o=ye,i=Fe;wn(n.type)&&(ye=n.stateNode,Fe=!1),Kt(e,t,n),fi(n.stateNode),ye=o,Fe=i;break;case 5:Me||zt(n,t);case 6:if(o=ye,i=Fe,ye=null,Kt(e,t,n),ye=o,Fe=i,ye!==null)if(Fe)try{(ye.nodeType===9?ye.body:ye.nodeName==="HTML"?ye.ownerDocument.body:ye).removeChild(n.stateNode)}catch(a){re(n,t,a)}else try{ye.removeChild(n.stateNode)}catch(a){re(n,t,a)}break;case 18:ye!==null&&(Fe?(e=ye,Nd(e.nodeType===9?e.body:e.nodeName==="HTML"?e.ownerDocument.body:e,n.stateNode),qo(e)):Nd(ye,n.stateNode));break;case 4:o=ye,i=Fe,ye=n.stateNode.containerInfo,Fe=!0,Kt(e,t,n),ye=o,Fe=i;break;case 0:case 11:case 14:case 15:gn(2,n,t),Me||gn(4,n,t),Kt(e,t,n);break;case 1:Me||(zt(n,t),o=n.stateNode,typeof o.componentWillUnmount=="function"&&Ru(n,t,o)),Kt(e,t,n);break;case 21:Kt(e,t,n);break;case 22:Me=(o=Me)||n.memoizedState!==null,Kt(e,t,n),Me=o;break;default:Kt(e,t,n)}}function Xu(e,t){if(t.memoizedState===null&&(e=t.alternate,e!==null&&(e=e.memoizedState,e!==null))){e=e.dehydrated;try{qo(e)}catch(n){re(t,t.return,n)}}}function Zu(e,t){if(t.memoizedState===null&&(e=t.alternate,e!==null&&(e=e.memoizedState,e!==null&&(e=e.dehydrated,e!==null))))try{qo(e)}catch(n){re(t,t.return,n)}}function Xg(e){switch(e.tag){case 31:case 13:case 19:var t=e.stateNode;return t===null&&(t=e.stateNode=new Wu),t;case 22:return e=e.stateNode,t=e._retryCache,t===null&&(t=e._retryCache=new Wu),t;default:throw Error(m(435,e.tag))}}function ma(e,t){var n=Xg(e);t.forEach(function(o){if(!n.has(o)){n.add(o);var i=ah.bind(null,e,o);o.then(i,i)}})}function Xe(e,t){var n=t.deletions;if(n!==null)for(var o=0;o<n.length;o++){var i=n[o],a=e,s=t,r=s;e:for(;r!==null;){switch(r.tag){case 27:if(wn(r.type)){ye=r.stateNode,Fe=!1;break e}break;case 5:ye=r.stateNode,Fe=!1;break e;case 3:case 4:ye=r.stateNode.containerInfo,Fe=!0;break e}r=r.return}if(ye===null)throw Error(m(160));Fu(a,s,i),ye=null,Fe=!1,a=i.alternate,a!==null&&(a.return=null),i.return=null}if(t.subtreeFlags&13886)for(t=t.child;t!==null;)Ju(t,e),t=t.sibling}var Et=null;function Ju(e,t){var n=e.alternate,o=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:Xe(t,e),Ze(e),o&4&&(gn(3,e,e.return),si(3,e),gn(5,e,e.return));break;case 1:Xe(t,e),Ze(e),o&512&&(Me||n===null||zt(n,n.return)),o&64&&_t&&(e=e.updateQueue,e!==null&&(o=e.callbacks,o!==null&&(n=e.shared.hiddenCallbacks,e.shared.hiddenCallbacks=n===null?o:n.concat(o))));break;case 26:var i=Et;if(Xe(t,e),Ze(e),o&512&&(Me||n===null||zt(n,n.return)),o&4){var a=n!==null?n.memoizedState:null;if(o=e.memoizedState,n===null)if(o===null)if(e.stateNode===null){e:{o=e.type,n=e.memoizedProps,i=i.ownerDocument||i;t:switch(o){case"title":a=i.getElementsByTagName("title")[0],(!a||a[Go]||a[Ve]||a.namespaceURI==="http://www.w3.org/2000/svg"||a.hasAttribute("itemprop"))&&(a=i.createElement(o),i.head.insertBefore(a,i.querySelector("head > title"))),Le(a,o,n),a[Ve]=e,De(a),o=a;break e;case"link":var s=Jd("link","href",i).get(o+(n.href||""));if(s){for(var r=0;r<s.length;r++)if(a=s[r],a.getAttribute("href")===(n.href==null||n.href===""?null:n.href)&&a.getAttribute("rel")===(n.rel==null?null:n.rel)&&a.getAttribute("title")===(n.title==null?null:n.title)&&a.getAttribute("crossorigin")===(n.crossOrigin==null?null:n.crossOrigin)){s.splice(r,1);break t}}a=i.createElement(o),Le(a,o,n),i.head.appendChild(a);break;case"meta":if(s=Jd("meta","content",i).get(o+(n.content||""))){for(r=0;r<s.length;r++)if(a=s[r],a.getAttribute("content")===(n.content==null?null:""+n.content)&&a.getAttribute("name")===(n.name==null?null:n.name)&&a.getAttribute("property")===(n.property==null?null:n.property)&&a.getAttribute("http-equiv")===(n.httpEquiv==null?null:n.httpEquiv)&&a.getAttribute("charset")===(n.charSet==null?null:n.charSet)){s.splice(r,1);break t}}a=i.createElement(o),Le(a,o,n),i.head.appendChild(a);break;default:throw Error(m(468,o))}a[Ve]=e,De(a),o=a}e.stateNode=o}else $d(i,e.type,e.stateNode);else e.stateNode=Zd(i,o,e.memoizedProps);else a!==o?(a===null?n.stateNode!==null&&(n=n.stateNode,n.parentNode.removeChild(n)):a.count--,o===null?$d(i,e.type,e.stateNode):Zd(i,o,e.memoizedProps)):o===null&&e.stateNode!==null&&vr(e,e.memoizedProps,n.memoizedProps)}break;case 27:Xe(t,e),Ze(e),o&512&&(Me||n===null||zt(n,n.return)),n!==null&&o&4&&vr(e,e.memoizedProps,n.memoizedProps);break;case 5:if(Xe(t,e),Ze(e),o&512&&(Me||n===null||zt(n,n.return)),e.flags&32){i=e.stateNode;try{Jn(i,"")}catch(M){re(e,e.return,M)}}o&4&&e.stateNode!=null&&(i=e.memoizedProps,vr(e,i,n!==null?n.memoizedProps:i)),o&1024&&(Sr=!0);break;case 6:if(Xe(t,e),Ze(e),o&4){if(e.stateNode===null)throw Error(m(162));o=e.memoizedProps,n=e.stateNode;try{n.nodeValue=o}catch(M){re(e,e.return,M)}}break;case 3:if(za=null,i=Et,Et=Pa(t.containerInfo),Xe(t,e),Et=i,Ze(e),o&4&&n!==null&&n.memoizedState.isDehydrated)try{qo(t.containerInfo)}catch(M){re(e,e.return,M)}Sr&&(Sr=!1,$u(e));break;case 4:o=Et,Et=Pa(e.stateNode.containerInfo),Xe(t,e),Ze(e),Et=o;break;case 12:Xe(t,e),Ze(e);break;case 31:Xe(t,e),Ze(e),o&4&&(o=e.updateQueue,o!==null&&(e.updateQueue=null,ma(e,o)));break;case 13:Xe(t,e),Ze(e),e.child.flags&8192&&e.memoizedState!==null!=(n!==null&&n.memoizedState!==null)&&(ya=it()),o&4&&(o=e.updateQueue,o!==null&&(e.updateQueue=null,ma(e,o)));break;case 22:i=e.memoizedState!==null;var c=n!==null&&n.memoizedState!==null,g=_t,b=Me;if(_t=g||i,Me=b||c,Xe(t,e),Me=b,_t=g,Ze(e),o&8192)e:for(t=e.stateNode,t._visibility=i?t._visibility&-2:t._visibility|1,i&&(n===null||c||_t||Me||Qn(e)),n=null,t=e;;){if(t.tag===5||t.tag===26){if(n===null){c=n=t;try{if(a=c.stateNode,i)s=a.style,typeof s.setProperty=="function"?s.setProperty("display","none","important"):s.display="none";else{r=c.stateNode;var C=c.memoizedProps.style,h=C!=null&&C.hasOwnProperty("display")?C.display:null;r.style.display=h==null||typeof h=="boolean"?"":(""+h).trim()}}catch(M){re(c,c.return,M)}}}else if(t.tag===6){if(n===null){c=t;try{c.stateNode.nodeValue=i?"":c.memoizedProps}catch(M){re(c,c.return,M)}}}else if(t.tag===18){if(n===null){c=t;try{var y=c.stateNode;i?Rd(y,!0):Rd(c.stateNode,!1)}catch(M){re(c,c.return,M)}}}else if((t.tag!==22&&t.tag!==23||t.memoizedState===null||t===e)&&t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break e;for(;t.sibling===null;){if(t.return===null||t.return===e)break e;n===t&&(n=null),t=t.return}n===t&&(n=null),t.sibling.return=t.return,t=t.sibling}o&4&&(o=e.updateQueue,o!==null&&(n=o.retryQueue,n!==null&&(o.retryQueue=null,ma(e,n))));break;case 19:Xe(t,e),Ze(e),o&4&&(o=e.updateQueue,o!==null&&(e.updateQueue=null,ma(e,o)));break;case 30:break;case 21:break;default:Xe(t,e),Ze(e)}}function Ze(e){var t=e.flags;if(t&2){try{for(var n,o=e.return;o!==null;){if(Qu(o)){n=o;break}o=o.return}if(n==null)throw Error(m(160));switch(n.tag){case 27:var i=n.stateNode,a=wr(e);ha(e,a,i);break;case 5:var s=n.stateNode;n.flags&32&&(Jn(s,""),n.flags&=-33);var r=wr(e);ha(e,r,s);break;case 3:case 4:var c=n.stateNode.containerInfo,g=wr(e);Cr(e,g,c);break;default:throw Error(m(161))}}catch(b){re(e,e.return,b)}e.flags&=-3}t&4096&&(e.flags&=-4097)}function $u(e){if(e.subtreeFlags&1024)for(e=e.child;e!==null;){var t=e;$u(t),t.tag===5&&t.flags&1024&&t.stateNode.reset(),e=e.sibling}}function Ft(e,t){if(t.subtreeFlags&8772)for(t=t.child;t!==null;)_u(e,t.alternate,t),t=t.sibling}function Qn(e){for(e=e.child;e!==null;){var t=e;switch(t.tag){case 0:case 11:case 14:case 15:gn(4,t,t.return),Qn(t);break;case 1:zt(t,t.return);var n=t.stateNode;typeof n.componentWillUnmount=="function"&&Ru(t,t.return,n),Qn(t);break;case 27:fi(t.stateNode);case 26:case 5:zt(t,t.return),Qn(t);break;case 22:t.memoizedState===null&&Qn(t);break;case 30:Qn(t);break;default:Qn(t)}e=e.sibling}}function Xt(e,t,n){for(n=n&&(t.subtreeFlags&8772)!==0,t=t.child;t!==null;){var o=t.alternate,i=e,a=t,s=a.flags;switch(a.tag){case 0:case 11:case 15:Xt(i,a,n),si(4,a);break;case 1:if(Xt(i,a,n),o=a,i=o.stateNode,typeof i.componentDidMount=="function")try{i.componentDidMount()}catch(g){re(o,o.return,g)}if(o=a,i=o.updateQueue,i!==null){var r=o.stateNode;try{var c=i.shared.hiddenCallbacks;if(c!==null)for(i.shared.hiddenCallbacks=null,i=0;i<c.length;i++)qc(c[i],r)}catch(g){re(o,o.return,g)}}n&&s&64&&Nu(a),ri(a,a.return);break;case 27:Hu(a);case 26:case 5:Xt(i,a,n),n&&o===null&&s&4&&Ou(a),ri(a,a.return);break;case 12:Xt(i,a,n);break;case 31:Xt(i,a,n),n&&s&4&&Xu(i,a);break;case 13:Xt(i,a,n),n&&s&4&&Zu(i,a);break;case 22:a.memoizedState===null&&Xt(i,a,n),ri(a,a.return);break;case 30:break;default:Xt(i,a,n)}t=t.sibling}}function xr(e,t){var n=null;e!==null&&e.memoizedState!==null&&e.memoizedState.cachePool!==null&&(n=e.memoizedState.cachePool.pool),e=null,t.memoizedState!==null&&t.memoizedState.cachePool!==null&&(e=t.memoizedState.cachePool.pool),e!==n&&(e!=null&&e.refCount++,n!=null&&_o(n))}function Ar(e,t){e=null,t.alternate!==null&&(e=t.alternate.memoizedState.cache),t=t.memoizedState.cache,t!==e&&(t.refCount++,e!=null&&_o(e))}function Tt(e,t,n,o){if(t.subtreeFlags&10256)for(t=t.child;t!==null;)ed(e,t,n,o),t=t.sibling}function ed(e,t,n,o){var i=t.flags;switch(t.tag){case 0:case 11:case 15:Tt(e,t,n,o),i&2048&&si(9,t);break;case 1:Tt(e,t,n,o);break;case 3:Tt(e,t,n,o),i&2048&&(e=null,t.alternate!==null&&(e=t.alternate.memoizedState.cache),t=t.memoizedState.cache,t!==e&&(t.refCount++,e!=null&&_o(e)));break;case 12:if(i&2048){Tt(e,t,n,o),e=t.stateNode;try{var a=t.memoizedProps,s=a.id,r=a.onPostCommit;typeof r=="function"&&r(s,t.alternate===null?"mount":"update",e.passiveEffectDuration,-0)}catch(c){re(t,t.return,c)}}else Tt(e,t,n,o);break;case 31:Tt(e,t,n,o);break;case 13:Tt(e,t,n,o);break;case 23:break;case 22:a=t.stateNode,s=t.alternate,t.memoizedState!==null?a._visibility&2?Tt(e,t,n,o):li(e,t):a._visibility&2?Tt(e,t,n,o):(a._visibility|=2,vo(e,t,n,o,(t.subtreeFlags&10256)!==0||!1)),i&2048&&xr(s,t);break;case 24:Tt(e,t,n,o),i&2048&&Ar(t.alternate,t);break;default:Tt(e,t,n,o)}}function vo(e,t,n,o,i){for(i=i&&((t.subtreeFlags&10256)!==0||!1),t=t.child;t!==null;){var a=e,s=t,r=n,c=o,g=s.flags;switch(s.tag){case 0:case 11:case 15:vo(a,s,r,c,i),si(8,s);break;case 23:break;case 22:var b=s.stateNode;s.memoizedState!==null?b._visibility&2?vo(a,s,r,c,i):li(a,s):(b._visibility|=2,vo(a,s,r,c,i)),i&&g&2048&&xr(s.alternate,s);break;case 24:vo(a,s,r,c,i),i&&g&2048&&Ar(s.alternate,s);break;default:vo(a,s,r,c,i)}t=t.sibling}}function li(e,t){if(t.subtreeFlags&10256)for(t=t.child;t!==null;){var n=e,o=t,i=o.flags;switch(o.tag){case 22:li(n,o),i&2048&&xr(o.alternate,o);break;case 24:li(n,o),i&2048&&Ar(o.alternate,o);break;default:li(n,o)}t=t.sibling}}var ci=8192;function wo(e,t,n){if(e.subtreeFlags&ci)for(e=e.child;e!==null;)td(e,t,n),e=e.sibling}function td(e,t,n){switch(e.tag){case 26:wo(e,t,n),e.flags&ci&&e.memoizedState!==null&&Uh(n,Et,e.memoizedState,e.memoizedProps);break;case 5:wo(e,t,n);break;case 3:case 4:var o=Et;Et=Pa(e.stateNode.containerInfo),wo(e,t,n),Et=o;break;case 22:e.memoizedState===null&&(o=e.alternate,o!==null&&o.memoizedState!==null?(o=ci,ci=16777216,wo(e,t,n),ci=o):wo(e,t,n));break;default:wo(e,t,n)}}function nd(e){var t=e.alternate;if(t!==null&&(e=t.child,e!==null)){t.child=null;do t=e.sibling,e.sibling=null,e=t;while(e!==null)}}function ui(e){var t=e.deletions;if((e.flags&16)!==0){if(t!==null)for(var n=0;n<t.length;n++){var o=t[n];Ie=o,id(o,e)}nd(e)}if(e.subtreeFlags&10256)for(e=e.child;e!==null;)od(e),e=e.sibling}function od(e){switch(e.tag){case 0:case 11:case 15:ui(e),e.flags&2048&&gn(9,e,e.return);break;case 3:ui(e);break;case 12:ui(e);break;case 22:var t=e.stateNode;e.memoizedState!==null&&t._visibility&2&&(e.return===null||e.return.tag!==13)?(t._visibility&=-3,fa(e)):ui(e);break;default:ui(e)}}function fa(e){var t=e.deletions;if((e.flags&16)!==0){if(t!==null)for(var n=0;n<t.length;n++){var o=t[n];Ie=o,id(o,e)}nd(e)}for(e=e.child;e!==null;){switch(t=e,t.tag){case 0:case 11:case 15:gn(8,t,t.return),fa(t);break;case 22:n=t.stateNode,n._visibility&2&&(n._visibility&=-3,fa(t));break;default:fa(t)}e=e.sibling}}function id(e,t){for(;Ie!==null;){var n=Ie;switch(n.tag){case 0:case 11:case 15:gn(8,n,t);break;case 23:case 22:if(n.memoizedState!==null&&n.memoizedState.cachePool!==null){var o=n.memoizedState.cachePool.pool;o!=null&&o.refCount++}break;case 24:_o(n.memoizedState.cache)}if(o=n.child,o!==null)o.return=n,Ie=o;else e:for(n=e;Ie!==null;){o=Ie;var i=o.sibling,a=o.return;if(Ku(o),o===n){Ie=null;break e}if(i!==null){i.return=a,Ie=i;break e}Ie=a}}}var Zg={getCacheForType:function(e){var t=je(ke),n=t.data.get(e);return n===void 0&&(n=e(),t.data.set(e,n)),n},cacheSignal:function(){return je(ke).controller.signal}},Jg=typeof WeakMap=="function"?WeakMap:Map,ie=0,pe=null,H=null,K=0,se=0,dt=null,hn=!1,Co=!1,kr=!1,Zt=0,Ce=0,mn=0,Hn=0,Er=0,pt=0,So=0,di=null,Je=null,Tr=!1,ya=0,ad=0,ba=1/0,va=null,fn=null,qe=0,yn=null,xo=null,Jt=0,Mr=0,Pr=null,sd=null,pi=0,qr=null;function gt(){return(ie&2)!==0&&K!==0?K&-K:v.T!==null?Ur():Sl()}function rd(){if(pt===0)if((K&536870912)===0||X){var e=Ti;Ti<<=1,(Ti&3932160)===0&&(Ti=262144),pt=e}else pt=536870912;return e=ct.current,e!==null&&(e.flags|=32),pt}function $e(e,t,n){(e===pe&&(se===2||se===9)||e.cancelPendingCommit!==null)&&(Ao(e,0),bn(e,K,pt,!1)),Io(e,n),((ie&2)===0||e!==pe)&&(e===pe&&((ie&2)===0&&(Hn|=n),Ce===4&&bn(e,K,pt,!1)),Dt(e))}function ld(e,t,n){if((ie&6)!==0)throw Error(m(327));var o=!n&&(t&127)===0&&(t&e.expiredLanes)===0||Do(e,t),i=o?th(e,t):Dr(e,t,!0),a=o;do{if(i===0){Co&&!o&&bn(e,t,0,!1);break}else{if(n=e.current.alternate,a&&!$g(n)){i=Dr(e,t,!1),a=!1;continue}if(i===2){if(a=t,e.errorRecoveryDisabledLanes&a)var s=0;else s=e.pendingLanes&-536870913,s=s!==0?s:s&536870912?536870912:0;if(s!==0){t=s;e:{var r=e;i=di;var c=r.current.memoizedState.isDehydrated;if(c&&(Ao(r,s).flags|=256),s=Dr(r,s,!1),s!==2){if(kr&&!c){r.errorRecoveryDisabledLanes|=a,Hn|=a,i=4;break e}a=Je,Je=i,a!==null&&(Je===null?Je=a:Je.push.apply(Je,a))}i=s}if(a=!1,i!==2)continue}}if(i===1){Ao(e,0),bn(e,t,0,!0);break}e:{switch(o=e,a=i,a){case 0:case 1:throw Error(m(345));case 4:if((t&4194048)!==t)break;case 6:bn(o,t,pt,!hn);break e;case 2:Je=null;break;case 3:case 5:break;default:throw Error(m(329))}if((t&62914560)===t&&(i=ya+300-it(),10<i)){if(bn(o,t,pt,!hn),Pi(o,0,!0)!==0)break e;Jt=t,o.timeoutHandle=Ld(cd.bind(null,o,n,Je,va,Tr,t,pt,Hn,So,hn,a,"Throttled",-0,0),i);break e}cd(o,n,Je,va,Tr,t,pt,Hn,So,hn,a,null,-0,0)}}break}while(!0);Dt(e)}function cd(e,t,n,o,i,a,s,r,c,g,b,C,h,y){if(e.timeoutHandle=-1,C=t.subtreeFlags,C&8192||(C&16785408)===16785408){C={stylesheets:null,count:0,imgCount:0,imgBytes:0,suspenseyImages:[],waitingForImages:!0,waitingForViewTransition:!1,unsuspend:jt},td(t,a,C);var M=(a&62914560)===a?ya-it():(a&4194048)===a?ad-it():0;if(M=jh(C,M),M!==null){Jt=a,e.cancelPendingCommit=M(yd.bind(null,e,t,a,n,o,i,s,r,c,b,C,null,h,y)),bn(e,a,s,!g);return}}yd(e,t,a,n,o,i,s,r,c)}function $g(e){for(var t=e;;){var n=t.tag;if((n===0||n===11||n===15)&&t.flags&16384&&(n=t.updateQueue,n!==null&&(n=n.stores,n!==null)))for(var o=0;o<n.length;o++){var i=n[o],a=i.getSnapshot;i=i.value;try{if(!rt(a(),i))return!1}catch{return!1}}if(n=t.child,t.subtreeFlags&16384&&n!==null)n.return=t,t=n;else{if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}function bn(e,t,n,o){t&=~Er,t&=~Hn,e.suspendedLanes|=t,e.pingedLanes&=~t,o&&(e.warmLanes|=t),o=e.expirationTimes;for(var i=t;0<i;){var a=31-st(i),s=1<<a;o[a]=-1,i&=~s}n!==0&&vl(e,n,t)}function wa(){return(ie&6)===0?(gi(0),!1):!0}function zr(){if(H!==null){if(se===0)var e=H.return;else e=H,Nt=Un=null,_s(e),ho=null,Fo=0,e=H;for(;e!==null;)Bu(e.alternate,e),e=e.return;H=null}}function Ao(e,t){var n=e.timeoutHandle;n!==-1&&(e.timeoutHandle=-1,vh(n)),n=e.cancelPendingCommit,n!==null&&(e.cancelPendingCommit=null,n()),Jt=0,zr(),pe=e,H=n=Lt(e.current,null),K=t,se=0,dt=null,hn=!1,Co=Do(e,t),kr=!1,So=pt=Er=Hn=mn=Ce=0,Je=di=null,Tr=!1,(t&8)!==0&&(t|=t&32);var o=e.entangledLanes;if(o!==0)for(e=e.entanglements,o&=t;0<o;){var i=31-st(o),a=1<<i;t|=e[i],o&=~a}return Zt=t,Ni(),n}function ud(e,t){L=null,v.H=oi,t===go||t===Fi?(t=Ec(),se=3):t===Vs?(t=Ec(),se=4):se=t===cr?8:t!==null&&typeof t=="object"&&typeof t.then=="function"?6:1,dt=t,H===null&&(Ce=1,ca(e,bt(t,e.current)))}function dd(){var e=ct.current;return e===null?!0:(K&4194048)===K?St===null:(K&62914560)===K||(K&536870912)!==0?e===St:!1}function pd(){var e=v.H;return v.H=oi,e===null?oi:e}function gd(){var e=v.A;return v.A=Zg,e}function Ca(){Ce=4,hn||(K&4194048)!==K&&ct.current!==null||(Co=!0),(mn&134217727)===0&&(Hn&134217727)===0||pe===null||bn(pe,K,pt,!1)}function Dr(e,t,n){var o=ie;ie|=2;var i=pd(),a=gd();(pe!==e||K!==t)&&(va=null,Ao(e,t)),t=!1;var s=Ce;e:do try{if(se!==0&&H!==null){var r=H,c=dt;switch(se){case 8:zr(),s=6;break e;case 3:case 2:case 9:case 6:ct.current===null&&(t=!0);var g=se;if(se=0,dt=null,ko(e,r,c,g),n&&Co){s=0;break e}break;default:g=se,se=0,dt=null,ko(e,r,c,g)}}eh(),s=Ce;break}catch(b){ud(e,b)}while(!0);return t&&e.shellSuspendCounter++,Nt=Un=null,ie=o,v.H=i,v.A=a,H===null&&(pe=null,K=0,Ni()),s}function eh(){for(;H!==null;)hd(H)}function th(e,t){var n=ie;ie|=2;var o=pd(),i=gd();pe!==e||K!==t?(va=null,ba=it()+500,Ao(e,t)):Co=Do(e,t);e:do try{if(se!==0&&H!==null){t=H;var a=dt;t:switch(se){case 1:se=0,dt=null,ko(e,t,a,1);break;case 2:case 9:if(Ac(a)){se=0,dt=null,md(t);break}t=function(){se!==2&&se!==9||pe!==e||(se=7),Dt(e)},a.then(t,t);break e;case 3:se=7;break e;case 4:se=5;break e;case 7:Ac(a)?(se=0,dt=null,md(t)):(se=0,dt=null,ko(e,t,a,7));break;case 5:var s=null;switch(H.tag){case 26:s=H.memoizedState;case 5:case 27:var r=H;if(s?ep(s):r.stateNode.complete){se=0,dt=null;var c=r.sibling;if(c!==null)H=c;else{var g=r.return;g!==null?(H=g,Sa(g)):H=null}break t}}se=0,dt=null,ko(e,t,a,5);break;case 6:se=0,dt=null,ko(e,t,a,6);break;case 8:zr(),Ce=6;break e;default:throw Error(m(462))}}nh();break}catch(b){ud(e,b)}while(!0);return Nt=Un=null,v.H=o,v.A=i,ie=n,H!==null?0:(pe=null,K=0,Ni(),Ce)}function nh(){for(;H!==null&&!kp();)hd(H)}function hd(e){var t=Yu(e.alternate,e,Zt);e.memoizedProps=e.pendingProps,t===null?Sa(e):H=t}function md(e){var t=e,n=t.alternate;switch(t.tag){case 15:case 0:t=Du(n,t,t.pendingProps,t.type,void 0,K);break;case 11:t=Du(n,t,t.pendingProps,t.type.render,t.ref,K);break;case 5:_s(t);default:Bu(n,t),t=H=gc(t,Zt),t=Yu(n,t,Zt)}e.memoizedProps=e.pendingProps,t===null?Sa(e):H=t}function ko(e,t,n,o){Nt=Un=null,_s(t),ho=null,Fo=0;var i=t.return;try{if(Qg(e,i,t,n,K)){Ce=1,ca(e,bt(n,e.current)),H=null;return}}catch(a){if(i!==null)throw H=i,a;Ce=1,ca(e,bt(n,e.current)),H=null;return}t.flags&32768?(X||o===1?e=!0:Co||(K&536870912)!==0?e=!1:(hn=e=!0,(o===2||o===9||o===3||o===6)&&(o=ct.current,o!==null&&o.tag===13&&(o.flags|=16384))),fd(t,e)):Sa(t)}function Sa(e){var t=e;do{if((t.flags&32768)!==0){fd(t,hn);return}e=t.return;var n=_g(t.alternate,t,Zt);if(n!==null){H=n;return}if(t=t.sibling,t!==null){H=t;return}H=t=e}while(t!==null);Ce===0&&(Ce=5)}function fd(e,t){do{var n=Kg(e.alternate,e);if(n!==null){n.flags&=32767,H=n;return}if(n=e.return,n!==null&&(n.flags|=32768,n.subtreeFlags=0,n.deletions=null),!t&&(e=e.sibling,e!==null)){H=e;return}H=e=n}while(e!==null);Ce=6,H=null}function yd(e,t,n,o,i,a,s,r,c){e.cancelPendingCommit=null;do xa();while(qe!==0);if((ie&6)!==0)throw Error(m(327));if(t!==null){if(t===e.current)throw Error(m(177));if(a=t.lanes|t.childLanes,a|=ws,Vp(e,n,a,s,r,c),e===pe&&(H=pe=null,K=0),xo=t,yn=e,Jt=n,Mr=a,Pr=i,sd=o,(t.subtreeFlags&10256)!==0||(t.flags&10256)!==0?(e.callbackNode=null,e.callbackPriority=0,sh(ki,function(){return Sd(),null})):(e.callbackNode=null,e.callbackPriority=0),o=(t.flags&13878)!==0,(t.subtreeFlags&13878)!==0||o){o=v.T,v.T=null,i=T.p,T.p=2,s=ie,ie|=4;try{Fg(e,t,n)}finally{ie=s,T.p=i,v.T=o}}qe=1,bd(),vd(),wd()}}function bd(){if(qe===1){qe=0;var e=yn,t=xo,n=(t.flags&13878)!==0;if((t.subtreeFlags&13878)!==0||n){n=v.T,v.T=null;var o=T.p;T.p=2;var i=ie;ie|=4;try{Ju(t,e);var a=Qr,s=ic(e.containerInfo),r=a.focusedElem,c=a.selectionRange;if(s!==r&&r&&r.ownerDocument&&oc(r.ownerDocument.documentElement,r)){if(c!==null&&ms(r)){var g=c.start,b=c.end;if(b===void 0&&(b=g),"selectionStart"in r)r.selectionStart=g,r.selectionEnd=Math.min(b,r.value.length);else{var C=r.ownerDocument||document,h=C&&C.defaultView||window;if(h.getSelection){var y=h.getSelection(),M=r.textContent.length,U=Math.min(c.start,M),de=c.end===void 0?U:Math.min(c.end,M);!y.extend&&U>de&&(s=de,de=U,U=s);var d=nc(r,U),u=nc(r,de);if(d&&u&&(y.rangeCount!==1||y.anchorNode!==d.node||y.anchorOffset!==d.offset||y.focusNode!==u.node||y.focusOffset!==u.offset)){var p=C.createRange();p.setStart(d.node,d.offset),y.removeAllRanges(),U>de?(y.addRange(p),y.extend(u.node,u.offset)):(p.setEnd(u.node,u.offset),y.addRange(p))}}}}for(C=[],y=r;y=y.parentNode;)y.nodeType===1&&C.push({element:y,left:y.scrollLeft,top:y.scrollTop});for(typeof r.focus=="function"&&r.focus(),r=0;r<C.length;r++){var w=C[r];w.element.scrollLeft=w.left,w.element.scrollTop=w.top}}Va=!!Or,Qr=Or=null}finally{ie=i,T.p=o,v.T=n}}e.current=t,qe=2}}function vd(){if(qe===2){qe=0;var e=yn,t=xo,n=(t.flags&8772)!==0;if((t.subtreeFlags&8772)!==0||n){n=v.T,v.T=null;var o=T.p;T.p=2;var i=ie;ie|=4;try{_u(e,t.alternate,t)}finally{ie=i,T.p=o,v.T=n}}qe=3}}function wd(){if(qe===4||qe===3){qe=0,Ep();var e=yn,t=xo,n=Jt,o=sd;(t.subtreeFlags&10256)!==0||(t.flags&10256)!==0?qe=5:(qe=0,xo=yn=null,Cd(e,e.pendingLanes));var i=e.pendingLanes;if(i===0&&(fn=null),Xa(n),t=t.stateNode,at&&typeof at.onCommitFiberRoot=="function")try{at.onCommitFiberRoot(zo,t,void 0,(t.current.flags&128)===128)}catch{}if(o!==null){t=v.T,i=T.p,T.p=2,v.T=null;try{for(var a=e.onRecoverableError,s=0;s<o.length;s++){var r=o[s];a(r.value,{componentStack:r.stack})}}finally{v.T=t,T.p=i}}(Jt&3)!==0&&xa(),Dt(e),i=e.pendingLanes,(n&261930)!==0&&(i&42)!==0?e===qr?pi++:(pi=0,qr=e):pi=0,gi(0)}}function Cd(e,t){(e.pooledCacheLanes&=t)===0&&(t=e.pooledCache,t!=null&&(e.pooledCache=null,_o(t)))}function xa(){return bd(),vd(),wd(),Sd()}function Sd(){if(qe!==5)return!1;var e=yn,t=Mr;Mr=0;var n=Xa(Jt),o=v.T,i=T.p;try{T.p=32>n?32:n,v.T=null,n=Pr,Pr=null;var a=yn,s=Jt;if(qe=0,xo=yn=null,Jt=0,(ie&6)!==0)throw Error(m(331));var r=ie;if(ie|=4,od(a.current),ed(a,a.current,s,n),ie=r,gi(0,!1),at&&typeof at.onPostCommitFiberRoot=="function")try{at.onPostCommitFiberRoot(zo,a)}catch{}return!0}finally{T.p=i,v.T=o,Cd(e,t)}}function xd(e,t,n){t=bt(n,t),t=lr(e.stateNode,t,2),e=un(e,t,2),e!==null&&(Io(e,2),Dt(e))}function re(e,t,n){if(e.tag===3)xd(e,e,n);else for(;t!==null;){if(t.tag===3){xd(t,e,n);break}else if(t.tag===1){var o=t.stateNode;if(typeof t.type.getDerivedStateFromError=="function"||typeof o.componentDidCatch=="function"&&(fn===null||!fn.has(o))){e=bt(n,e),n=Au(2),o=un(t,n,2),o!==null&&(ku(n,o,t,e),Io(o,2),Dt(o));break}}t=t.return}}function Ir(e,t,n){var o=e.pingCache;if(o===null){o=e.pingCache=new Jg;var i=new Set;o.set(t,i)}else i=o.get(t),i===void 0&&(i=new Set,o.set(t,i));i.has(n)||(kr=!0,i.add(n),e=oh.bind(null,e,t,n),t.then(e,e))}function oh(e,t,n){var o=e.pingCache;o!==null&&o.delete(t),e.pingedLanes|=e.suspendedLanes&n,e.warmLanes&=~n,pe===e&&(K&n)===n&&(Ce===4||Ce===3&&(K&62914560)===K&&300>it()-ya?(ie&2)===0&&Ao(e,0):Er|=n,So===K&&(So=0)),Dt(e)}function Ad(e,t){t===0&&(t=bl()),e=In(e,t),e!==null&&(Io(e,t),Dt(e))}function ih(e){var t=e.memoizedState,n=0;t!==null&&(n=t.retryLane),Ad(e,n)}function ah(e,t){var n=0;switch(e.tag){case 31:case 13:var o=e.stateNode,i=e.memoizedState;i!==null&&(n=i.retryLane);break;case 19:o=e.stateNode;break;case 22:o=e.stateNode._retryCache;break;default:throw Error(m(314))}o!==null&&o.delete(t),Ad(e,n)}function sh(e,t){return Wa(e,t)}var Aa=null,Eo=null,Gr=!1,ka=!1,Vr=!1,vn=0;function Dt(e){e!==Eo&&e.next===null&&(Eo===null?Aa=Eo=e:Eo=Eo.next=e),ka=!0,Gr||(Gr=!0,lh())}function gi(e,t){if(!Vr&&ka){Vr=!0;do for(var n=!1,o=Aa;o!==null;){if(e!==0){var i=o.pendingLanes;if(i===0)var a=0;else{var s=o.suspendedLanes,r=o.pingedLanes;a=(1<<31-st(42|e)+1)-1,a&=i&~(s&~r),a=a&201326741?a&201326741|1:a?a|2:0}a!==0&&(n=!0,Md(o,a))}else a=K,a=Pi(o,o===pe?a:0,o.cancelPendingCommit!==null||o.timeoutHandle!==-1),(a&3)===0||Do(o,a)||(n=!0,Md(o,a));o=o.next}while(n);Vr=!1}}function rh(){kd()}function kd(){ka=Gr=!1;var e=0;vn!==0&&bh()&&(e=vn);for(var t=it(),n=null,o=Aa;o!==null;){var i=o.next,a=Ed(o,t);a===0?(o.next=null,n===null?Aa=i:n.next=i,i===null&&(Eo=n)):(n=o,(e!==0||(a&3)!==0)&&(ka=!0)),o=i}qe!==0&&qe!==5||gi(e),vn!==0&&(vn=0)}function Ed(e,t){for(var n=e.suspendedLanes,o=e.pingedLanes,i=e.expirationTimes,a=e.pendingLanes&-62914561;0<a;){var s=31-st(a),r=1<<s,c=i[s];c===-1?((r&n)===0||(r&o)!==0)&&(i[s]=Gp(r,t)):c<=t&&(e.expiredLanes|=r),a&=~r}if(t=pe,n=K,n=Pi(e,e===t?n:0,e.cancelPendingCommit!==null||e.timeoutHandle!==-1),o=e.callbackNode,n===0||e===t&&(se===2||se===9)||e.cancelPendingCommit!==null)return o!==null&&o!==null&&_a(o),e.callbackNode=null,e.callbackPriority=0;if((n&3)===0||Do(e,n)){if(t=n&-n,t===e.callbackPriority)return t;switch(o!==null&&_a(o),Xa(n)){case 2:case 8:n=fl;break;case 32:n=ki;break;case 268435456:n=yl;break;default:n=ki}return o=Td.bind(null,e),n=Wa(n,o),e.callbackPriority=t,e.callbackNode=n,t}return o!==null&&o!==null&&_a(o),e.callbackPriority=2,e.callbackNode=null,2}function Td(e,t){if(qe!==0&&qe!==5)return e.callbackNode=null,e.callbackPriority=0,null;var n=e.callbackNode;if(xa()&&e.callbackNode!==n)return null;var o=K;return o=Pi(e,e===pe?o:0,e.cancelPendingCommit!==null||e.timeoutHandle!==-1),o===0?null:(ld(e,o,t),Ed(e,it()),e.callbackNode!=null&&e.callbackNode===n?Td.bind(null,e):null)}function Md(e,t){if(xa())return null;ld(e,t,!0)}function lh(){wh(function(){(ie&6)!==0?Wa(ml,rh):kd()})}function Ur(){if(vn===0){var e=uo;e===0&&(e=Ei,Ei<<=1,(Ei&261888)===0&&(Ei=256)),vn=e}return vn}function Pd(e){return e==null||typeof e=="symbol"||typeof e=="boolean"?null:typeof e=="function"?e:Ii(""+e)}function qd(e,t){var n=t.ownerDocument.createElement("input");return n.name=t.name,n.value=t.value,e.id&&n.setAttribute("form",e.id),t.parentNode.insertBefore(n,t),e=new FormData(e),n.parentNode.removeChild(n),e}function ch(e,t,n,o,i){if(t==="submit"&&n&&n.stateNode===i){var a=Pd((i[_e]||null).action),s=o.submitter;s&&(t=(t=s[_e]||null)?Pd(t.formAction):s.getAttribute("formAction"),t!==null&&(a=t,s=null));var r=new ji("action","action",null,o,i);e.push({event:r,listeners:[{instance:null,listener:function(){if(o.defaultPrevented){if(vn!==0){var c=s?qd(i,s):new FormData(i);nr(n,{pending:!0,data:c,method:i.method,action:a},null,c)}}else typeof a=="function"&&(r.preventDefault(),c=s?qd(i,s):new FormData(i),nr(n,{pending:!0,data:c,method:i.method,action:a},a,c))},currentTarget:i}]})}}for(var jr=0;jr<vs.length;jr++){var Yr=vs[jr],uh=Yr.toLowerCase(),dh=Yr[0].toUpperCase()+Yr.slice(1);kt(uh,"on"+dh)}kt(rc,"onAnimationEnd"),kt(lc,"onAnimationIteration"),kt(cc,"onAnimationStart"),kt("dblclick","onDoubleClick"),kt("focusin","onFocus"),kt("focusout","onBlur"),kt(Tg,"onTransitionRun"),kt(Mg,"onTransitionStart"),kt(Pg,"onTransitionCancel"),kt(uc,"onTransitionEnd"),Xn("onMouseEnter",["mouseout","mouseover"]),Xn("onMouseLeave",["mouseout","mouseover"]),Xn("onPointerEnter",["pointerout","pointerover"]),Xn("onPointerLeave",["pointerout","pointerover"]),Pn("onChange","change click focusin focusout input keydown keyup selectionchange".split(" ")),Pn("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" ")),Pn("onBeforeInput",["compositionend","keypress","textInput","paste"]),Pn("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" ")),Pn("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" ")),Pn("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var hi="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),ph=new Set("beforetoggle cancel close invalid load scroll scrollend toggle".split(" ").concat(hi));function zd(e,t){t=(t&4)!==0;for(var n=0;n<e.length;n++){var o=e[n],i=o.event;o=o.listeners;e:{var a=void 0;if(t)for(var s=o.length-1;0<=s;s--){var r=o[s],c=r.instance,g=r.currentTarget;if(r=r.listener,c!==a&&i.isPropagationStopped())break e;a=r,i.currentTarget=g;try{a(i)}catch(b){Bi(b)}i.currentTarget=null,a=c}else for(s=0;s<o.length;s++){if(r=o[s],c=r.instance,g=r.currentTarget,r=r.listener,c!==a&&i.isPropagationStopped())break e;a=r,i.currentTarget=g;try{a(i)}catch(b){Bi(b)}i.currentTarget=null,a=c}}}}function W(e,t){var n=t[Za];n===void 0&&(n=t[Za]=new Set);var o=e+"__bubble";n.has(o)||(Dd(t,e,2,!1),n.add(o))}function Lr(e,t,n){var o=0;t&&(o|=4),Dd(n,e,o,t)}var Ea="_reactListening"+Math.random().toString(36).slice(2);function Br(e){if(!e[Ea]){e[Ea]=!0,kl.forEach(function(n){n!=="selectionchange"&&(ph.has(n)||Lr(n,!1,e),Lr(n,!0,e))});var t=e.nodeType===9?e:e.ownerDocument;t===null||t[Ea]||(t[Ea]=!0,Lr("selectionchange",!1,t))}}function Dd(e,t,n,o){switch(rp(t)){case 2:var i=Bh;break;case 8:i=Nh;break;default:i=tl}n=i.bind(null,t,n,e),i=void 0,!ss||t!=="touchstart"&&t!=="touchmove"&&t!=="wheel"||(i=!0),o?i!==void 0?e.addEventListener(t,n,{capture:!0,passive:i}):e.addEventListener(t,n,!0):i!==void 0?e.addEventListener(t,n,{passive:i}):e.addEventListener(t,n,!1)}function Nr(e,t,n,o,i){var a=o;if((t&1)===0&&(t&2)===0&&o!==null)e:for(;;){if(o===null)return;var s=o.tag;if(s===3||s===4){var r=o.stateNode.containerInfo;if(r===i)break;if(s===4)for(s=o.return;s!==null;){var c=s.tag;if((c===3||c===4)&&s.stateNode.containerInfo===i)return;s=s.return}for(;r!==null;){if(s=_n(r),s===null)return;if(c=s.tag,c===5||c===6||c===26||c===27){o=a=s;continue e}r=r.parentNode}}o=o.return}jl(function(){var g=a,b=is(n),C=[];e:{var h=dc.get(e);if(h!==void 0){var y=ji,M=e;switch(e){case"keypress":if(Vi(n)===0)break e;case"keydown":case"keyup":y=ag;break;case"focusin":M="focus",y=us;break;case"focusout":M="blur",y=us;break;case"beforeblur":case"afterblur":y=us;break;case"click":if(n.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":y=Bl;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":y=_p;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":y=lg;break;case rc:case lc:case cc:y=Xp;break;case uc:y=ug;break;case"scroll":case"scrollend":y=Hp;break;case"wheel":y=pg;break;case"copy":case"cut":case"paste":y=Jp;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":y=Rl;break;case"toggle":case"beforetoggle":y=hg}var U=(t&4)!==0,de=!U&&(e==="scroll"||e==="scrollend"),d=U?h!==null?h+"Capture":null:h;U=[];for(var u=g,p;u!==null;){var w=u;if(p=w.stateNode,w=w.tag,w!==5&&w!==26&&w!==27||p===null||d===null||(w=Uo(u,d),w!=null&&U.push(mi(u,w,p))),de)break;u=u.return}0<U.length&&(h=new y(h,M,null,n,b),C.push({event:h,listeners:U}))}}if((t&7)===0){e:{if(h=e==="mouseover"||e==="pointerover",y=e==="mouseout"||e==="pointerout",h&&n!==os&&(M=n.relatedTarget||n.fromElement)&&(_n(M)||M[Wn]))break e;if((y||h)&&(h=b.window===b?b:(h=b.ownerDocument)?h.defaultView||h.parentWindow:window,y?(M=n.relatedTarget||n.toElement,y=g,M=M?_n(M):null,M!==null&&(de=_(M),U=M.tag,M!==de||U!==5&&U!==27&&U!==6)&&(M=null)):(y=null,M=g),y!==M)){if(U=Bl,w="onMouseLeave",d="onMouseEnter",u="mouse",(e==="pointerout"||e==="pointerover")&&(U=Rl,w="onPointerLeave",d="onPointerEnter",u="pointer"),de=y==null?h:Vo(y),p=M==null?h:Vo(M),h=new U(w,u+"leave",y,n,b),h.target=de,h.relatedTarget=p,w=null,_n(b)===g&&(U=new U(d,u+"enter",M,n,b),U.target=p,U.relatedTarget=de,w=U),de=w,y&&M)t:{for(U=gh,d=y,u=M,p=0,w=d;w;w=U(w))p++;w=0;for(var G=u;G;G=U(G))w++;for(;0<p-w;)d=U(d),p--;for(;0<w-p;)u=U(u),w--;for(;p--;){if(d===u||u!==null&&d===u.alternate){U=d;break t}d=U(d),u=U(u)}U=null}else U=null;y!==null&&Id(C,h,y,U,!1),M!==null&&de!==null&&Id(C,de,M,U,!0)}}e:{if(h=g?Vo(g):window,y=h.nodeName&&h.nodeName.toLowerCase(),y==="select"||y==="input"&&h.type==="file")var ee=Xl;else if(Kl(h))if(Zl)ee=Ag;else{ee=Sg;var q=Cg}else y=h.nodeName,!y||y.toLowerCase()!=="input"||h.type!=="checkbox"&&h.type!=="radio"?g&&ns(g.elementType)&&(ee=Xl):ee=xg;if(ee&&(ee=ee(e,g))){Fl(C,ee,n,b);break e}q&&q(e,h,g),e==="focusout"&&g&&h.type==="number"&&g.memoizedProps.value!=null&&ts(h,"number",h.value)}switch(q=g?Vo(g):window,e){case"focusin":(Kl(q)||q.contentEditable==="true")&&(no=q,fs=g,Qo=null);break;case"focusout":Qo=fs=no=null;break;case"mousedown":ys=!0;break;case"contextmenu":case"mouseup":case"dragend":ys=!1,ac(C,n,b);break;case"selectionchange":if(Eg)break;case"keydown":case"keyup":ac(C,n,b)}var B;if(ps)e:{switch(e){case"compositionstart":var F="onCompositionStart";break e;case"compositionend":F="onCompositionEnd";break e;case"compositionupdate":F="onCompositionUpdate";break e}F=void 0}else to?Wl(e,n)&&(F="onCompositionEnd"):e==="keydown"&&n.keyCode===229&&(F="onCompositionStart");F&&(Ol&&n.locale!=="ko"&&(to||F!=="onCompositionStart"?F==="onCompositionEnd"&&to&&(B=Yl()):(nn=b,rs="value"in nn?nn.value:nn.textContent,to=!0)),q=Ta(g,F),0<q.length&&(F=new Nl(F,e,null,n,b),C.push({event:F,listeners:q}),B?F.data=B:(B=_l(n),B!==null&&(F.data=B)))),(B=fg?yg(e,n):bg(e,n))&&(F=Ta(g,"onBeforeInput"),0<F.length&&(q=new Nl("onBeforeInput","beforeinput",null,n,b),C.push({event:q,listeners:F}),q.data=B)),ch(C,e,g,n,b)}zd(C,t)})}function mi(e,t,n){return{instance:e,listener:t,currentTarget:n}}function Ta(e,t){for(var n=t+"Capture",o=[];e!==null;){var i=e,a=i.stateNode;if(i=i.tag,i!==5&&i!==26&&i!==27||a===null||(i=Uo(e,n),i!=null&&o.unshift(mi(e,i,a)),i=Uo(e,t),i!=null&&o.push(mi(e,i,a))),e.tag===3)return o;e=e.return}return[]}function gh(e){if(e===null)return null;do e=e.return;while(e&&e.tag!==5&&e.tag!==27);return e||null}function Id(e,t,n,o,i){for(var a=t._reactName,s=[];n!==null&&n!==o;){var r=n,c=r.alternate,g=r.stateNode;if(r=r.tag,c!==null&&c===o)break;r!==5&&r!==26&&r!==27||g===null||(c=g,i?(g=Uo(n,a),g!=null&&s.unshift(mi(n,g,c))):i||(g=Uo(n,a),g!=null&&s.push(mi(n,g,c)))),n=n.return}s.length!==0&&e.push({event:t,listeners:s})}var hh=/\r\n?/g,mh=/\u0000|\uFFFD/g;function Gd(e){return(typeof e=="string"?e:""+e).replace(hh,`
`).replace(mh,"")}function Vd(e,t){return t=Gd(t),Gd(e)===t}function ue(e,t,n,o,i,a){switch(n){case"children":typeof o=="string"?t==="body"||t==="textarea"&&o===""||Jn(e,o):(typeof o=="number"||typeof o=="bigint")&&t!=="body"&&Jn(e,""+o);break;case"className":zi(e,"class",o);break;case"tabIndex":zi(e,"tabindex",o);break;case"dir":case"role":case"viewBox":case"width":case"height":zi(e,n,o);break;case"style":Vl(e,o,a);break;case"data":if(t!=="object"){zi(e,"data",o);break}case"src":case"href":if(o===""&&(t!=="a"||n!=="href")){e.removeAttribute(n);break}if(o==null||typeof o=="function"||typeof o=="symbol"||typeof o=="boolean"){e.removeAttribute(n);break}o=Ii(""+o),e.setAttribute(n,o);break;case"action":case"formAction":if(typeof o=="function"){e.setAttribute(n,"javascript:throw new Error('A React form was unexpectedly submitted. If you called form.submit() manually, consider using form.requestSubmit() instead. If you\\'re trying to use event.stopPropagation() in a submit event handler, consider also calling event.preventDefault().')");break}else typeof a=="function"&&(n==="formAction"?(t!=="input"&&ue(e,t,"name",i.name,i,null),ue(e,t,"formEncType",i.formEncType,i,null),ue(e,t,"formMethod",i.formMethod,i,null),ue(e,t,"formTarget",i.formTarget,i,null)):(ue(e,t,"encType",i.encType,i,null),ue(e,t,"method",i.method,i,null),ue(e,t,"target",i.target,i,null)));if(o==null||typeof o=="symbol"||typeof o=="boolean"){e.removeAttribute(n);break}o=Ii(""+o),e.setAttribute(n,o);break;case"onClick":o!=null&&(e.onclick=jt);break;case"onScroll":o!=null&&W("scroll",e);break;case"onScrollEnd":o!=null&&W("scrollend",e);break;case"dangerouslySetInnerHTML":if(o!=null){if(typeof o!="object"||!("__html"in o))throw Error(m(61));if(n=o.__html,n!=null){if(i.children!=null)throw Error(m(60));e.innerHTML=n}}break;case"multiple":e.multiple=o&&typeof o!="function"&&typeof o!="symbol";break;case"muted":e.muted=o&&typeof o!="function"&&typeof o!="symbol";break;case"suppressContentEditableWarning":case"suppressHydrationWarning":case"defaultValue":case"defaultChecked":case"innerHTML":case"ref":break;case"autoFocus":break;case"xlinkHref":if(o==null||typeof o=="function"||typeof o=="boolean"||typeof o=="symbol"){e.removeAttribute("xlink:href");break}n=Ii(""+o),e.setAttributeNS("http://www.w3.org/1999/xlink","xlink:href",n);break;case"contentEditable":case"spellCheck":case"draggable":case"value":case"autoReverse":case"externalResourcesRequired":case"focusable":case"preserveAlpha":o!=null&&typeof o!="function"&&typeof o!="symbol"?e.setAttribute(n,""+o):e.removeAttribute(n);break;case"inert":case"allowFullScreen":case"async":case"autoPlay":case"controls":case"default":case"defer":case"disabled":case"disablePictureInPicture":case"disableRemotePlayback":case"formNoValidate":case"hidden":case"loop":case"noModule":case"noValidate":case"open":case"playsInline":case"readOnly":case"required":case"reversed":case"scoped":case"seamless":case"itemScope":o&&typeof o!="function"&&typeof o!="symbol"?e.setAttribute(n,""):e.removeAttribute(n);break;case"capture":case"download":o===!0?e.setAttribute(n,""):o!==!1&&o!=null&&typeof o!="function"&&typeof o!="symbol"?e.setAttribute(n,o):e.removeAttribute(n);break;case"cols":case"rows":case"size":case"span":o!=null&&typeof o!="function"&&typeof o!="symbol"&&!isNaN(o)&&1<=o?e.setAttribute(n,o):e.removeAttribute(n);break;case"rowSpan":case"start":o==null||typeof o=="function"||typeof o=="symbol"||isNaN(o)?e.removeAttribute(n):e.setAttribute(n,o);break;case"popover":W("beforetoggle",e),W("toggle",e),qi(e,"popover",o);break;case"xlinkActuate":Ut(e,"http://www.w3.org/1999/xlink","xlink:actuate",o);break;case"xlinkArcrole":Ut(e,"http://www.w3.org/1999/xlink","xlink:arcrole",o);break;case"xlinkRole":Ut(e,"http://www.w3.org/1999/xlink","xlink:role",o);break;case"xlinkShow":Ut(e,"http://www.w3.org/1999/xlink","xlink:show",o);break;case"xlinkTitle":Ut(e,"http://www.w3.org/1999/xlink","xlink:title",o);break;case"xlinkType":Ut(e,"http://www.w3.org/1999/xlink","xlink:type",o);break;case"xmlBase":Ut(e,"http://www.w3.org/XML/1998/namespace","xml:base",o);break;case"xmlLang":Ut(e,"http://www.w3.org/XML/1998/namespace","xml:lang",o);break;case"xmlSpace":Ut(e,"http://www.w3.org/XML/1998/namespace","xml:space",o);break;case"is":qi(e,"is",o);break;case"innerText":case"textContent":break;default:(!(2<n.length)||n[0]!=="o"&&n[0]!=="O"||n[1]!=="n"&&n[1]!=="N")&&(n=Op.get(n)||n,qi(e,n,o))}}function Rr(e,t,n,o,i,a){switch(n){case"style":Vl(e,o,a);break;case"dangerouslySetInnerHTML":if(o!=null){if(typeof o!="object"||!("__html"in o))throw Error(m(61));if(n=o.__html,n!=null){if(i.children!=null)throw Error(m(60));e.innerHTML=n}}break;case"children":typeof o=="string"?Jn(e,o):(typeof o=="number"||typeof o=="bigint")&&Jn(e,""+o);break;case"onScroll":o!=null&&W("scroll",e);break;case"onScrollEnd":o!=null&&W("scrollend",e);break;case"onClick":o!=null&&(e.onclick=jt);break;case"suppressContentEditableWarning":case"suppressHydrationWarning":case"innerHTML":case"ref":break;case"innerText":case"textContent":break;default:if(!El.hasOwnProperty(n))e:{if(n[0]==="o"&&n[1]==="n"&&(i=n.endsWith("Capture"),t=n.slice(2,i?n.length-7:void 0),a=e[_e]||null,a=a!=null?a[n]:null,typeof a=="function"&&e.removeEventListener(t,a,i),typeof o=="function")){typeof a!="function"&&a!==null&&(n in e?e[n]=null:e.hasAttribute(n)&&e.removeAttribute(n)),e.addEventListener(t,o,i);break e}n in e?e[n]=o:o===!0?e.setAttribute(n,""):qi(e,n,o)}}}function Le(e,t,n){switch(t){case"div":case"span":case"svg":case"path":case"a":case"g":case"p":case"li":break;case"img":W("error",e),W("load",e);var o=!1,i=!1,a;for(a in n)if(n.hasOwnProperty(a)){var s=n[a];if(s!=null)switch(a){case"src":o=!0;break;case"srcSet":i=!0;break;case"children":case"dangerouslySetInnerHTML":throw Error(m(137,t));default:ue(e,t,a,s,n,null)}}i&&ue(e,t,"srcSet",n.srcSet,n,null),o&&ue(e,t,"src",n.src,n,null);return;case"input":W("invalid",e);var r=a=s=i=null,c=null,g=null;for(o in n)if(n.hasOwnProperty(o)){var b=n[o];if(b!=null)switch(o){case"name":i=b;break;case"type":s=b;break;case"checked":c=b;break;case"defaultChecked":g=b;break;case"value":a=b;break;case"defaultValue":r=b;break;case"children":case"dangerouslySetInnerHTML":if(b!=null)throw Error(m(137,t));break;default:ue(e,t,o,b,n,null)}}zl(e,a,r,c,g,s,i,!1);return;case"select":W("invalid",e),o=s=a=null;for(i in n)if(n.hasOwnProperty(i)&&(r=n[i],r!=null))switch(i){case"value":a=r;break;case"defaultValue":s=r;break;case"multiple":o=r;default:ue(e,t,i,r,n,null)}t=a,n=s,e.multiple=!!o,t!=null?Zn(e,!!o,t,!1):n!=null&&Zn(e,!!o,n,!0);return;case"textarea":W("invalid",e),a=i=o=null;for(s in n)if(n.hasOwnProperty(s)&&(r=n[s],r!=null))switch(s){case"value":o=r;break;case"defaultValue":i=r;break;case"children":a=r;break;case"dangerouslySetInnerHTML":if(r!=null)throw Error(m(91));break;default:ue(e,t,s,r,n,null)}Il(e,o,i,a);return;case"option":for(c in n)if(n.hasOwnProperty(c)&&(o=n[c],o!=null))switch(c){case"selected":e.selected=o&&typeof o!="function"&&typeof o!="symbol";break;default:ue(e,t,c,o,n,null)}return;case"dialog":W("beforetoggle",e),W("toggle",e),W("cancel",e),W("close",e);break;case"iframe":case"object":W("load",e);break;case"video":case"audio":for(o=0;o<hi.length;o++)W(hi[o],e);break;case"image":W("error",e),W("load",e);break;case"details":W("toggle",e);break;case"embed":case"source":case"link":W("error",e),W("load",e);case"area":case"base":case"br":case"col":case"hr":case"keygen":case"meta":case"param":case"track":case"wbr":case"menuitem":for(g in n)if(n.hasOwnProperty(g)&&(o=n[g],o!=null))switch(g){case"children":case"dangerouslySetInnerHTML":throw Error(m(137,t));default:ue(e,t,g,o,n,null)}return;default:if(ns(t)){for(b in n)n.hasOwnProperty(b)&&(o=n[b],o!==void 0&&Rr(e,t,b,o,n,void 0));return}}for(r in n)n.hasOwnProperty(r)&&(o=n[r],o!=null&&ue(e,t,r,o,n,null))}function fh(e,t,n,o){switch(t){case"div":case"span":case"svg":case"path":case"a":case"g":case"p":case"li":break;case"input":var i=null,a=null,s=null,r=null,c=null,g=null,b=null;for(y in n){var C=n[y];if(n.hasOwnProperty(y)&&C!=null)switch(y){case"checked":break;case"value":break;case"defaultValue":c=C;default:o.hasOwnProperty(y)||ue(e,t,y,null,o,C)}}for(var h in o){var y=o[h];if(C=n[h],o.hasOwnProperty(h)&&(y!=null||C!=null))switch(h){case"type":a=y;break;case"name":i=y;break;case"checked":g=y;break;case"defaultChecked":b=y;break;case"value":s=y;break;case"defaultValue":r=y;break;case"children":case"dangerouslySetInnerHTML":if(y!=null)throw Error(m(137,t));break;default:y!==C&&ue(e,t,h,y,o,C)}}es(e,s,r,c,g,b,a,i);return;case"select":y=s=r=h=null;for(a in n)if(c=n[a],n.hasOwnProperty(a)&&c!=null)switch(a){case"value":break;case"multiple":y=c;default:o.hasOwnProperty(a)||ue(e,t,a,null,o,c)}for(i in o)if(a=o[i],c=n[i],o.hasOwnProperty(i)&&(a!=null||c!=null))switch(i){case"value":h=a;break;case"defaultValue":r=a;break;case"multiple":s=a;default:a!==c&&ue(e,t,i,a,o,c)}t=r,n=s,o=y,h!=null?Zn(e,!!n,h,!1):!!o!=!!n&&(t!=null?Zn(e,!!n,t,!0):Zn(e,!!n,n?[]:"",!1));return;case"textarea":y=h=null;for(r in n)if(i=n[r],n.hasOwnProperty(r)&&i!=null&&!o.hasOwnProperty(r))switch(r){case"value":break;case"children":break;default:ue(e,t,r,null,o,i)}for(s in o)if(i=o[s],a=n[s],o.hasOwnProperty(s)&&(i!=null||a!=null))switch(s){case"value":h=i;break;case"defaultValue":y=i;break;case"children":break;case"dangerouslySetInnerHTML":if(i!=null)throw Error(m(91));break;default:i!==a&&ue(e,t,s,i,o,a)}Dl(e,h,y);return;case"option":for(var M in n)if(h=n[M],n.hasOwnProperty(M)&&h!=null&&!o.hasOwnProperty(M))switch(M){case"selected":e.selected=!1;break;default:ue(e,t,M,null,o,h)}for(c in o)if(h=o[c],y=n[c],o.hasOwnProperty(c)&&h!==y&&(h!=null||y!=null))switch(c){case"selected":e.selected=h&&typeof h!="function"&&typeof h!="symbol";break;default:ue(e,t,c,h,o,y)}return;case"img":case"link":case"area":case"base":case"br":case"col":case"embed":case"hr":case"keygen":case"meta":case"param":case"source":case"track":case"wbr":case"menuitem":for(var U in n)h=n[U],n.hasOwnProperty(U)&&h!=null&&!o.hasOwnProperty(U)&&ue(e,t,U,null,o,h);for(g in o)if(h=o[g],y=n[g],o.hasOwnProperty(g)&&h!==y&&(h!=null||y!=null))switch(g){case"children":case"dangerouslySetInnerHTML":if(h!=null)throw Error(m(137,t));break;default:ue(e,t,g,h,o,y)}return;default:if(ns(t)){for(var de in n)h=n[de],n.hasOwnProperty(de)&&h!==void 0&&!o.hasOwnProperty(de)&&Rr(e,t,de,void 0,o,h);for(b in o)h=o[b],y=n[b],!o.hasOwnProperty(b)||h===y||h===void 0&&y===void 0||Rr(e,t,b,h,o,y);return}}for(var d in n)h=n[d],n.hasOwnProperty(d)&&h!=null&&!o.hasOwnProperty(d)&&ue(e,t,d,null,o,h);for(C in o)h=o[C],y=n[C],!o.hasOwnProperty(C)||h===y||h==null&&y==null||ue(e,t,C,h,o,y)}function Ud(e){switch(e){case"css":case"script":case"font":case"img":case"image":case"input":case"link":return!0;default:return!1}}function yh(){if(typeof performance.getEntriesByType=="function"){for(var e=0,t=0,n=performance.getEntriesByType("resource"),o=0;o<n.length;o++){var i=n[o],a=i.transferSize,s=i.initiatorType,r=i.duration;if(a&&r&&Ud(s)){for(s=0,r=i.responseEnd,o+=1;o<n.length;o++){var c=n[o],g=c.startTime;if(g>r)break;var b=c.transferSize,C=c.initiatorType;b&&Ud(C)&&(c=c.responseEnd,s+=b*(c<r?1:(r-g)/(c-g)))}if(--o,t+=8*(a+s)/(i.duration/1e3),e++,10<e)break}}if(0<e)return t/e/1e6}return navigator.connection&&(e=navigator.connection.downlink,typeof e=="number")?e:5}var Or=null,Qr=null;function Ma(e){return e.nodeType===9?e:e.ownerDocument}function jd(e){switch(e){case"http://www.w3.org/2000/svg":return 1;case"http://www.w3.org/1998/Math/MathML":return 2;default:return 0}}function Yd(e,t){if(e===0)switch(t){case"svg":return 1;case"math":return 2;default:return 0}return e===1&&t==="foreignObject"?0:e}function Hr(e,t){return e==="textarea"||e==="noscript"||typeof t.children=="string"||typeof t.children=="number"||typeof t.children=="bigint"||typeof t.dangerouslySetInnerHTML=="object"&&t.dangerouslySetInnerHTML!==null&&t.dangerouslySetInnerHTML.__html!=null}var Wr=null;function bh(){var e=window.event;return e&&e.type==="popstate"?e===Wr?!1:(Wr=e,!0):(Wr=null,!1)}var Ld=typeof setTimeout=="function"?setTimeout:void 0,vh=typeof clearTimeout=="function"?clearTimeout:void 0,Bd=typeof Promise=="function"?Promise:void 0,wh=typeof queueMicrotask=="function"?queueMicrotask:typeof Bd<"u"?function(e){return Bd.resolve(null).then(e).catch(Ch)}:Ld;function Ch(e){setTimeout(function(){throw e})}function wn(e){return e==="head"}function Nd(e,t){var n=t,o=0;do{var i=n.nextSibling;if(e.removeChild(n),i&&i.nodeType===8)if(n=i.data,n==="/$"||n==="/&"){if(o===0){e.removeChild(i),qo(t);return}o--}else if(n==="$"||n==="$?"||n==="$~"||n==="$!"||n==="&")o++;else if(n==="html")fi(e.ownerDocument.documentElement);else if(n==="head"){n=e.ownerDocument.head,fi(n);for(var a=n.firstChild;a;){var s=a.nextSibling,r=a.nodeName;a[Go]||r==="SCRIPT"||r==="STYLE"||r==="LINK"&&a.rel.toLowerCase()==="stylesheet"||n.removeChild(a),a=s}}else n==="body"&&fi(e.ownerDocument.body);n=i}while(n);qo(t)}function Rd(e,t){var n=e;e=0;do{var o=n.nextSibling;if(n.nodeType===1?t?(n._stashedDisplay=n.style.display,n.style.display="none"):(n.style.display=n._stashedDisplay||"",n.getAttribute("style")===""&&n.removeAttribute("style")):n.nodeType===3&&(t?(n._stashedText=n.nodeValue,n.nodeValue=""):n.nodeValue=n._stashedText||""),o&&o.nodeType===8)if(n=o.data,n==="/$"){if(e===0)break;e--}else n!=="$"&&n!=="$?"&&n!=="$~"&&n!=="$!"||e++;n=o}while(n)}function _r(e){var t=e.firstChild;for(t&&t.nodeType===10&&(t=t.nextSibling);t;){var n=t;switch(t=t.nextSibling,n.nodeName){case"HTML":case"HEAD":case"BODY":_r(n),Ja(n);continue;case"SCRIPT":case"STYLE":continue;case"LINK":if(n.rel.toLowerCase()==="stylesheet")continue}e.removeChild(n)}}function Sh(e,t,n,o){for(;e.nodeType===1;){var i=n;if(e.nodeName.toLowerCase()!==t.toLowerCase()){if(!o&&(e.nodeName!=="INPUT"||e.type!=="hidden"))break}else if(o){if(!e[Go])switch(t){case"meta":if(!e.hasAttribute("itemprop"))break;return e;case"link":if(a=e.getAttribute("rel"),a==="stylesheet"&&e.hasAttribute("data-precedence"))break;if(a!==i.rel||e.getAttribute("href")!==(i.href==null||i.href===""?null:i.href)||e.getAttribute("crossorigin")!==(i.crossOrigin==null?null:i.crossOrigin)||e.getAttribute("title")!==(i.title==null?null:i.title))break;return e;case"style":if(e.hasAttribute("data-precedence"))break;return e;case"script":if(a=e.getAttribute("src"),(a!==(i.src==null?null:i.src)||e.getAttribute("type")!==(i.type==null?null:i.type)||e.getAttribute("crossorigin")!==(i.crossOrigin==null?null:i.crossOrigin))&&a&&e.hasAttribute("async")&&!e.hasAttribute("itemprop"))break;return e;default:return e}}else if(t==="input"&&e.type==="hidden"){var a=i.name==null?null:""+i.name;if(i.type==="hidden"&&e.getAttribute("name")===a)return e}else return e;if(e=xt(e.nextSibling),e===null)break}return null}function xh(e,t,n){if(t==="")return null;for(;e.nodeType!==3;)if((e.nodeType!==1||e.nodeName!=="INPUT"||e.type!=="hidden")&&!n||(e=xt(e.nextSibling),e===null))return null;return e}function Od(e,t){for(;e.nodeType!==8;)if((e.nodeType!==1||e.nodeName!=="INPUT"||e.type!=="hidden")&&!t||(e=xt(e.nextSibling),e===null))return null;return e}function Kr(e){return e.data==="$?"||e.data==="$~"}function Fr(e){return e.data==="$!"||e.data==="$?"&&e.ownerDocument.readyState!=="loading"}function Ah(e,t){var n=e.ownerDocument;if(e.data==="$~")e._reactRetry=t;else if(e.data!=="$?"||n.readyState!=="loading")t();else{var o=function(){t(),n.removeEventListener("DOMContentLoaded",o)};n.addEventListener("DOMContentLoaded",o),e._reactRetry=o}}function xt(e){for(;e!=null;e=e.nextSibling){var t=e.nodeType;if(t===1||t===3)break;if(t===8){if(t=e.data,t==="$"||t==="$!"||t==="$?"||t==="$~"||t==="&"||t==="F!"||t==="F")break;if(t==="/$"||t==="/&")return null}}return e}var Xr=null;function Qd(e){e=e.nextSibling;for(var t=0;e;){if(e.nodeType===8){var n=e.data;if(n==="/$"||n==="/&"){if(t===0)return xt(e.nextSibling);t--}else n!=="$"&&n!=="$!"&&n!=="$?"&&n!=="$~"&&n!=="&"||t++}e=e.nextSibling}return null}function Hd(e){e=e.previousSibling;for(var t=0;e;){if(e.nodeType===8){var n=e.data;if(n==="$"||n==="$!"||n==="$?"||n==="$~"||n==="&"){if(t===0)return e;t--}else n!=="/$"&&n!=="/&"||t++}e=e.previousSibling}return null}function Wd(e,t,n){switch(t=Ma(n),e){case"html":if(e=t.documentElement,!e)throw Error(m(452));return e;case"head":if(e=t.head,!e)throw Error(m(453));return e;case"body":if(e=t.body,!e)throw Error(m(454));return e;default:throw Error(m(451))}}function fi(e){for(var t=e.attributes;t.length;)e.removeAttributeNode(t[0]);Ja(e)}var At=new Map,_d=new Set;function Pa(e){return typeof e.getRootNode=="function"?e.getRootNode():e.nodeType===9?e:e.ownerDocument}var $t=T.d;T.d={f:kh,r:Eh,D:Th,C:Mh,L:Ph,m:qh,X:Dh,S:zh,M:Ih};function kh(){var e=$t.f(),t=wa();return e||t}function Eh(e){var t=Kn(e);t!==null&&t.tag===5&&t.type==="form"?uu(t):$t.r(e)}var To=typeof document>"u"?null:document;function Kd(e,t,n){var o=To;if(o&&typeof t=="string"&&t){var i=ft(t);i='link[rel="'+e+'"][href="'+i+'"]',typeof n=="string"&&(i+='[crossorigin="'+n+'"]'),_d.has(i)||(_d.add(i),e={rel:e,crossOrigin:n,href:t},o.querySelector(i)===null&&(t=o.createElement("link"),Le(t,"link",e),De(t),o.head.appendChild(t)))}}function Th(e){$t.D(e),Kd("dns-prefetch",e,null)}function Mh(e,t){$t.C(e,t),Kd("preconnect",e,t)}function Ph(e,t,n){$t.L(e,t,n);var o=To;if(o&&e&&t){var i='link[rel="preload"][as="'+ft(t)+'"]';t==="image"&&n&&n.imageSrcSet?(i+='[imagesrcset="'+ft(n.imageSrcSet)+'"]',typeof n.imageSizes=="string"&&(i+='[imagesizes="'+ft(n.imageSizes)+'"]')):i+='[href="'+ft(e)+'"]';var a=i;switch(t){case"style":a=Mo(e);break;case"script":a=Po(e)}At.has(a)||(e=V({rel:"preload",href:t==="image"&&n&&n.imageSrcSet?void 0:e,as:t},n),At.set(a,e),o.querySelector(i)!==null||t==="style"&&o.querySelector(yi(a))||t==="script"&&o.querySelector(bi(a))||(t=o.createElement("link"),Le(t,"link",e),De(t),o.head.appendChild(t)))}}function qh(e,t){$t.m(e,t);var n=To;if(n&&e){var o=t&&typeof t.as=="string"?t.as:"script",i='link[rel="modulepreload"][as="'+ft(o)+'"][href="'+ft(e)+'"]',a=i;switch(o){case"audioworklet":case"paintworklet":case"serviceworker":case"sharedworker":case"worker":case"script":a=Po(e)}if(!At.has(a)&&(e=V({rel:"modulepreload",href:e},t),At.set(a,e),n.querySelector(i)===null)){switch(o){case"audioworklet":case"paintworklet":case"serviceworker":case"sharedworker":case"worker":case"script":if(n.querySelector(bi(a)))return}o=n.createElement("link"),Le(o,"link",e),De(o),n.head.appendChild(o)}}}function zh(e,t,n){$t.S(e,t,n);var o=To;if(o&&e){var i=Fn(o).hoistableStyles,a=Mo(e);t=t||"default";var s=i.get(a);if(!s){var r={loading:0,preload:null};if(s=o.querySelector(yi(a)))r.loading=5;else{e=V({rel:"stylesheet",href:e,"data-precedence":t},n),(n=At.get(a))&&Zr(e,n);var c=s=o.createElement("link");De(c),Le(c,"link",e),c._p=new Promise(function(g,b){c.onload=g,c.onerror=b}),c.addEventListener("load",function(){r.loading|=1}),c.addEventListener("error",function(){r.loading|=2}),r.loading|=4,qa(s,t,o)}s={type:"stylesheet",instance:s,count:1,state:r},i.set(a,s)}}}function Dh(e,t){$t.X(e,t);var n=To;if(n&&e){var o=Fn(n).hoistableScripts,i=Po(e),a=o.get(i);a||(a=n.querySelector(bi(i)),a||(e=V({src:e,async:!0},t),(t=At.get(i))&&Jr(e,t),a=n.createElement("script"),De(a),Le(a,"link",e),n.head.appendChild(a)),a={type:"script",instance:a,count:1,state:null},o.set(i,a))}}function Ih(e,t){$t.M(e,t);var n=To;if(n&&e){var o=Fn(n).hoistableScripts,i=Po(e),a=o.get(i);a||(a=n.querySelector(bi(i)),a||(e=V({src:e,async:!0,type:"module"},t),(t=At.get(i))&&Jr(e,t),a=n.createElement("script"),De(a),Le(a,"link",e),n.head.appendChild(a)),a={type:"script",instance:a,count:1,state:null},o.set(i,a))}}function Fd(e,t,n,o){var i=(i=I.current)?Pa(i):null;if(!i)throw Error(m(446));switch(e){case"meta":case"title":return null;case"style":return typeof n.precedence=="string"&&typeof n.href=="string"?(t=Mo(n.href),n=Fn(i).hoistableStyles,o=n.get(t),o||(o={type:"style",instance:null,count:0,state:null},n.set(t,o)),o):{type:"void",instance:null,count:0,state:null};case"link":if(n.rel==="stylesheet"&&typeof n.href=="string"&&typeof n.precedence=="string"){e=Mo(n.href);var a=Fn(i).hoistableStyles,s=a.get(e);if(s||(i=i.ownerDocument||i,s={type:"stylesheet",instance:null,count:0,state:{loading:0,preload:null}},a.set(e,s),(a=i.querySelector(yi(e)))&&!a._p&&(s.instance=a,s.state.loading=5),At.has(e)||(n={rel:"preload",as:"style",href:n.href,crossOrigin:n.crossOrigin,integrity:n.integrity,media:n.media,hrefLang:n.hrefLang,referrerPolicy:n.referrerPolicy},At.set(e,n),a||Gh(i,e,n,s.state))),t&&o===null)throw Error(m(528,""));return s}if(t&&o!==null)throw Error(m(529,""));return null;case"script":return t=n.async,n=n.src,typeof n=="string"&&t&&typeof t!="function"&&typeof t!="symbol"?(t=Po(n),n=Fn(i).hoistableScripts,o=n.get(t),o||(o={type:"script",instance:null,count:0,state:null},n.set(t,o)),o):{type:"void",instance:null,count:0,state:null};default:throw Error(m(444,e))}}function Mo(e){return'href="'+ft(e)+'"'}function yi(e){return'link[rel="stylesheet"]['+e+"]"}function Xd(e){return V({},e,{"data-precedence":e.precedence,precedence:null})}function Gh(e,t,n,o){e.querySelector('link[rel="preload"][as="style"]['+t+"]")?o.loading=1:(t=e.createElement("link"),o.preload=t,t.addEventListener("load",function(){return o.loading|=1}),t.addEventListener("error",function(){return o.loading|=2}),Le(t,"link",n),De(t),e.head.appendChild(t))}function Po(e){return'[src="'+ft(e)+'"]'}function bi(e){return"script[async]"+e}function Zd(e,t,n){if(t.count++,t.instance===null)switch(t.type){case"style":var o=e.querySelector('style[data-href~="'+ft(n.href)+'"]');if(o)return t.instance=o,De(o),o;var i=V({},n,{"data-href":n.href,"data-precedence":n.precedence,href:null,precedence:null});return o=(e.ownerDocument||e).createElement("style"),De(o),Le(o,"style",i),qa(o,n.precedence,e),t.instance=o;case"stylesheet":i=Mo(n.href);var a=e.querySelector(yi(i));if(a)return t.state.loading|=4,t.instance=a,De(a),a;o=Xd(n),(i=At.get(i))&&Zr(o,i),a=(e.ownerDocument||e).createElement("link"),De(a);var s=a;return s._p=new Promise(function(r,c){s.onload=r,s.onerror=c}),Le(a,"link",o),t.state.loading|=4,qa(a,n.precedence,e),t.instance=a;case"script":return a=Po(n.src),(i=e.querySelector(bi(a)))?(t.instance=i,De(i),i):(o=n,(i=At.get(a))&&(o=V({},n),Jr(o,i)),e=e.ownerDocument||e,i=e.createElement("script"),De(i),Le(i,"link",o),e.head.appendChild(i),t.instance=i);case"void":return null;default:throw Error(m(443,t.type))}else t.type==="stylesheet"&&(t.state.loading&4)===0&&(o=t.instance,t.state.loading|=4,qa(o,n.precedence,e));return t.instance}function qa(e,t,n){for(var o=n.querySelectorAll('link[rel="stylesheet"][data-precedence],style[data-precedence]'),i=o.length?o[o.length-1]:null,a=i,s=0;s<o.length;s++){var r=o[s];if(r.dataset.precedence===t)a=r;else if(a!==i)break}a?a.parentNode.insertBefore(e,a.nextSibling):(t=n.nodeType===9?n.head:n,t.insertBefore(e,t.firstChild))}function Zr(e,t){e.crossOrigin==null&&(e.crossOrigin=t.crossOrigin),e.referrerPolicy==null&&(e.referrerPolicy=t.referrerPolicy),e.title==null&&(e.title=t.title)}function Jr(e,t){e.crossOrigin==null&&(e.crossOrigin=t.crossOrigin),e.referrerPolicy==null&&(e.referrerPolicy=t.referrerPolicy),e.integrity==null&&(e.integrity=t.integrity)}var za=null;function Jd(e,t,n){if(za===null){var o=new Map,i=za=new Map;i.set(n,o)}else i=za,o=i.get(n),o||(o=new Map,i.set(n,o));if(o.has(e))return o;for(o.set(e,null),n=n.getElementsByTagName(e),i=0;i<n.length;i++){var a=n[i];if(!(a[Go]||a[Ve]||e==="link"&&a.getAttribute("rel")==="stylesheet")&&a.namespaceURI!=="http://www.w3.org/2000/svg"){var s=a.getAttribute(t)||"";s=e+s;var r=o.get(s);r?r.push(a):o.set(s,[a])}}return o}function $d(e,t,n){e=e.ownerDocument||e,e.head.insertBefore(n,t==="title"?e.querySelector("head > title"):null)}function Vh(e,t,n){if(n===1||t.itemProp!=null)return!1;switch(e){case"meta":case"title":return!0;case"style":if(typeof t.precedence!="string"||typeof t.href!="string"||t.href==="")break;return!0;case"link":if(typeof t.rel!="string"||typeof t.href!="string"||t.href===""||t.onLoad||t.onError)break;switch(t.rel){case"stylesheet":return e=t.disabled,typeof t.precedence=="string"&&e==null;default:return!0}case"script":if(t.async&&typeof t.async!="function"&&typeof t.async!="symbol"&&!t.onLoad&&!t.onError&&t.src&&typeof t.src=="string")return!0}return!1}function ep(e){return!(e.type==="stylesheet"&&(e.state.loading&3)===0)}function Uh(e,t,n,o){if(n.type==="stylesheet"&&(typeof o.media!="string"||matchMedia(o.media).matches!==!1)&&(n.state.loading&4)===0){if(n.instance===null){var i=Mo(o.href),a=t.querySelector(yi(i));if(a){t=a._p,t!==null&&typeof t=="object"&&typeof t.then=="function"&&(e.count++,e=Da.bind(e),t.then(e,e)),n.state.loading|=4,n.instance=a,De(a);return}a=t.ownerDocument||t,o=Xd(o),(i=At.get(i))&&Zr(o,i),a=a.createElement("link"),De(a);var s=a;s._p=new Promise(function(r,c){s.onload=r,s.onerror=c}),Le(a,"link",o),n.instance=a}e.stylesheets===null&&(e.stylesheets=new Map),e.stylesheets.set(n,t),(t=n.state.preload)&&(n.state.loading&3)===0&&(e.count++,n=Da.bind(e),t.addEventListener("load",n),t.addEventListener("error",n))}}var $r=0;function jh(e,t){return e.stylesheets&&e.count===0&&Ga(e,e.stylesheets),0<e.count||0<e.imgCount?function(n){var o=setTimeout(function(){if(e.stylesheets&&Ga(e,e.stylesheets),e.unsuspend){var a=e.unsuspend;e.unsuspend=null,a()}},6e4+t);0<e.imgBytes&&$r===0&&($r=62500*yh());var i=setTimeout(function(){if(e.waitingForImages=!1,e.count===0&&(e.stylesheets&&Ga(e,e.stylesheets),e.unsuspend)){var a=e.unsuspend;e.unsuspend=null,a()}},(e.imgBytes>$r?50:800)+t);return e.unsuspend=n,function(){e.unsuspend=null,clearTimeout(o),clearTimeout(i)}}:null}function Da(){if(this.count--,this.count===0&&(this.imgCount===0||!this.waitingForImages)){if(this.stylesheets)Ga(this,this.stylesheets);else if(this.unsuspend){var e=this.unsuspend;this.unsuspend=null,e()}}}var Ia=null;function Ga(e,t){e.stylesheets=null,e.unsuspend!==null&&(e.count++,Ia=new Map,t.forEach(Yh,e),Ia=null,Da.call(e))}function Yh(e,t){if(!(t.state.loading&4)){var n=Ia.get(e);if(n)var o=n.get(null);else{n=new Map,Ia.set(e,n);for(var i=e.querySelectorAll("link[data-precedence],style[data-precedence]"),a=0;a<i.length;a++){var s=i[a];(s.nodeName==="LINK"||s.getAttribute("media")!=="not all")&&(n.set(s.dataset.precedence,s),o=s)}o&&n.set(null,o)}i=t.instance,s=i.getAttribute("data-precedence"),a=n.get(s)||o,a===o&&n.set(null,i),n.set(s,i),this.count++,o=Da.bind(this),i.addEventListener("load",o),i.addEventListener("error",o),a?a.parentNode.insertBefore(i,a.nextSibling):(e=e.nodeType===9?e.head:e,e.insertBefore(i,e.firstChild)),t.state.loading|=4}}var vi={$$typeof:ve,Provider:null,Consumer:null,_currentValue:j,_currentValue2:j,_threadCount:0};function Lh(e,t,n,o,i,a,s,r,c){this.tag=1,this.containerInfo=e,this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.next=this.pendingContext=this.context=this.cancelPendingCommit=null,this.callbackPriority=0,this.expirationTimes=Ka(-1),this.entangledLanes=this.shellSuspendCounter=this.errorRecoveryDisabledLanes=this.expiredLanes=this.warmLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=Ka(0),this.hiddenUpdates=Ka(null),this.identifierPrefix=o,this.onUncaughtError=i,this.onCaughtError=a,this.onRecoverableError=s,this.pooledCache=null,this.pooledCacheLanes=0,this.formState=c,this.incompleteTransitions=new Map}function tp(e,t,n,o,i,a,s,r,c,g,b,C){return e=new Lh(e,t,n,s,c,g,b,C,r),t=1,a===!0&&(t|=24),a=lt(3,null,null,t),e.current=a,a.stateNode=e,t=Ds(),t.refCount++,e.pooledCache=t,t.refCount++,a.memoizedState={element:o,isDehydrated:n,cache:t},Us(a),e}function np(e){return e?(e=ao,e):ao}function op(e,t,n,o,i,a){i=np(i),o.context===null?o.context=i:o.pendingContext=i,o=cn(t),o.payload={element:n},a=a===void 0?null:a,a!==null&&(o.callback=a),n=un(e,o,t),n!==null&&($e(n,e,t),Zo(n,e,t))}function ip(e,t){if(e=e.memoizedState,e!==null&&e.dehydrated!==null){var n=e.retryLane;e.retryLane=n!==0&&n<t?n:t}}function el(e,t){ip(e,t),(e=e.alternate)&&ip(e,t)}function ap(e){if(e.tag===13||e.tag===31){var t=In(e,67108864);t!==null&&$e(t,e,67108864),el(e,67108864)}}function sp(e){if(e.tag===13||e.tag===31){var t=gt();t=Fa(t);var n=In(e,t);n!==null&&$e(n,e,t),el(e,t)}}var Va=!0;function Bh(e,t,n,o){var i=v.T;v.T=null;var a=T.p;try{T.p=2,tl(e,t,n,o)}finally{T.p=a,v.T=i}}function Nh(e,t,n,o){var i=v.T;v.T=null;var a=T.p;try{T.p=8,tl(e,t,n,o)}finally{T.p=a,v.T=i}}function tl(e,t,n,o){if(Va){var i=nl(o);if(i===null)Nr(e,t,o,Ua,n),lp(e,o);else if(Oh(i,e,t,n,o))o.stopPropagation();else if(lp(e,o),t&4&&-1<Rh.indexOf(e)){for(;i!==null;){var a=Kn(i);if(a!==null)switch(a.tag){case 3:if(a=a.stateNode,a.current.memoizedState.isDehydrated){var s=Mn(a.pendingLanes);if(s!==0){var r=a;for(r.pendingLanes|=2,r.entangledLanes|=2;s;){var c=1<<31-st(s);r.entanglements[1]|=c,s&=~c}Dt(a),(ie&6)===0&&(ba=it()+500,gi(0))}}break;case 31:case 13:r=In(a,2),r!==null&&$e(r,a,2),wa(),el(a,2)}if(a=nl(o),a===null&&Nr(e,t,o,Ua,n),a===i)break;i=a}i!==null&&o.stopPropagation()}else Nr(e,t,o,null,n)}}function nl(e){return e=is(e),ol(e)}var Ua=null;function ol(e){if(Ua=null,e=_n(e),e!==null){var t=_(e);if(t===null)e=null;else{var n=t.tag;if(n===13){if(e=J(t),e!==null)return e;e=null}else if(n===31){if(e=Se(t),e!==null)return e;e=null}else if(n===3){if(t.stateNode.current.memoizedState.isDehydrated)return t.tag===3?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null)}}return Ua=e,null}function rp(e){switch(e){case"beforetoggle":case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"toggle":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 2;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 8;case"message":switch(Tp()){case ml:return 2;case fl:return 8;case ki:case Mp:return 32;case yl:return 268435456;default:return 32}default:return 32}}var il=!1,Cn=null,Sn=null,xn=null,wi=new Map,Ci=new Map,An=[],Rh="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset".split(" ");function lp(e,t){switch(e){case"focusin":case"focusout":Cn=null;break;case"dragenter":case"dragleave":Sn=null;break;case"mouseover":case"mouseout":xn=null;break;case"pointerover":case"pointerout":wi.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":Ci.delete(t.pointerId)}}function Si(e,t,n,o,i,a){return e===null||e.nativeEvent!==a?(e={blockedOn:t,domEventName:n,eventSystemFlags:o,nativeEvent:a,targetContainers:[i]},t!==null&&(t=Kn(t),t!==null&&ap(t)),e):(e.eventSystemFlags|=o,t=e.targetContainers,i!==null&&t.indexOf(i)===-1&&t.push(i),e)}function Oh(e,t,n,o,i){switch(t){case"focusin":return Cn=Si(Cn,e,t,n,o,i),!0;case"dragenter":return Sn=Si(Sn,e,t,n,o,i),!0;case"mouseover":return xn=Si(xn,e,t,n,o,i),!0;case"pointerover":var a=i.pointerId;return wi.set(a,Si(wi.get(a)||null,e,t,n,o,i)),!0;case"gotpointercapture":return a=i.pointerId,Ci.set(a,Si(Ci.get(a)||null,e,t,n,o,i)),!0}return!1}function cp(e){var t=_n(e.target);if(t!==null){var n=_(t);if(n!==null){if(t=n.tag,t===13){if(t=J(n),t!==null){e.blockedOn=t,xl(e.priority,function(){sp(n)});return}}else if(t===31){if(t=Se(n),t!==null){e.blockedOn=t,xl(e.priority,function(){sp(n)});return}}else if(t===3&&n.stateNode.current.memoizedState.isDehydrated){e.blockedOn=n.tag===3?n.stateNode.containerInfo:null;return}}}e.blockedOn=null}function ja(e){if(e.blockedOn!==null)return!1;for(var t=e.targetContainers;0<t.length;){var n=nl(e.nativeEvent);if(n===null){n=e.nativeEvent;var o=new n.constructor(n.type,n);os=o,n.target.dispatchEvent(o),os=null}else return t=Kn(n),t!==null&&ap(t),e.blockedOn=n,!1;t.shift()}return!0}function up(e,t,n){ja(e)&&n.delete(t)}function Qh(){il=!1,Cn!==null&&ja(Cn)&&(Cn=null),Sn!==null&&ja(Sn)&&(Sn=null),xn!==null&&ja(xn)&&(xn=null),wi.forEach(up),Ci.forEach(up)}function Ya(e,t){e.blockedOn===t&&(e.blockedOn=null,il||(il=!0,P.unstable_scheduleCallback(P.unstable_NormalPriority,Qh)))}var La=null;function dp(e){La!==e&&(La=e,P.unstable_scheduleCallback(P.unstable_NormalPriority,function(){La===e&&(La=null);for(var t=0;t<e.length;t+=3){var n=e[t],o=e[t+1],i=e[t+2];if(typeof o!="function"){if(ol(o||n)===null)continue;break}var a=Kn(n);a!==null&&(e.splice(t,3),t-=3,nr(a,{pending:!0,data:i,method:n.method,action:o},o,i))}}))}function qo(e){function t(c){return Ya(c,e)}Cn!==null&&Ya(Cn,e),Sn!==null&&Ya(Sn,e),xn!==null&&Ya(xn,e),wi.forEach(t),Ci.forEach(t);for(var n=0;n<An.length;n++){var o=An[n];o.blockedOn===e&&(o.blockedOn=null)}for(;0<An.length&&(n=An[0],n.blockedOn===null);)cp(n),n.blockedOn===null&&An.shift();if(n=(e.ownerDocument||e).$$reactFormReplay,n!=null)for(o=0;o<n.length;o+=3){var i=n[o],a=n[o+1],s=i[_e]||null;if(typeof a=="function")s||dp(n);else if(s){var r=null;if(a&&a.hasAttribute("formAction")){if(i=a,s=a[_e]||null)r=s.formAction;else if(ol(i)!==null)continue}else r=s.action;typeof r=="function"?n[o+1]=r:(n.splice(o,3),o-=3),dp(n)}}}function pp(){function e(a){a.canIntercept&&a.info==="react-transition"&&a.intercept({handler:function(){return new Promise(function(s){return i=s})},focusReset:"manual",scroll:"manual"})}function t(){i!==null&&(i(),i=null),o||setTimeout(n,20)}function n(){if(!o&&!navigation.transition){var a=navigation.currentEntry;a&&a.url!=null&&navigation.navigate(a.url,{state:a.getState(),info:"react-transition",history:"replace"})}}if(typeof navigation=="object"){var o=!1,i=null;return navigation.addEventListener("navigate",e),navigation.addEventListener("navigatesuccess",t),navigation.addEventListener("navigateerror",t),setTimeout(n,100),function(){o=!0,navigation.removeEventListener("navigate",e),navigation.removeEventListener("navigatesuccess",t),navigation.removeEventListener("navigateerror",t),i!==null&&(i(),i=null)}}}function al(e){this._internalRoot=e}Ba.prototype.render=al.prototype.render=function(e){var t=this._internalRoot;if(t===null)throw Error(m(409));var n=t.current,o=gt();op(n,o,e,t,null,null)},Ba.prototype.unmount=al.prototype.unmount=function(){var e=this._internalRoot;if(e!==null){this._internalRoot=null;var t=e.containerInfo;op(e.current,2,null,e,null,null),wa(),t[Wn]=null}};function Ba(e){this._internalRoot=e}Ba.prototype.unstable_scheduleHydration=function(e){if(e){var t=Sl();e={blockedOn:null,target:e,priority:t};for(var n=0;n<An.length&&t!==0&&t<An[n].priority;n++);An.splice(n,0,e),n===0&&cp(e)}};var gp=ae.version;if(gp!=="19.2.0")throw Error(m(527,gp,"19.2.0"));T.findDOMNode=function(e){var t=e._reactInternals;if(t===void 0)throw typeof e.render=="function"?Error(m(188)):(e=Object.keys(e).join(","),Error(m(268,e)));return e=x(t),e=e!==null?N(e):null,e=e===null?null:e.stateNode,e};var Hh={bundleType:0,version:"19.2.0",rendererPackageName:"react-dom",currentDispatcherRef:v,reconcilerVersion:"19.2.0"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var Na=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!Na.isDisabled&&Na.supportsFiber)try{zo=Na.inject(Hh),at=Na}catch{}}return Ai.createRoot=function(e,t){if(!R(e))throw Error(m(299));var n=!1,o="",i=wu,a=Cu,s=Su;return t!=null&&(t.unstable_strictMode===!0&&(n=!0),t.identifierPrefix!==void 0&&(o=t.identifierPrefix),t.onUncaughtError!==void 0&&(i=t.onUncaughtError),t.onCaughtError!==void 0&&(a=t.onCaughtError),t.onRecoverableError!==void 0&&(s=t.onRecoverableError)),t=tp(e,1,!1,null,null,n,o,null,i,a,s,pp),e[Wn]=t.current,Br(e),new al(t)},Ai.hydrateRoot=function(e,t,n){if(!R(e))throw Error(m(299));var o=!1,i="",a=wu,s=Cu,r=Su,c=null;return n!=null&&(n.unstable_strictMode===!0&&(o=!0),n.identifierPrefix!==void 0&&(i=n.identifierPrefix),n.onUncaughtError!==void 0&&(a=n.onUncaughtError),n.onCaughtError!==void 0&&(s=n.onCaughtError),n.onRecoverableError!==void 0&&(r=n.onRecoverableError),n.formState!==void 0&&(c=n.formState)),t=tp(e,1,!0,t,n??null,o,i,c,a,s,r,pp),t.context=np(null),n=t.current,o=gt(),o=Fa(o),i=cn(o),i.callback=null,un(n,i,o),n=o,t.current.lanes=n,Io(t,n),Dt(t),e[Wn]=t.current,Br(e),new Ba(t)},Ai.version="19.2.0",Ai}var xp;function tm(){if(xp)return ll.exports;xp=1;function P(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(P)}catch(ae){console.error(ae)}}return P(),ll.exports=em(),ll.exports}var nm=tm();const En=[{id:0,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"Your organization needs to manage multiple teams, each with several projects. The finance team requires cost tracking per team, and the security team needs to enforce policies at the team level. What resource hierarchy should you implement?",options:["Organization > Folders (per team) > Projects > Resources","Multiple Organizations, one per team, linked via VPC peering","Organization > Projects (no folders) > Resources","Individual Google accounts per team with shared billing"],correct:0,explanation:"Using folders per team allows cost tracking via labels/billing filters and enables policy enforcement at the folder level. This follows Google's recommended practice for enterprise resource management.",wrongExplanations:{1:"Multiple organizations cannot be linked through VPC peering and defeats the purpose of centralized management. Each organization would require separate billing and policy management.",2:"Without folders, you lose the ability to apply department-level policies and organizational structure. This makes management difficult as the number of projects grows.",3:"Individual Google accounts lack the enterprise features needed for centralized management, policy inheritance, and organizational controls required at scale."}},{id:1,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects and accounts - Gemini Cloud Assist",question:"You need to analyze your project's resource configuration to identify unused resources and security vulnerabilities. You want to use AI-powered assistance for this task. What should you do?",options:["Configure Cloud Asset Inventory and use Gemini Cloud Assist to analyze resources","Write custom scripts to query all APIs and analyze resource configurations","Use Cloud Console to manually review each resource type","Export all resources to BigQuery and write SQL queries"],correct:0,explanation:"Cloud Asset Inventory combined with Gemini Cloud Assist provides AI-powered analysis of your resources, identifying optimization opportunities, unused resources, and potential security issues. This is a new feature emphasized in the 2025 exam.",wrongExplanations:{1:"Custom scripts require significant development effort, are prone to errors, and lack the AI-powered insights that Gemini Cloud Assist provides automatically.",2:"Manual review doesn't scale, is time-consuming, and won't provide the intelligent recommendations that Gemini Cloud Assist offers.",3:"While BigQuery can store resource data, you'd still need to write complex queries and lack the AI-powered recommendations that Gemini provides."}},{id:2,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing configuration",question:"Your project needs to prevent cost overruns due to accidental resource provisioning. The finance team requires notifications at 50%, 75%, and 100% of the budget. What is the most effective approach?",options:["Set up billing budgets with alerts at 50%, 75%, and 100%, and configure quotas for compute resources","Monitor spending daily and manually stop resources when approaching budget","Set a spending limit on the credit card associated with the billing account","Create a Cloud Function to check costs hourly and delete resources if over budget"],correct:0,explanation:"Billing budgets provide proactive notifications at specified thresholds. Combined with quotas, this prevents accidental resource overprovisioning. Budgets alert but don't stop billing; quotas prevent resource creation beyond limits.",wrongExplanations:{1:"Manual monitoring doesn't scale, is error-prone, and won't provide timely alerts when costs spike unexpectedly.",2:"Credit card limits don't prevent resource creation; your card will incur charges and Google will bill you. This doesn't provide the granular control needed.",3:"Automatically deleting resources when over budget could cause production outages. Budgets with alerts allow informed decision-making before taking action."}},{id:3,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - GKE Autopilot",question:"You need to deploy a microservices application on GKE with minimal operational overhead. The application has variable traffic patterns requiring automatic scaling. You want Google to manage the cluster infrastructure. What should you do?",options:["Deploy a GKE Autopilot cluster and configure Horizontal Pod Autoscaler for your services","Deploy a Standard GKE cluster and manually configure node pools with autoscaling","Deploy individual Compute Engine instances and install Kubernetes manually","Use Cloud Run instead as it's always the better choice for microservices"],correct:0,explanation:"GKE Autopilot provides a fully managed Kubernetes experience where Google manages nodes, scaling, and security. Combined with HPA, it automatically handles variable traffic with minimal operational overhead.",wrongExplanations:{1:"Standard GKE requires you to manage node pools, upgrades, and scaling configurations. This increases operational overhead compared to Autopilot.",2:"Manual Kubernetes installation requires significant expertise and ongoing maintenance. It's the most operationally intensive option.",3:"Cloud Run is great for stateless services but doesn't provide the same Kubernetes ecosystem benefits, custom networking, or stateful workload support that GKE offers."}},{id:4,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data storage - Database selection",question:"You're migrating a PostgreSQL database that supports a financial analytics application with complex queries. The application requires strong consistency, ACID transactions, and SQL support. Which database should you choose?",options:["Cloud SQL for PostgreSQL with high availability configuration","Firestore in Datastore mode","BigQuery with streaming inserts","Cloud Spanner with PostgreSQL interface"],correct:0,explanation:"Cloud SQL for PostgreSQL provides a managed PostgreSQL service with ACID transactions, strong consistency, and full SQL support. HA configuration ensures reliability for production workloads.",wrongExplanations:{1:"Firestore is a NoSQL database that doesn't support complex SQL queries or PostgreSQL compatibility. It's designed for document-based, real-time applications.",2:"BigQuery is an analytics data warehouse, not an OLTP database. It's optimized for analytical queries, not transactional workloads with frequent updates.",3:"Spanner is for globally distributed, horizontally scalable workloads. It's more expensive and complex than needed for a PostgreSQL migration focused on analytics."}},{id:5,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Load balancers",question:"Your web application serves global users and requires SSL termination, URL-based routing to different backend services, and Cloud CDN integration. Which load balancer should you use?",options:["External Application Load Balancer (HTTP(S) Load Balancer)","Network Load Balancer (TCP/UDP)","Internal Application Load Balancer","Cloud Run with built-in load balancing"],correct:0,explanation:"External Application Load Balancer provides Layer 7 load balancing with SSL termination, URL-based routing, and native Cloud CDN integration for global content delivery.",wrongExplanations:{1:"Network Load Balancer operates at Layer 4 (TCP/UDP) and doesn't provide URL-based routing or native Cloud CDN integration. It's for non-HTTP workloads.",2:"Internal Application Load Balancer is for traffic within your VPC, not for serving external users. It's used for internal microservices communication.",3:"While Cloud Run has load balancing, it doesn't give you the same control over routing rules, backend service management, and CDN integration for complex applications."}},{id:6,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - VPC design",question:"You need to connect your on-premises data center to GCP with predictable bandwidth and low latency. The connection must be private and not traverse the public internet. What should you use?",options:["Dedicated Interconnect or Partner Interconnect depending on location","Cloud VPN with high-bandwidth tunnels","Direct peering with Google","Public internet with VPN encryption"],correct:0,explanation:"Dedicated or Partner Interconnect provides private, high-bandwidth, low-latency connections between on-premises and GCP without using the public internet. Choose based on proximity to Google facilities.",wrongExplanations:{1:"Cloud VPN traverses the public internet (encrypted), which doesn't meet the 'not traverse public internet' requirement and has variable latency.",2:"Direct peering is for accessing Google services, not for private connections to your VPC. It doesn't provide the private connectivity needed.",3:"Public internet explicitly violates the requirement. Even with VPN encryption, traffic still goes over the public internet with variable performance."}},{id:7,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud NGFW",question:"You need to implement advanced network security for your VPC that includes intrusion detection, threat prevention, and URL filtering. What should you configure?",options:["Cloud Next Generation Firewall (Cloud NGFW) with threat prevention profiles","VPC firewall rules with deny rules for specific IP ranges","Cloud Armor with custom security policies","Network tags with hierarchical firewall policies"],correct:0,explanation:"Cloud NGFW provides advanced security features including IDS/IPS, threat prevention, URL filtering, and TLS inspection - capabilities not available in standard VPC firewall rules.",wrongExplanations:{1:"VPC firewall rules provide basic allow/deny based on IP, port, and protocol but lack intrusion detection, threat prevention, and URL filtering capabilities.",2:"Cloud Armor protects against DDoS and web attacks at Layer 7 (HTTP/HTTPS) but doesn't provide network-level intrusion detection or URL filtering for all traffic types.",3:"Network tags with firewall policies improve organization but don't add advanced security features like threat detection and prevention."}},{id:8,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Infrastructure as code - Fabric FAST",question:"Your enterprise needs to deploy a multi-environment, multi-project GCP foundation following Google's best practices for large organizations. What tool should you use?",options:["Cloud Foundation Toolkit's Fabric FAST framework","Write custom Terraform modules from scratch","Use gcloud commands in shell scripts","Deploy manually through Cloud Console and export configurations"],correct:0,explanation:"Fabric FAST (Fabric is Abstractions for Service and Transfer) is specifically designed for enterprise GCP foundations, providing pre-built, tested patterns for multi-environment deployments following Google's best practices.",wrongExplanations:{1:"Custom Terraform requires significant development time, testing, and may not follow Google's best practices. Fabric FAST provides proven patterns.",2:"gcloud scripts are imperative and difficult to maintain. They don't provide the declarative infrastructure management needed for enterprise environments.",3:"Manual deployment is error-prone, not reproducible, and doesn't scale. Exporting configurations doesn't capture all settings and requires manual synchronization."}},{id:9,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - GKE Autopilot",question:"Your application running on GKE Autopilot is experiencing OOMKilled errors. You need to adjust Pod resource requests. What should you do?",options:["Update the Pod specification's resource requests for memory and redeploy","Scale up the node pool to have larger machines","Contact Google Cloud Support to increase node capacity","Migrate to Standard GKE where you have more control"],correct:0,explanation:"In GKE Autopilot, you configure resource requests in your Pod specs. Autopilot automatically provisions appropriate nodes. You don't manage nodes directly - Google handles that based on your Pod requirements.",wrongExplanations:{1:"In Autopilot, you don't manage node pools directly. Google automatically provisions nodes based on Pod resource requests. Trying to scale node pools won't work.",2:"Google manages node capacity in Autopilot automatically. You don't need to contact support - simply update your Pod resource requests and Autopilot handles provisioning.",3:"Migrating to Standard GKE for this issue is unnecessary. Autopilot can handle resource adjustments through Pod specifications - you just need to update them correctly."}},{id:10,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Database Center",question:"You have multiple database instances across Cloud SQL, AlloyDB, and Spanner. You need a centralized view of database health, performance, and recommendations. What should you use?",options:["Database Center to manage and monitor your Google Cloud database fleet","Create custom Cloud Monitoring dashboards for each database type","Use gcloud commands to query each database service separately","Set up separate monitoring tools for Cloud SQL, AlloyDB, and Spanner"],correct:0,explanation:"Database Center provides a unified interface for managing all Google Cloud databases (Cloud SQL, AlloyDB, Spanner, Firestore) with health monitoring, performance insights, and optimization recommendations.",wrongExplanations:{1:"Custom dashboards require manual setup for each database and don't provide the unified insights and recommendations that Database Center offers automatically.",2:"gcloud commands require scripting and manual aggregation. You won't get the centralized view, health monitoring, and AI-powered recommendations Database Center provides.",3:"Separate monitoring tools increase complexity, costs, and maintenance. Database Center provides native integration with all Google Cloud database services."}},{id:11,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring and logging - Ops Agent",question:"You need to collect metrics and logs from your Compute Engine instances for Cloud Monitoring and Cloud Logging. What is the recommended approach?",options:["Install and configure the Ops Agent on your instances","Use the legacy Stackdriver Logging and Monitoring agents","Write custom scripts to push logs via the Cloud Logging API","Manually export logs to Cloud Storage and import to Cloud Logging"],correct:0,explanation:"Ops Agent is the recommended unified agent that replaces legacy Stackdriver agents. It provides optimized collection of metrics and logs with better performance and easier configuration.",wrongExplanations:{1:"Legacy Stackdriver agents are deprecated. Ops Agent provides better performance, simpler configuration, and is the current recommended solution.",2:"Custom scripts add maintenance overhead and complexity. Ops Agent handles metrics and logs collection automatically with built-in integrations.",3:"Manual export/import is inefficient and loses real-time monitoring capabilities. Ops Agent provides automated, real-time log and metric collection."}},{id:12,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring and logging - Error Reporting",question:"Your application deployed on Cloud Run is experiencing intermittent errors. You want to automatically group similar errors and receive notifications. What should you use?",options:["Error Reporting to automatically group and track errors with Cloud Logging integration","Set up custom log-based metrics in Cloud Logging","Query Cloud Logging manually to find error patterns","Use Cloud Trace to identify errors"],correct:0,explanation:"Error Reporting automatically groups similar errors from Cloud Logging, provides real-time notifications, shows error trends, and integrates with alerting - perfect for tracking application errors.",wrongExplanations:{1:"Log-based metrics require manual configuration and don't provide automatic error grouping or detailed error analysis that Error Reporting offers.",2:"Manual querying is time-consuming, reactive, and doesn't provide automatic grouping or notifications when new errors occur.",3:"Cloud Trace is for distributed tracing and latency analysis, not error detection and grouping. It's used to understand request flow, not aggregate errors."}},{id:13,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Cloud Run - Secrets management",question:"Your Cloud Run service needs to access a database password. How should you securely provide this credential?",options:["Store the password in Secret Manager and mount it as an environment variable in Cloud Run","Hard-code the password in the container image","Pass the password as a query parameter in the Cloud Run URL","Store the password in a Cloud Storage bucket and read it at startup"],correct:0,explanation:"Secret Manager provides secure storage for sensitive data with encryption, versioning, and audit logging. Cloud Run can directly access secrets as environment variables or volume mounts.",wrongExplanations:{1:"Hard-coding passwords in container images exposes them to anyone with access to the image. Images should never contain secrets.",2:"Query parameters are logged and visible in URLs. This is extremely insecure for any sensitive data.",3:"While better than hard-coding, Cloud Storage doesn't provide the specialized secret management features like rotation, versioning, and audit logging that Secret Manager offers."}},{id:14,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Compute Engine",question:"You need to perform maintenance on a Compute Engine instance without losing its ephemeral IP address. What should you do?",options:["Stop the instance, perform maintenance, then start it again","Delete and recreate the instance with the same configuration","Use live migration while the instance is running","Take a snapshot and restore to a new instance"],correct:0,explanation:"Stopping and starting an instance preserves its ephemeral IP address. The instance retains the same internal and external IPs when restarted.",wrongExplanations:{1:"Deleting and recreating an instance assigns new IP addresses. The ephemeral IP is released when the instance is deleted.",2:"Live migration is for Google's maintenance events, not for user-initiated maintenance. You can't trigger it manually for your own maintenance tasks.",3:"Restoring to a new instance creates a different instance with different IP addresses. The original IPs aren't transferred."}},{id:15,domain:"Configuring access and security",subdomain:"4.1 IAM - Least privilege",question:"A data analyst needs read-only access to BigQuery datasets to run queries but should not be able to create or delete datasets. What IAM role should you grant?",options:["roles/bigquery.dataViewer at the dataset level","roles/viewer at the project level","roles/bigquery.user at the project level","roles/bigquery.admin with conditions limiting to read operations"],correct:0,explanation:"bigquery.dataViewer provides read-only access to table data and metadata at the dataset level, following least privilege. It doesn't allow resource creation or modification.",wrongExplanations:{1:"Project Viewer grants read access to ALL resources in the project, not just BigQuery. This violates least privilege by providing unnecessary permissions.",2:"bigquery.user allows running queries and creating jobs, which could include INSERT/UPDATE/DELETE operations. It provides more permissions than needed for read-only access.",3:"bigquery.admin grants full BigQuery permissions including creating and deleting resources. IAM conditions don't effectively restrict this to read-only operations."}},{id:16,domain:"Configuring access and security",subdomain:"4.1 IAM - Organization policies",question:"You need to prevent all users in your organization from creating external IP addresses on Compute Engine instances to improve security. What should you do?",options:["Set the 'constraints/compute.vmExternalIpAccess' organization policy to deny all","Remove the Compute Instance Admin role from all users","Use VPC firewall rules to block all external traffic","Manually review and delete external IPs daily"],correct:0,explanation:"Organization policies provide centralized control over resource configurations. The vmExternalIpAccess constraint prevents creation of instances with external IPs across the entire organization.",wrongExplanations:{1:"Removing roles prevents users from creating any instances, not just preventing external IPs. This is too restrictive and breaks legitimate use cases.",2:"Firewall rules control traffic flow but don't prevent the assignment of external IP addresses. Instances can still have external IPs even if firewall blocks traffic.",3:"Manual review is reactive, doesn't scale, and allows windows where insecure configurations exist. Organization policies prevent the issue proactively."}},{id:17,domain:"Configuring access and security",subdomain:"4.1 IAM - Custom roles",question:"You need to create a role that allows starting and stopping Compute Engine instances but not creating or deleting them. What should you do?",options:["Create a custom role with compute.instances.start and compute.instances.stop permissions","Use the predefined Compute Instance Admin role","Grant Compute Admin role with IAM conditions","Use the Compute Viewer role"],correct:0,explanation:"Custom roles allow you to bundle specific permissions for precise access control. Including only start and stop permissions follows least privilege.",wrongExplanations:{1:"Compute Instance Admin includes create and delete permissions, violating the requirement. It grants far more access than needed.",2:"IAM conditions work on resource attributes (like resource names or tags), not on specific operations. You can't use conditions to limit Admin role to only start/stop.",3:"Compute Viewer only allows reading instance information, not starting or stopping them. It doesn't provide the needed operational permissions."}},{id:18,domain:"Configuring access and security",subdomain:"4.1 IAM - Service accounts",question:"Your Cloud Function needs to write data to BigQuery. Following best practices, how should you configure access?",options:["Create a dedicated service account with only BigQuery Data Editor role and assign it to the function","Use the default App Engine service account","Use your personal user account","Grant the function Owner role for simplicity"],correct:0,explanation:"Dedicated service accounts with minimal required permissions (least privilege) are the best practice. BigQuery Data Editor allows writing data without unnecessary permissions.",wrongExplanations:{1:"Default App Engine service account has Editor permissions across the project - far more than needed. This violates least privilege principle.",2:"Personal user accounts shouldn't be used for services. This breaks automation and creates security and auditing issues.",3:"Owner role grants full control over all resources - massive security risk. Always follow least privilege by granting only required permissions."}},{id:19,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Best practices",question:"You are deploying a Cloud Function that needs to write to Cloud Storage and publish to Pub/Sub. How should you configure the service account?",options:["Create a dedicated service account with only Storage Object Creator and Pub/Sub Publisher roles","Use the default App Engine service account","Create a service account with Owner role for simplicity","Use your personal account to deploy the function"],correct:0,explanation:"A dedicated service account with only required permissions (Storage Object Creator and Pub/Sub Publisher) follows least privilege and provides clear audit trails for the function's actions.",wrongExplanations:{1:"Default App Engine service account has Editor-level permissions on the project - far more than needed. This violates least privilege.",2:"Owner role grants full control over all resources, creating massive security risks. If the function is compromised, attackers have complete project access.",3:"Personal accounts break automation, make auditing difficult, and tie permissions to a specific person rather than the service requirement."}},{id:20,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Quotas",question:"Your application deployment fails with a 'QUOTA_EXCEEDED' error when creating Compute Engine instances. You need more resources immediately. What should you do?",options:["Request a quota increase through the Cloud Console quotas page for your project","Create a new project and deploy there","Contact Google Cloud Sales to purchase more quota","Delete existing resources to free up quota"],correct:0,explanation:"Quota increases can be requested through the Cloud Console. For immediate needs, explain the business justification. Most requests are approved quickly, especially for established accounts.",wrongExplanations:{1:"Creating a new project resets quotas to default limits, which might not solve the problem. You'd also lose the existing project's configuration and resources.",2:"Quota isn't purchased - it's requested based on need. Sales can help with very large increases, but standard increases go through the Console quota request process.",3:"Deleting resources only helps if you're genuinely over-provisioned. If you need more capacity for legitimate use, request a quota increase instead."}},{id:21,domain:"Setting up a cloud solution environment",subdomain:"1.1 Cloud Identity - User management",question:"Your organization has 500 employees, and you need to provision Google Cloud access for all of them using your existing Active Directory. What should you do?",options:["Set up Cloud Identity with Google Cloud Directory Sync (GCDS) to synchronize users from Active Directory","Manually create Google accounts for each user","Have each user create their own Gmail account","Use a single shared Google account for all users"],correct:0,explanation:"Cloud Identity with GCDS automatically syncs users and groups from Active Directory to Cloud Identity, enabling centralized management and SSO. This is the enterprise solution for user management.",wrongExplanations:{1:"Manual creation doesn't scale, is error-prone, and creates ongoing maintenance burden. When users join/leave, you must manually update Google Cloud.",2:"Individual Gmail accounts don't integrate with enterprise systems, make auditing difficult, and can't be centrally managed. This isn't suitable for enterprise use.",3:"Shared accounts completely break access control, auditing, and security. You can't track who did what or apply least privilege principles."}},{id:22,domain:"Setting up a cloud solution environment",subdomain:"1.2 Billing - Budget alerts",question:"You set up a billing budget with a 100% threshold alert, but you're still exceeding your budget. What's the likely issue?",options:["Budget alerts only notify; they don't stop spending. You need to implement automated responses or manual intervention","The alert wasn't properly configured","Budget limits automatically stop resource creation","Google Cloud doesn't enforce budgets"],correct:0,explanation:"Billing budgets are informational only - they send alerts but don't prevent spending. You must take action (manual or automated via Cloud Functions/Pub/Sub) to control costs.",wrongExplanations:{1:"While misconfiguration is possible, the more common issue is expecting budgets to enforce limits. Budget alerts work even if they don't stop spending.",2:"Budget limits don't automatically stop anything. Google Cloud will continue to bill you for resources you create regardless of budget settings.",3:"Google Cloud does track spending and send budget alerts, but budgets are advisory, not enforced limits. You remain responsible for managing your costs."}},{id:23,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Compute - Preemptible VMs",question:"You're running a batch processing job that can tolerate interruptions. You want to minimize costs. What type of Compute Engine instance should you use?",options:["Spot VMs (formerly Preemptible VMs) which offer significant discounts for interruptible workloads","Standard VMs with sustained use discounts","Committed use contracts for 1-year term","Instance templates with autoscaling"],correct:0,explanation:"Spot VMs offer up to 91% discount compared to standard instances. They're perfect for fault-tolerant, batch processing workloads that can handle interruptions and restarts.",wrongExplanations:{1:"Sustained use discounts apply automatically but don't provide as much savings as Spot VMs. For interruptible batch jobs, Spot VMs are more cost-effective.",2:"Committed use contracts require long-term commitment and don't provide as much discount as Spot VMs. They're better for predictable, continuous workloads.",3:"Autoscaling helps with variable load but doesn't reduce per-instance costs. For batch jobs that can tolerate interruptions, Spot VMs are the cost-optimization choice."}},{id:24,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Cloud SQL - High availability",question:"Your production database on Cloud SQL needs to survive a zonal failure with minimal downtime. What should you configure?",options:["Enable high availability (HA) configuration which creates a standby replica in another zone","Create manual backups hourly","Use read replicas in multiple zones","Export data to Cloud Storage regularly"],correct:0,explanation:"Cloud SQL HA configuration maintains a standby instance in another zone with synchronous replication. Automatic failover occurs if the primary zone fails, minimizing downtime.",wrongExplanations:{1:"Backups help with data recovery but don't provide automatic failover. Restoring from backup takes time and results in significant downtime.",2:"Read replicas are for read scaling, not automatic failover. They use asynchronous replication and aren't promoted automatically during failures.",3:"Exports to Cloud Storage help with disaster recovery but don't provide high availability or automatic failover. Recovery time would be lengthy."}},{id:25,domain:"Planning and implementing a cloud solution",subdomain:"2.2 BigQuery - Cost optimization",question:"Your BigQuery queries are expensive because they scan entire tables. Most queries only need data from the last 30 days. How can you reduce costs?",options:["Partition the table by date and use WHERE clauses on the partition column","Export old data to Cloud Storage","Create separate tables for each day","Use clustering only"],correct:0,explanation:"Partitioning by date allows BigQuery to prune unnecessary data. Queries with WHERE clauses on the partition column only scan relevant partitions, dramatically reducing costs.",wrongExplanations:{1:"Exporting data loses BigQuery's query capabilities and creates management overhead. Partitioning keeps data in BigQuery while reducing scan costs.",2:"Separate daily tables create management complexity and make cross-day queries difficult. Partitioned tables are the native solution for time-based data.",3:"Clustering alone doesn't eliminate unnecessary data scanning. Partitioning is required to physically separate data and prune partitions at query time."}},{id:26,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Cloud Load Balancing - SSL certificates",question:"You're setting up an HTTPS load balancer and need to manage SSL certificates with automatic renewal. What should you use?",options:["Google-managed SSL certificates which automatically provision and renew certificates","Self-signed certificates","Manual Let's Encrypt certificates uploaded to Cloud Load Balancing","Certificates purchased from a third-party CA and manually uploaded"],correct:0,explanation:"Google-managed certificates automatically provision (via Let's Encrypt) and renew SSL certificates for your domains. This eliminates manual certificate management overhead.",wrongExplanations:{1:"Self-signed certificates trigger browser warnings and aren't trusted by clients. They're only suitable for testing, never production.",2:"Manual Let's Encrypt certificates require you to handle renewal every 90 days. Google-managed certificates automate this process completely.",3:"Third-party certificates cost money and require manual renewal and upload. Google-managed certificates are free and fully automated."}},{id:27,domain:"Planning and implementing a cloud solution",subdomain:"2.3 VPC - Shared VPC",question:"Your organization has multiple projects that need to communicate privately and share network resources. What networking architecture should you implement?",options:["Shared VPC where a host project shares VPC networks with service projects","VPC Network Peering between all projects","Separate VPCs with Cloud VPN connections","Public IP addresses for cross-project communication"],correct:0,explanation:"Shared VPC allows centralized network administration where service projects can use networks from a host project. This simplifies management and maintains private connectivity.",wrongExplanations:{1:"VPC Peering works but creates a mesh topology that becomes complex with many projects. Shared VPC provides better centralized management for organizations.",2:"Cloud VPN is for connecting to on-premises or other clouds, not for internal GCP project connectivity. It adds unnecessary complexity and costs.",3:"Public IPs expose services to the internet and incur egress charges. Private connectivity through Shared VPC is more secure and cost-effective."}},{id:28,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Terraform - State management",question:"You're using Terraform to manage GCP infrastructure. Where should you store the Terraform state file for a production environment?",options:["Cloud Storage bucket with versioning enabled and appropriate IAM controls","Local file on your workstation","Commit to Git repository","Store in Cloud SQL database"],correct:0,explanation:"Cloud Storage with versioning provides durable, shared state storage with history. IAM controls limit access. This enables team collaboration and disaster recovery.",wrongExplanations:{1:"Local state files don't allow team collaboration and can be easily lost. They're only suitable for personal testing, never for production or team environments.",2:"State files contain sensitive information (resource IDs, metadata). Committing to Git exposes this data and creates merge conflicts with team collaboration.",3:"Cloud SQL adds unnecessary complexity for Terraform state. Cloud Storage is the recommended backend with built-in versioning and locking support."}},{id:29,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Compute Engine - Live migration",question:"Google announces maintenance for the zone where your Compute Engine instance runs. What happens to your instance by default?",options:["The instance is live-migrated to another host in the same zone with no downtime","The instance is automatically terminated","The instance is moved to another zone","Nothing happens; you must manually migrate"],correct:0,explanation:"Compute Engine live migration moves running instances to different physical hosts during maintenance events. This happens automatically with no downtime for most instance types.",wrongExplanations:{1:"Instances are not terminated during maintenance by default. Live migration keeps them running. Termination only occurs if you've set the maintenance policy to 'TERMINATE'.",2:"Instances stay in the same zone during live migration. Cross-zone migration would change the instance's internal IP and isn't done automatically.",3:"Live migration is automatic for most instances. You don't need to take action unless you're using instance types that don't support live migration (like preemptible VMs)."}},{id:30,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 GKE - Node pools",question:"You need to run both CPU-intensive and memory-intensive workloads on GKE. How should you configure your cluster?",options:["Create multiple node pools with different machine types optimized for each workload, then use node selectors","Use a single node pool with the largest available machine type","Create separate clusters for each workload type","Use Autopilot which doesn't allow custom node configuration"],correct:0,explanation:"Multiple node pools allow workload-optimized machine types. Node selectors or node affinity ensure pods schedule on appropriate nodes, optimizing cost and performance.",wrongExplanations:{1:"Single large machine types waste resources. CPU-intensive workloads don't need excessive memory and vice versa. This increases costs unnecessarily.",2:"Separate clusters add management overhead, increase costs (multiple control planes), and make cross-workload communication more complex.",3:"Autopilot does allow resource specification through Pod requests. However, the question is about Standard GKE where multiple node pools with selectors is the right approach."}},{id:31,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Cloud Storage - Lifecycle management",question:"You have log files in Cloud Storage that must be retained for 7 years for compliance but are rarely accessed after 90 days. How should you optimize costs?",options:["Create a lifecycle policy to move objects to Archive storage after 90 days","Manually move old files to Archive storage quarterly","Keep everything in Standard storage for compliance","Delete files after 90 days and restore from backups if needed"],correct:0,explanation:"Lifecycle policies automatically transition objects between storage classes. Archive storage provides lowest cost for long-term retention while maintaining compliance requirements.",wrongExplanations:{1:"Manual processes are error-prone, don't scale, and may miss files. Lifecycle policies automate transitions reliably without manual intervention.",2:"Different storage classes have the same durability and compliance capabilities. Standard storage is much more expensive for rarely-accessed data.",3:"Deleting files violates compliance requirements. Archive storage maintains data for the required retention period at much lower cost than standard storage."}},{id:32,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Cloud Monitoring - Uptime checks",question:"You want to monitor whether your web application is accessible from different global locations. What should you configure?",options:["Cloud Monitoring uptime checks from multiple geographic locations","Write a Cloud Function to ping your application every minute","Set up Compute Engine instances worldwide to test access","Use Cloud Trace to monitor application availability"],correct:0,explanation:"Uptime checks are specifically designed for monitoring endpoint availability from multiple global locations. They integrate with alerting and are included with Cloud Monitoring.",wrongExplanations:{1:"Custom Cloud Functions add unnecessary complexity and costs. Uptime checks provide this functionality natively with better integration and features.",2:"Compute Engine instances for monitoring are expensive and complex to manage. Uptime checks provide the same functionality at lower cost.",3:"Cloud Trace is for distributed tracing and latency analysis, not availability monitoring. It shows request flow, not uptime status."}},{id:33,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Cloud Logging - Log sinks",question:"You need to retain audit logs for 5 years for compliance but Cloud Logging's default retention is 30 days. What should you do?",options:["Create a log sink to export logs to Cloud Storage with appropriate retention policies","Increase Cloud Logging retention to 5 years","Manually export logs monthly","Use BigQuery for all logging"],correct:0,explanation:"Log sinks export logs to Cloud Storage, BigQuery, or Pub/Sub. Cloud Storage with bucket retention policies provides cost-effective long-term log retention for compliance.",wrongExplanations:{1:"Cloud Logging supports custom retention (up to 3650 days/10 years) but it's more expensive than Cloud Storage for long-term retention. Log sinks to Storage are more cost-effective.",2:"Manual export processes are unreliable and may miss logs during system outages or human error. Automated log sinks ensure continuous, reliable export.",3:"BigQuery can store logs but is optimized for queries, not cold storage. Cloud Storage is more cost-effective for compliance retention with infrequent access."}},{id:34,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Cloud Storage - Object management",question:"You accidentally deleted critical files from a Cloud Storage bucket. The files were deleted 4 days ago. How can you recover them?",options:["List object versions in the bucket and restore the previous version if Object Versioning is enabled","Contact Google Cloud Support to restore from backup","Check the Cloud Storage trash folder","Recover from local backups since Cloud Storage doesn't keep deleted files"],correct:0,explanation:"If Object Versioning is enabled, deleted objects become non-current versions that can be restored. Without versioning, deleted objects are permanently gone after a short grace period.",wrongExplanations:{1:"Google Cloud Support cannot restore deleted Cloud Storage objects. Storage is customer-managed, and deletions are permanent without versioning.",2:"Cloud Storage doesn't have a trash folder. Once deleted (and the grace period passes), objects are gone unless versioning was enabled.",3:"Without Object Versioning, deleted files cannot be recovered from Cloud Storage. This is why versioning and proper backup strategies are critical."}},{id:35,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Data solutions - BigQuery",question:"Your BigQuery query is returning an error: 'Resources exceeded during query execution'. What should you try first?",options:["Add WHERE clauses to limit the data scanned or partition the table","Request a BigQuery quota increase","Switch to Dataflow for processing","Export data to Cloud Storage and process locally"],correct:0,explanation:"Limiting data scanned through WHERE clauses or partitioning reduces resource usage. Partitioned tables allow BigQuery to prune unnecessary data, dramatically improving performance and staying within limits.",wrongExplanations:{1:"Quota increases don't fix poorly optimized queries. The query is likely scanning too much data unnecessarily. Optimize first before requesting more quota.",2:"Switching to Dataflow is overkill for a query optimization issue. BigQuery can handle large datasets efficiently with proper table design and query optimization.",3:"Exporting data loses BigQuery's power and creates complexity. Fix the query or table structure instead."}},{id:36,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 GKE - Troubleshooting",question:"A Pod in your GKE cluster is stuck in 'CrashLoopBackOff' status. What's the first step to diagnose the issue?",options:["Check Pod logs using 'kubectl logs <pod-name>' to see application errors","Delete the Pod and recreate it","Scale the deployment to zero and back","Restart the entire cluster"],correct:0,explanation:"Pod logs contain application output and error messages that explain why the container is crashing. This is always the first diagnostic step for CrashLoopBackOff issues.",wrongExplanations:{1:"Deleting the Pod might temporarily mask the issue but doesn't solve the underlying problem. The new Pod will likely crash the same way. Investigate first.",2:"Scaling down doesn't provide diagnostic information and is just a different way of deleting Pods. Check logs first to understand why it's crashing.",3:"Restarting the cluster is extremely disruptive and won't fix application-level issues. CrashLoopBackOff indicates a Pod problem, not a cluster problem."}},{id:37,domain:"Configuring access and security",subdomain:"4.1 IAM - Predefined roles",question:"A developer needs to deploy applications to App Engine but should not be able to modify IAM policies. What role should you grant?",options:["App Engine Deployer role at the project level","App Engine Admin role","Editor role at the project level","Project Owner role"],correct:0,explanation:"App Engine Deployer allows deploying applications but not modifying App Engine settings or IAM. This follows least privilege for the developer's needs.",wrongExplanations:{1:"App Engine Admin can modify App Engine settings including IAM policies. This violates the requirement.",2:"Editor role grants broad permissions including IAM modifications across many services. It far exceeds what the developer needs.",3:"Owner role includes full IAM control and billing management. This is the most privileged role and completely violates least privilege here."}},{id:38,domain:"Configuring access and security",subdomain:"4.1 IAM - Resource hierarchy",question:"You granted a user Editor role at the folder level. What access does this provide?",options:["Editor access to all projects within that folder and its subfolders","Editor access only to the folder itself, not projects within","Editor access to the entire organization","Read-only access to projects in the folder"],correct:0,explanation:"IAM permissions are inherited down the resource hierarchy. A role granted at folder level applies to all projects and resources within that folder and its subfolders.",wrongExplanations:{1:"Folders are containers for projects. IAM roles on folders automatically apply to all contained resources through inheritance.",2:"Folder-level roles don't grant organization-level access. They're scoped to that folder and its children.",3:"Editor role grants modification permissions, not read-only. The question asked about Editor role specifically."}},{id:39,domain:"Configuring access and security",subdomain:"4.2 VPC Service Controls",question:"You need to prevent data exfiltration from sensitive BigQuery datasets. What security control should you implement?",options:["VPC Service Controls to create a security perimeter around the BigQuery project","IAM conditions limiting access by IP address","Cloud Armor security policies","VPC firewall rules blocking BigQuery API access"],correct:0,explanation:"VPC Service Controls create security perimeters that prevent data from leaving defined GCP resources, even if someone has IAM permissions. This is specifically designed to prevent exfiltration.",wrongExplanations:{1:"IAM conditions can restrict access by attributes but don't prevent data copying to authorized locations. Someone with proper IAM could still exfiltrate data to another GCP project.",2:"Cloud Armor protects against external web-based attacks but doesn't control data movement between GCP resources or prevent authorized users from copying data.",3:"VPC firewall rules control network traffic to Compute Engine, not API-level access to BigQuery. They don't prevent data exfiltration through BigQuery API."}},{id:40,domain:"Configuring access and security",subdomain:"4.2 Binary Authorization",question:"You want to ensure only container images from your approved registry can be deployed to GKE. What should you implement?",options:["Binary Authorization with policies requiring images from specific registries","Container Analysis API to scan images","VPC Service Controls around GKE","IAM policies restricting who can deploy"],correct:0,explanation:"Binary Authorization enforces deployment policies that can require images to be from specific registries, be signed, or pass vulnerability scans before deployment to GKE.",wrongExplanations:{1:"Container Analysis scans for vulnerabilities but doesn't enforce where images come from or prevent deployment of unapproved images.",2:"VPC Service Controls limit data exfiltration but don't control which container images can be deployed.",3:"IAM controls who can deploy but not what they can deploy. Authorized users could still deploy images from unapproved registries."}},{id:41,domain:"Configuring access and security",subdomain:"4.1 IAM - Conditions",question:"You need to grant a contractor temporary access to Cloud Storage that automatically expires after 30 days. How should you configure this?",options:["Grant IAM role with a condition that expires after 30 days using temporal constraints","Manually revoke access after 30 days","Use a service account that you'll delete after 30 days","Set a calendar reminder to remove permissions"],correct:0,explanation:"IAM conditions support temporal constraints (expiration dates). The permission automatically expires on the specified date without manual intervention.",wrongExplanations:{1:"Manual revocation is error-prone. People forget, get busy, or leave the company. Automated expiration through IAM conditions is more reliable.",2:"Service account deletion is disruptive if the account is being used. IAM conditions allow fine-grained, automatic expiration of specific permissions.",3:"Calendar reminders depend on humans remembering and acting. IAM conditions provide automated, enforced expiration that doesn't rely on manual processes."}},{id:42,domain:"Configuring access and security",subdomain:"4.3 Data encryption - Customer-managed keys",question:"Your compliance requirements mandate control over encryption keys used for Cloud Storage data. What should you use?",options:["Customer-Managed Encryption Keys (CMEK) using Cloud KMS","Default encryption (Google-managed keys)","Customer-Supplied Encryption Keys (CSEK)","Client-side encryption before uploading"],correct:0,explanation:"CMEK with Cloud KMS gives you control over key management while Google handles the cryptographic operations. This balances security, compliance, and operational simplicity.",wrongExplanations:{1:"Google-managed keys don't give you control over key management. While secure, they don't meet compliance requirements for customer control.",2:"CSEK requires you to provide keys with every operation and manage key storage yourself. CMEK with Cloud KMS is easier to manage while meeting compliance needs.",3:"Client-side encryption works but adds application complexity and you lose features like server-side operations. CMEK provides the compliance benefit more simply."}},{id:43,domain:"Configuring access and security",subdomain:"4.4 Audit logging",question:"You need to track all IAM policy changes across your organization for security auditing. What should you use?",options:["Admin Activity audit logs which are always enabled and log IAM changes","Set up custom Cloud Logging sinks","Enable Data Access audit logs","Use Cloud Asset Inventory"],correct:0,explanation:"Admin Activity logs automatically track administrative actions including IAM policy changes. They're enabled by default and don't incur charges.",wrongExplanations:{1:"Custom logging solutions are unnecessary when Admin Activity logs already capture IAM changes. This adds complexity without benefit.",2:"Data Access logs track who accessed data, not who changed IAM policies. Admin Activity logs are the correct choice for IAM changes.",3:"Cloud Asset Inventory tracks resource state but isn't designed for real-time audit logging. Admin Activity logs provide detailed IAM change tracking."}},{id:44,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Key management",question:"You discovered a service account key file in a public GitHub repository. What should you do immediately?",options:["Delete the service account key in GCP and rotate all credentials","Make the repository private","Change the service account's IAM roles","Contact GitHub to remove the file"],correct:0,explanation:"Once exposed, keys must be immediately revoked. Delete the key in GCP, rotate any other keys, and investigate what resources were accessed. Repository changes don't remove the key from Git history.",wrongExplanations:{1:"Making the repository private doesn't remove the key from Git history. Anyone who cloned/forked it already has the key. The key must be deleted.",2:"Changing roles doesn't invalidate the exposed key. The key can still be used with its original permissions until explicitly deleted.",3:"Contacting GitHub won't quickly secure your environment. Even if they remove it, it may be cached/archived elsewhere. Immediate key deletion is critical."}},{id:45,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC - Config Connector",question:"Your team manages Kubernetes resources and wants to manage GCP resources using the same Kubernetes tools and workflows. What should you use?",options:["Config Connector to manage GCP resources as Kubernetes Custom Resources","Terraform with Kubernetes provider","gcloud commands in Kubernetes Jobs","Manual GCP resource creation separate from Kubernetes"],correct:0,explanation:"Config Connector extends Kubernetes to manage GCP resources as Custom Resources (CRDs), enabling you to use kubectl and GitOps workflows for both Kubernetes and GCP infrastructure.",wrongExplanations:{1:"Terraform with Kubernetes provider manages Kubernetes resources from Terraform, not the reverse. Config Connector manages GCP from Kubernetes.",2:"gcloud in Jobs is imperative and doesn't integrate with Kubernetes' declarative model. Config Connector provides true Kubernetes-native management.",3:"Manual creation defeats infrastructure-as-code principles and creates operational inconsistency between Kubernetes and GCP resource management."}},{id:46,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Database - Query Insights",question:"Your Cloud SQL database is experiencing slow query performance. You need to identify which queries are consuming the most resources. What tool should you use?",options:["Query Insights in Cloud SQL to analyze query performance and get optimization recommendations","Enable general query logging and manually analyze logs","Use Cloud Monitoring to view CPU metrics","Run EXPLAIN on all queries manually"],correct:0,explanation:"Query Insights automatically identifies slow queries, provides performance statistics, execution plans, and recommendations for indexes and optimization without manual intervention.",wrongExplanations:{1:"General query logs capture all queries but don't provide performance analysis, rankings, or recommendations. Manual analysis is time-consuming.",2:"CPU metrics show overall load but don't identify specific problematic queries or suggest optimizations.",3:"Manually running EXPLAIN on queries requires knowing which queries to investigate. Query Insights automatically identifies and analyzes problematic queries."}},{id:47,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Cloud Run - Traffic splitting",question:"You deployed a new version of your Cloud Run service and want to gradually shift traffic from the old version to test stability. What should you do?",options:["Use Cloud Run's traffic splitting feature to route a percentage of traffic to the new revision","Deploy the new version to a separate service and use a load balancer","Delete the old revision and hope the new one works","Use feature flags in your application code"],correct:0,explanation:"Cloud Run's built-in traffic splitting allows you to split traffic between revisions by percentage, enabling canary deployments and gradual rollouts with quick rollback capability.",wrongExplanations:{1:"Separate services with a load balancer adds complexity. Cloud Run has native traffic management that's simpler and integrated.",2:"Deleting old revisions removes the ability to quickly roll back if issues arise. Traffic splitting allows safe, gradual rollout with instant rollback.",3:"Feature flags add application complexity. Cloud Run's infrastructure-level traffic splitting is simpler and works regardless of application code."}},{id:48,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Cloud Trace",question:"Your microservices application has high latency, but you're unsure which service is causing delays. What tool should you use to investigate?",options:["Cloud Trace to visualize request flow and identify latency bottlenecks across services","Cloud Monitoring dashboards showing CPU usage","Cloud Logging to search for error messages","Cloud Profiler to analyze CPU usage"],correct:0,explanation:"Cloud Trace provides distributed tracing that shows how requests flow through microservices, where time is spent, and which services contribute to latency.",wrongExplanations:{1:"CPU metrics show resource usage but don't reveal how requests flow between services or where latency occurs in the request path.",2:"Logs show events but don't visualize request flow timing or show latency distribution across service calls.",3:"Cloud Profiler analyzes code-level CPU and memory usage within a service, not request flow and latency across multiple services."}},{id:49,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 GKE - kubectl",question:"You need to view logs from a specific Pod in your GKE cluster. What command should you use?",options:["kubectl logs <pod-name> to fetch logs from the Pod's containers","gcloud logging read to query Cloud Logging","kubectl get pod <pod-name> --show-logs","docker logs <container-id>"],correct:0,explanation:"kubectl logs is the standard command for viewing Pod logs in Kubernetes. It fetches logs from the Pod's containers, optionally filtering by namespace.",wrongExplanations:{1:"gcloud logging read queries Cloud Logging, which works but adds latency and requires proper log ingestion setup. kubectl logs is more direct for real-time troubleshooting.",2:"kubectl get doesn't have a --show-logs option. The get command is for retrieving resource definitions and status, not logs.",3:"docker logs requires direct node access and container ID knowledge. In GKE, you work at the Kubernetes abstraction level with kubectl, not directly with Docker."}},{id:50,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Resource hierarchy",question:"Your organization is setting up a new Google Cloud environment. You need to organize resources for three business units (Sales, Engineering, and Marketing), each with development and production environments. You want to enforce different organizational policies per business unit and enable centralized billing. What is the most appropriate resource hierarchy structure?",options:["Create an Organization, create three Folders for each business unit, create dev and prod Projects within each Folder, and apply policies at the Folder level","Create separate Organizations for each business unit, create dev and prod Projects in each Organization, and link to a centralized billing account","Create a single Project with labels for business-unit and environment, and use IAM conditions to enforce policies","Create six Projects (one for each business unit and environment combination) under the Organization root, and apply policies at the Project level"],correct:0,explanation:"The correct approach is to use Folders to represent business units within a single Organization. This allows you to: 1) Apply organizational policies at the Folder level that automatically inherit to all child Projects, 2) Maintain centralized billing through the Organization, 3) Provide clear separation between business units while maintaining centralized governance, 4) Enable separate dev/prod Projects within each business unit Folder. Folders are specifically designed for grouping Projects and applying hierarchical policies.",wrongExplanations:{1:"Creating separate Organizations would prevent centralized billing and governance. Organizations are typically limited to one per company/domain, and managing multiple Organizations creates unnecessary complexity and prevents unified policy enforcement.",2:"Using labels within a single Project doesn't provide resource isolation or policy enforcement capabilities. Labels are metadata for organization and billing attribution, but cannot enforce security boundaries or apply organizational policies. This would create a security and operational management nightmare.",3:"While simpler initially, applying policies at the Project level lacks scalability and doesn't leverage the hierarchical policy inheritance that Folders provide. You'd need to manually configure policies for each of the six Projects, and adding new Projects would require manual policy configuration each time."}},{id:51,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Organizational policies",question:"Your company has a policy that all Compute Engine instances must be created only in us-central1 and europe-west1 regions for data residency compliance. You need to enforce this across all projects in your organization. What should you do?",options:["Set an organizational policy constraint `constraints/gcp.resourceLocations` with allowed values of `in:us-central1-locations` and `in:europe-west1-locations` at the Organization level","Create a custom IAM role that only grants compute.instances.create permission for us-central1 and europe-west1, and assign it to all users","Use Cloud Asset Inventory to monitor VM creation and automatically delete VMs created in other regions","Configure firewall rules to block traffic to VMs created outside the specified regions"],correct:0,explanation:"The `constraints/gcp.resourceLocations` organizational policy is the correct solution. This constraint allows you to define allowed or denied locations for resource creation across your entire organization. By setting it at the Organization level with allowed values for US-central1 and Europe-west1 locations, you preventively enforce compliance - no user can create resources outside these locations regardless of their IAM permissions.",wrongExplanations:{1:"IAM roles control WHO can perform actions, not WHERE resources can be created. While you could theoretically create complex conditional IAM policies, this approach is extremely difficult to maintain, doesn't scale, and can be bypassed by users with broader permissions.",2:"This is a reactive approach that allows policy violations to occur before detection. Cloud Asset Inventory is for monitoring and auditing, not enforcement. Additionally, automatically deleting resources could cause significant business disruption.",3:"Firewall rules control network traffic between resources, not where resources can be created. This approach doesn't address the core requirement and would only affect connectivity, not creation."}},{id:52,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Enabling APIs",question:"You are setting up a new project and need to deploy a Compute Engine instance that will read data from Cloud Storage and write results to BigQuery. When you try to create the instance, you receive an error: 'Access Not Configured. Compute Engine API has not been used in project.' What is the most efficient way to resolve this?",options:["Run `gcloud services enable compute.googleapis.com storage.googleapis.com bigquery.googleapis.com` to enable all required APIs at once","Enable only Compute Engine API now, then enable other APIs later when you encounter errors","Contact Google Cloud support to request API enablement","Delete the project and create a new one with APIs pre-enabled"],correct:0,explanation:"The most efficient approach is to enable all required APIs at once using the gcloud command. This prevents future errors and allows immediate use of all services.",wrongExplanations:{1:"This reactive approach is inefficient and disrupts development workflow. You'd encounter errors at each stage requiring you to stop and enable APIs.",2:"API enablement is a self-service operation that doesn't require support. This would cause unnecessary delays.",3:"Project deletion is wasteful and doesn't solve the problem. API enablement is the correct solution."}},{id:53,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Budgets and alerts",question:"Your team deployed a new application and costs are higher than expected. You want an email alert when monthly spend reaches $5,000, and another when it's forecasted to reach $7,000. How should you configure this?",options:["Create a budget of $7,000 with two thresholds: 71% actual spend and 100% forecasted spend","Create two separate budgets: $5,000 actual and $7,000 forecasted","Set a billing hard cap at $7,000 with notification at $5,000","Use Pub/Sub to monitor billing exports and trigger alerts"],correct:0,explanation:"A single budget with both actual and forecasted thresholds is most efficient. 71% of $7,000 equals $5,000.",wrongExplanations:{1:"Two budgets create unnecessary management overhead when one budget can handle both thresholds.",2:"Google Cloud doesn't support hard spending caps. Budgets only provide alerts, they don't stop spending.",3:"This over-engineers a solution that budgets handle natively."}},{id:54,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Machine types",question:"Your video transcoding application is CPU-intensive but doesn't require much memory. You're using n2-standard-8 (8 vCPUs, 32 GB RAM). How can you optimize costs?",options:["Switch to n2-highcpu-8 with 8 vCPUs and only 8 GB RAM at lower cost","Switch to e2-medium to reduce costs","Switch to n2-highmem-8 for better performance","Keep n2-standard-8 and rely on sustained use discounts"],correct:0,explanation:"The highcpu family is designed for compute-intensive workloads, providing the same vCPUs with less RAM at ~30% lower cost.",wrongExplanations:{1:"e2-medium only has 2 vCPUs, drastically reducing performance. Your jobs would take 4x longer.",2:"highmem instances have MORE RAM (64 GB), moving in the wrong direction and increasing costs.",3:"Sustained use discounts apply automatically but don't address paying for unused RAM."}},{id:55,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Spot VMs",question:"You have a batch processing job running nightly for 3-4 hours that can be restarted if interrupted. How can you minimize costs?",options:["Switch to Spot VMs which offer up to 91% discount for interruptible workloads","Purchase 1-year committed use discounts","Switch to e2-micro instances","Use autoscaling to scale to zero"],correct:0,explanation:"Spot VMs are perfect for fault-tolerant batch jobs, providing 60-91% discounts with 30-second preemption warnings.",wrongExplanations:{1:"CUDs require 24/7 commitment but your job runs only ~15% of the time. Spot VMs provide better savings.",2:"e2-micro would be insufficient for processing jobs and make them run much slower.",3:"Autoscaling addresses scheduling but doesn't reduce per-instance costs like Spot VMs do."}},{id:56,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud Storage classes",question:"Security camera footage must be retained for 7 years. It's accessed frequently for 30 days, occasionally for days 31-365, and rarely after. How should you configure storage?",options:["Start in Standard, transition to Nearline after 30 days, then Archive after 365 days using lifecycle policies","Store everything in Archive from the beginning","Use Coldline for all objects","Manually move files quarterly"],correct:0,explanation:"Lifecycle policies automatically transition between storage classes based on age, optimizing costs for each access pattern.",wrongExplanations:{1:"Archive has high retrieval costs and early deletion fees. Starting there would be expensive for frequent first-30-day access.",2:"Coldline doesn't optimize for the frequent early access or the very rare post-year access.",3:"Manual transitions are error-prone, don't scale, and delay cost optimization."}},{id:57,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud SQL HA",question:"Your MySQL database must survive a complete zone failure with automatic failover and minimal data loss. What should you configure?",options:["Enable high availability configuration with synchronous replication to a standby in another zone","Create read replicas in multiple zones","Enable automated backups every hour","Manually manage instances in each zone"],correct:0,explanation:"Cloud SQL HA provides automatic failover in 60-120 seconds with zero data loss via synchronous replication to a standby replica in a different zone.",wrongExplanations:{1:"Read replicas use asynchronous replication and don't automatically promote during failures. Manual intervention is required.",2:"Backups are for disaster recovery but cause downtime during restore. They don't provide automatic failover.",3:"Manual management is complex and doesn't leverage Google's automatic failover capabilities."}},{id:58,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Load balancer selection",question:"You need a global web application with HTTPS, URL path-based routing, and Cloud CDN integration. Which load balancer should you use?",options:["External Application Load Balancer with URL maps for path-based routing","Network Load Balancer for TCP load balancing","Internal Application Load Balancer","Regional External Application Load Balancer"],correct:0,explanation:"External Application Load Balancer provides Layer 7 HTTP(S) load balancing with global reach, URL-based routing, and native Cloud CDN integration.",wrongExplanations:{1:"Network Load Balancer operates at Layer 4 and can't perform URL routing or integrate with CDN.",2:"Internal Application Load Balancer is for private VPC traffic, not internet-facing applications.",3:"Regional load balancer lacks global reach and edge caching benefits."}},{id:59,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - GKE Autopilot",question:"Your team wants Kubernetes without managing nodes or cluster operations. The application has variable traffic. What should you use?",options:["GKE Autopilot where Google manages all cluster infrastructure and you only configure Pod resources","GKE Standard with node auto-provisioning","GKE Standard with manual node pools","Managed instance groups with containers"],correct:0,explanation:"GKE Autopilot provides fully managed Kubernetes - you define Pod requirements, Google handles all node provisioning, scaling, and updates.",wrongExplanations:{1:"Standard GKE requires managing nodes, pools, and cluster configuration even with auto-provisioning.",2:"Manual management has highest operational overhead.",3:"MIGs lack Kubernetes orchestration features and would require reinventing Kubernetes."}},{id:60,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - IAM roles",question:"You need to grant a data science team access to run BigQuery queries across multiple projects in a folder. They should NOT be able to create or delete datasets. What is the most appropriate approach?",options:["Grant BigQuery User role at the folder level","Grant BigQuery Admin role with IAM conditions","Grant Editor role at the folder level","Grant BigQuery Data Owner for each dataset"],correct:0,explanation:"BigQuery User role allows running queries and browsing datasets without modification permissions. At folder level, it applies to all projects automatically.",wrongExplanations:{1:"BigQuery Admin grants full control including creating/deleting datasets, violating least privilege even with conditions.",2:"Editor role grants broad permissions across ALL services, massively violating least privilege.",3:"Data Owner requires per-dataset management and doesn't scale well or provide query execution capabilities."}},{id:61,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Cost allocation",question:"Multiple teams share projects and Finance needs to allocate costs back to teams for chargeback. What is the most effective approach?",options:["Apply labels to resources with team and cost-center keys, enable billing export to BigQuery, query costs by label","Create separate billing accounts for each team","Create separate projects for each team","Manually review monthly invoices"],correct:0,explanation:"Labels provide flexible multi-dimensional cost attribution. Billing export to BigQuery captures label data for flexible reporting and chargeback automation.",wrongExplanations:{1:"Separate billing accounts create management overhead, lose volume discounts, and complicate cross-team resource sharing.",2:"Separate projects are inflexible and don't handle shared resources or multi-dimensional attribution (team AND app AND cost center).",3:"Manual allocation is labor-intensive, error-prone, not scalable, and delays chargeback processes."}},{id:62,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Firewall rules",question:"Users cannot access your web application on port 80. The app works when you curl localhost:80 from the VM. What is the most likely issue?",options:["VPC firewall rules don't allow ingress on port 80 - create an allow rule for tcp:80 from 0.0.0.0/0","Private Google Access is not enabled","Instances lack external IP addresses","Cloud NAT is not configured"],correct:0,explanation:"VPC firewall rules default to DENY all ingress. An ingress allow rule for tcp:80 is needed to permit external HTTP traffic to reach the instances.",wrongExplanations:{1:"Private Google Access is for VM-to-Google API connectivity, not for inbound user traffic.",2:"If IPs were missing, connection attempts would fail differently. The symptom suggests blocking, not routing failure.",3:"Cloud NAT provides outbound internet for private VMs, not inbound access for users."}},{id:63,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Private Google Access",question:"Your VMs without external IPs need to access Cloud Storage and BigQuery. What is the minimum configuration required?",options:["Enable Private Google Access on the subnet where instances are deployed","Configure Cloud NAT","Assign temporary external IPs","Set up Cloud VPN"],correct:0,explanation:"Private Google Access allows VMs with only internal IPs to reach Google APIs using Google's internal network without internet traversal.",wrongExplanations:{1:"Cloud NAT provides general internet access but is overkill and less secure for Google API access only.",2:"Temporary external IPs defeat security requirements and add operational complexity.",3:"Cloud VPN connects on-premises to Google Cloud, not relevant for VM-to-API connectivity within Google Cloud."}},{id:64,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Cloud NAT",question:"VMs without external IPs need to download software updates from the internet and call external APIs. What should you implement?",options:["Create a Cloud Router, then create a Cloud NAT gateway associated with that router for all subnets","Enable Private Google Access which provides full internet access","Assign temporary external IPs when needed","Set up a proxy server on a VM with external IP"],correct:0,explanation:"Cloud NAT provides managed outbound internet connectivity for private VMs with automatic HA, scalability, and no VM configuration needed.",wrongExplanations:{1:"Private Google Access only reaches Google APIs, not general internet or third-party services.",2:"Temporary IPs defeat security and create operational complexity managing assignments.",3:"Self-managed proxy creates single point of failure, requires maintenance, and doesn't scale automatically."}},{id:65,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Cloud Identity",question:"Your company has 500 employees and needs identity management for Google Cloud without Google Workspace. What should you implement to minimize costs?",options:["Set up Cloud Identity Free edition for your domain","Purchase Google Workspace licenses for all employees","Have employees create personal Gmail accounts","Use external identity federation without Cloud Identity"],correct:0,explanation:"Cloud Identity Free provides enterprise identity management, IAM integration, and organizational features without productivity apps or per-user costs.",wrongExplanations:{1:"Workspace adds $6/user/month ($36k/year for 500 users) for productivity tools you don't need.",2:"Personal Gmail accounts prevent centralized management, can't create Organizations, and violate security policies.",3:"Federation still requires Cloud Identity to establish Organization and manage configurations."}},{id:66,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Quotas",question:"Your team reports they cannot create more than 8 VMs in us-central1-a. You need to allow up to 24 CPUs. What should you do?",options:["Navigate to IAM & Admin > Quotas, filter for Compute Engine CPU quota in us-central1, request increase to 24","Use gcloud command to update quota","Delete existing VMs and recreate with smaller machine types","Upgrade to premium support for higher quotas"],correct:0,explanation:"Quota increases are requested through the Quotas page by selecting the CPU quota and providing justification. Requests are typically approved quickly.",wrongExplanations:{1:"No gcloud command exists to modify quotas directly. Quotas must be requested through Console or API.",2:"Deleting VMs is disruptive and doesn't solve the need for more capacity. Request proper quota.",3:"Quota increases are independent of support plans. All customers can request increases based on need."}},{id:67,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Billing exports",question:"Finance needs detailed cost analysis with data updated throughout the day, queryable by project, service, SKU, and labels. What should you configure?",options:["Enable billing export to BigQuery with detailed usage cost data in a multi-region dataset","Download monthly PDF invoices","Export to Cloud Storage CSV files daily","Give Billing Account Viewer role for Console access"],correct:0,explanation:"BigQuery export provides detailed resource-level data with throughout-the-day updates, SQL query capabilities, and integration with BI tools.",wrongExplanations:{1:"PDF invoices are high-level summaries lacking granular data, labels, or SKU details, and are monthly only.",2:"CSV exports update daily (not throughout day), require manual processing, and lack native query capabilities.",3:"Console provides basic reports but no SQL queries, automation, or advanced analysis capabilities."}},{id:68,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - VPC creation",question:"You're setting up a VPC for production across three regions. You want full control over subnet IP ranges following best practices. What should you do?",options:["Create custom mode VPC, then create subnets in required regions with non-overlapping RFC 1918 IP ranges","Create auto mode VPC which automatically creates subnets in all regions","Create three separate VPCs and connect with VPC Peering","Use the default VPC and modify subnets"],correct:0,explanation:"Custom mode VPC provides full control over subnet creation, IP addressing, and follows production best practices with explicit configuration.",wrongExplanations:{1:"Auto mode creates subnets in ALL 40+ regions using predefined ranges that may conflict with on-premises networks.",2:"Three separate VPCs add unnecessary complexity. Single global VPC can span multiple regions natively.",3:"Default VPC is auto mode and may have experimental resources. Best practice is purpose-built custom VPCs for production."}},{id:69,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - BigQuery partitioning",question:"Your 50TB dataset contains 5 years of logs. Most queries analyze last 3 months. Queries scan entire table and are expensive. How can you optimize costs?",options:["Partition table by date and ensure queries include date filter in WHERE clause for partition pruning","Export old data to Cloud Storage","Enable BigQuery flat-rate pricing","Add clustering without partitioning"],correct:0,explanation:"Partitioning by date physically separates data. Queries with date filters only scan relevant partitions, reducing costs by ~95% when querying 3 of 60 months.",wrongExplanations:{1:"Exporting loses BigQuery query capabilities and creates operational complexity managing data in two places.",2:"Flat-rate pricing changes billing model but doesn't fix inefficient queries. Optimize query first.",3:"Clustering improves performance within partitions but doesn't reduce data scanned like partitioning does."}},{id:70,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Run",question:"You need to deploy a containerized API with highly variable traffic and long idle periods. You want to minimize costs and avoid infrastructure management. What should you use?",options:["Cloud Run, which automatically scales to zero during idle periods and charges only for request processing time","GKE Autopilot with scale-to-zero","Compute Engine with autoscaling to zero","Cloud Functions instead"],correct:0,explanation:"Cloud Run is designed for this use case: auto-scales to zero (no idle charges), fast startup, pay-per-request pricing, fully managed, supports any container.",wrongExplanations:{1:"GKE Autopilot can scale to zero but has Kubernetes overhead and complexity unnecessary for simple APIs.",2:"Compute Engine MIGs don't scale to zero - minimum is 1 instance, resulting in 24/7 VM costs.",3:"Cloud Functions works but requires specific language runtimes. The workload is already containerized, making Cloud Run the direct choice."}},{id:71,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud VPN",question:"You need encrypted connectivity from on-premises to Google Cloud over internet with 3 Gbps throughput and high availability. What should you implement?",options:["HA VPN with two tunnels to two VPN gateways for 99.99% SLA and up to 3 Gbps per tunnel","Classic VPN with single tunnel","Direct connection without encryption","VPC Peering"],correct:0,explanation:"HA VPN provides 99.99% SLA with two interfaces in different zones, supports up to 3 Gbps per tunnel, uses BGP for automatic failover.",wrongExplanations:{1:"Classic VPN is deprecated, has no SLA, single point of failure, and static routing only.",2:"Unencrypted connection violates security and compliance requirements for data in transit.",3:"VPC Peering connects VPCs within Google Cloud, not on-premises to cloud."}},{id:72,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - VM operations",question:"You need to perform monthly maintenance requiring a VM to be powered off while preserving all data. What is the correct procedure?",options:["Stop the instance, perform maintenance, then start it again - all disks and IPs are preserved","Delete and recreate from snapshot","Reset the instance","Suspend the instance"],correct:0,explanation:"Stop/start is the standard procedure for offline maintenance. VM state, disks, metadata, and IP addresses are all preserved during stop state.",wrongExplanations:{1:"Delete/recreate is complex, risky, may lose metadata and IPs, and risks configuration errors.",2:"Reset performs hard reboot without powering off - doesn't allow offline maintenance and may cause data loss.",3:"Suspend is specialized for preserving RAM state - simpler stop/start is sufficient for routine maintenance."}},{id:73,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Cloud Monitoring alerts",question:"Your app has intermittent latency spikes. You want alerts when average latency exceeds 500ms for 5 minutes. What should you configure?",options:["Create alerting policy with metric threshold condition for latency > 500ms with 5-minute evaluation window","Set up log-based metric querying every minute","Use Cloud Trace for manual monitoring","Configure uptime checks"],correct:0,explanation:"Cloud Monitoring alerting policies continuously evaluate metrics, trigger on conditions, and send notifications automatically with configurable thresholds and windows.",wrongExplanations:{1:"Log-based metrics are more complex, less efficient, and higher latency than native metric-based alerts.",2:"Cloud Trace is for distributed tracing, not threshold-based alerting on aggregate metrics.",3:"Uptime checks monitor availability, not aggregate application latency across all requests."}},{id:74,domain:"Configuring access and security",subdomain:"4.1 IAM - Service accounts for VMs",question:"Your Compute Engine application needs to upload files to Cloud Storage. Following security best practices, how should you configure authentication?",options:["Create dedicated service account with Storage Object Creator role, attach to VM, use Application Default Credentials","Generate service account keys and store on VM disk","Use personal user account credentials","Make bucket publicly writable"],correct:0,explanation:"Attaching service account to VM with ADC is secure best practice: no key management, automatic rotation, easy auditing, follows least privilege.",wrongExplanations:{1:"Service account keys create security risks: can be stolen, don't rotate automatically, require secure storage.",2:"Personal accounts tie permissions to individuals, break when users leave, violate separation of duties.",3:"Public buckets allow anyone to upload, creating severe security violations and unlimited cost exposure."}},{id:75,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Workload Identity",question:"Your GKE application needs Cloud SQL access without managing service account keys. What is the recommended approach?",options:["Configure Workload Identity to allow Pods to authenticate as Google service accounts without key files","Store service account keys in Kubernetes Secrets","Use Compute Engine default service account","Mount key JSON files as volumes"],correct:0,explanation:"Workload Identity is Google's recommended method: no key files, automatic rotation, fine-grained permissions per workload, eliminates key management.",wrongExplanations:{1:"Secrets are base64-encoded (not encrypted by default), anyone with access can extract keys, no automatic rotation.",2:"Default service account has Editor role - too broad, can't differentiate between applications, violates least privilege.",3:"Mounting key files has same security issues as Secrets plus risk of logging or accidental export."}},{id:76,domain:"Configuring access and security",subdomain:"4.3 Security - Cloud KMS CMEK",question:"You must store customer data with customer-controlled encryption keys and ability to revoke access independently of deletion. What should you implement?",options:["Customer-Managed Encryption Keys (CMEK) using Cloud KMS to manage keys while Google handles crypto operations","Default Google-managed encryption","Customer-Supplied Encryption Keys (CSEK)","Client-side encryption"],correct:0,explanation:"CMEK provides key control for compliance while Google handles operations. Disabling key revokes access without deleting objects. Balances control and manageability.",wrongExplanations:{1:"Default encryption doesn't give key control, can't revoke by disabling keys, doesn't meet compliance for customer-controlled keys.",2:"CSEK requires providing keys with every request, creates operational burden, if lost data is inaccessible.",3:"Client-side encryption requires application implementation, loses Google Cloud features, more complex and risky."}},{id:77,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Cloud Storage operations",question:"You need to grant a partner temporary 7-day read access to specific files without them having a Google account. What should you do?",options:["Generate signed URLs with 7-day expiration for specific objects and share URLs","Make entire bucket public for 7 days","Create Google account for partner","Download and email files"],correct:0,explanation:"Signed URLs provide time-limited access to specific objects without Google accounts using cryptographic signatures. Auto-expire, secure, auditable.",wrongExplanations:{1:"Public buckets expose ALL objects to internet, creating massive security risk even temporarily.",2:"Creating accounts adds overhead and doesn't automatically expire - signed URLs are simpler.",3:"Email has size limits, security concerns, no audit trail, manual process doesn't scale."}},{id:78,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Gemini Cloud Assist",question:"You need AI-powered analysis to identify overprovisioned VMs, unused disks, and security misconfigurations. What should you use?",options:["Gemini Cloud Assist with Cloud Asset Inventory for AI-powered recommendations","Active Assist Recommender API only","Cloud Asset Inventory only","Custom scripts analyzing gcloud outputs"],correct:0,explanation:"Gemini Cloud Assist (2025 feature) provides AI-powered analysis with natural language interaction, proactive recommendations, and comprehensive resource visibility.",wrongExplanations:{1:"Active Assist gives rule-based recommendations but lacks AI conversational interface and comprehensive analysis.",2:"Asset Inventory shows resources but doesn't provide optimization recommendations or analysis.",3:"Custom scripts require development, maintenance, and expertise - don't build what's provided as managed service."}},{id:79,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - GKE troubleshooting",question:"Your GKE cluster can't pull images from Artifact Registry in the same project. What is the most likely issue?",options:["Node service account needs Artifact Registry Reader role","Artifact Registry API is not enabled","Repository needs to be public","Need VPC peering to Artifact Registry"],correct:0,explanation:"GKE nodes use their service account to pull images. Grant Artifact Registry Reader role to the node service account for authentication.",wrongExplanations:{1:"Disabled API produces different error. Authentication failure indicates permission issues.",2:"Making repositories public is security anti-pattern - fix authentication instead.",3:"VPC connectivity not required for Artifact Registry from GKE - this is authentication issue."}},{id:80,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud Storage lifecycle",question:"Application logs must be retained 7 years. Frequently accessed first 30 days, occasionally to day 365, rarely after. How to optimize costs?",options:["Use Standard class with lifecycle rules transitioning to Nearline at 30 days, Archive at 365 days","Store everything in Archive from start","Use Coldline for all objects","Manually move files quarterly"],correct:0,explanation:"Lifecycle policies automatically transition between classes optimizing for each access pattern: Standard (frequent), Nearline (occasional), Archive (rare).",wrongExplanations:{1:"Archive has high retrieval costs and early deletion fees - expensive for frequent first-30-day access.",2:"Coldline doesn't optimize for both frequent early access and very rare post-year access.",3:"Manual transitions are error-prone, don't scale, delay optimization."}},{id:81,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Instance types",question:"Your batch processing is CPU-bound and doesn't need much memory. Currently using n2-standard-8 (8 vCPUs, 32GB RAM). How to optimize costs?",options:["Switch to n2-highcpu-8 (8 vCPUs, 8GB RAM) at ~30% lower cost","Switch to e2-medium","Switch to n2-highmem-8","Keep n2-standard-8 for sustained use discounts"],correct:0,explanation:"highcpu family provides same vCPUs with less RAM (1:1 ratio vs 1:4) for compute-intensive workloads at significant cost savings.",wrongExplanations:{1:"e2-medium only has 2 vCPUs - would make jobs 4x slower.",2:"highmem has MORE RAM (64GB) - wrong direction, increases costs.",3:"Sustained use discounts apply automatically but don't address paying for unused RAM."}},{id:82,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Database Center",question:"You have Cloud SQL, AlloyDB, and Spanner instances. You need centralized database health monitoring and recommendations. What should you use?",options:["Database Center for unified interface managing all Google Cloud databases with AI recommendations","Create custom Cloud Monitoring dashboards for each type","Use gcloud commands for each service separately","Set up separate monitoring tools"],correct:0,explanation:"Database Center (2025 feature) provides unified interface for all Google Cloud databases with health monitoring, performance insights, and AI-powered optimization recommendations.",wrongExplanations:{1:"Custom dashboards require manual setup per database and don't provide unified insights and AI recommendations.",2:"gcloud requires scripting and manual aggregation, no centralized view or AI recommendations.",3:"Separate tools increase complexity, costs, and maintenance. Database Center provides native integration."}},{id:83,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud NGFW",question:"You need advanced network security with intrusion detection, threat prevention, and URL filtering for your VPC. What should you configure?",options:["Cloud Next Generation Firewall (Cloud NGFW) with threat prevention profiles","VPC firewall rules with deny rules","Cloud Armor security policies","Network tags with hierarchical policies"],correct:0,explanation:"Cloud NGFW (2025 feature) provides IDS/IPS, threat prevention, URL filtering, TLS inspection - advanced security not available in standard firewall rules.",wrongExplanations:{1:"VPC firewall rules provide basic allow/deny but lack intrusion detection and threat prevention.",2:"Cloud Armor protects against DDoS/web attacks at Layer 7 but doesn't provide network-level intrusion detection.",3:"Network tags improve organization but don't add threat detection capabilities."}},{id:84,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC - Fabric FAST",question:"Your enterprise needs multi-environment, multi-project GCP foundation following Google best practices. What tool should you use?",options:["Cloud Foundation Toolkit's Fabric FAST framework with pre-built patterns","Write custom Terraform from scratch","Use gcloud scripts","Deploy manually and export configs"],correct:0,explanation:"Fabric FAST is designed for enterprise GCP foundations with pre-built, tested patterns following Google best practices for multi-environment deployments.",wrongExplanations:{1:"Custom Terraform requires significant development and may not follow best practices.",2:"gcloud scripts are imperative and don't provide declarative infrastructure management needed for enterprise.",3:"Manual deployment is error-prone, not reproducible, doesn't scale, doesn't capture all settings."}},{id:85,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Ops Agent",question:"You need to collect metrics and logs from Compute Engine instances for Cloud Monitoring and Logging. What is the recommended approach?",options:["Install and configure Ops Agent on instances","Use legacy Stackdriver agents","Write custom scripts pushing to Logging API","Manually export to Cloud Storage"],correct:0,explanation:"Ops Agent is the recommended unified agent replacing legacy agents, providing optimized collection with better performance and easier configuration.",wrongExplanations:{1:"Legacy Stackdriver agents are deprecated. Ops Agent provides better performance and simpler configuration.",2:"Custom scripts add maintenance overhead and complexity. Ops Agent handles this automatically.",3:"Manual export is inefficient and loses real-time monitoring capabilities."}},{id:86,domain:"Configuring access and security",subdomain:"4.1 IAM - Least privilege",question:"A data analyst needs read-only BigQuery dataset access for queries but not dataset/table modification. What role should you grant?",options:["BigQuery Data Viewer at dataset level","Project Viewer role","BigQuery User at project level","BigQuery Admin with conditions"],correct:0,explanation:"BigQuery Data Viewer provides read-only access to table data and metadata at dataset level, following least privilege without modification permissions.",wrongExplanations:{1:"Project Viewer grants read access to ALL project resources, not just BigQuery - violates least privilege.",2:"BigQuery User allows creating jobs including INSERT/UPDATE/DELETE operations - more than needed.",3:"BigQuery Admin grants full permissions. IAM conditions don't effectively restrict to read-only."}},{id:87,domain:"Configuring access and security",subdomain:"4.1 IAM - Organization policies",question:"You need to prevent all users from creating external IPs on Compute Engine instances organization-wide. What should you do?",options:["Set organizational policy constraint 'constraints/compute.vmExternalIpAccess' to deny all at Organization level","Remove Compute Instance Admin role from all users","Use VPC firewall rules to block external traffic","Manually review and delete external IPs daily"],correct:0,explanation:"Organization policies provide centralized control. The vmExternalIpAccess constraint preventively blocks external IP creation across the organization.",wrongExplanations:{1:"Removing roles prevents any instance creation, not just external IPs - too restrictive.",2:"Firewall rules control traffic flow but don't prevent external IP assignment.",3:"Manual review is reactive, doesn't scale, allows windows where insecure configs exist."}},{id:88,domain:"Configuring access and security",subdomain:"4.1 IAM - Custom roles",question:"You need a role allowing starting/stopping VMs but not creating or deleting them. What should you do?",options:["Create custom role with compute.instances.start and compute.instances.stop permissions only","Use Compute Instance Admin role","Grant Compute Admin with IAM conditions","Use Compute Viewer role"],correct:0,explanation:"Custom roles allow bundling specific permissions for precise access control, following least privilege with exactly needed permissions.",wrongExplanations:{1:"Instance Admin includes create/delete permissions - violates requirements and least privilege.",2:"IAM conditions restrict WHERE/WHEN, not specific operations within a role.",3:"Viewer only allows reading, not starting/stopping instances."}},{id:89,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Query Insights",question:"Your Cloud SQL database has slow queries. You need to identify resource-consuming queries. What tool should you use?",options:["Query Insights to analyze performance and get optimization recommendations automatically","Enable general query logging and manually analyze","Use Cloud Monitoring CPU metrics","Run EXPLAIN on all queries manually"],correct:0,explanation:"Query Insights (2025 feature) automatically identifies slow queries, provides performance statistics, execution plans, and optimization recommendations without manual work.",wrongExplanations:{1:"General query logs don't provide performance analysis, rankings, or recommendations - requires manual analysis.",2:"CPU metrics show overall load but don't identify specific problematic queries.",3:"Manual EXPLAIN requires knowing which queries to investigate. Query Insights automatically finds problem queries."}},{id:90,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Cloud Run traffic splitting",question:"You deployed new Cloud Run version and want gradual traffic shift to test stability. What should you do?",options:["Use Cloud Run's traffic splitting feature to route percentage of traffic to new revision","Deploy to separate service with load balancer","Delete old revision immediately","Use feature flags in application code"],correct:0,explanation:"Cloud Run's built-in traffic splitting enables canary deployments with percentage-based routing between revisions and quick rollback capability.",wrongExplanations:{1:"Separate services add complexity. Cloud Run has native traffic management that's simpler.",2:"Deleting old revision removes quick rollback ability. Traffic splitting enables safe gradual rollout.",3:"Feature flags add app complexity. Infrastructure-level splitting is simpler and works regardless of code."}},{id:91,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Cloud Trace",question:"Your microservices app has high latency but you're unsure which service causes delays. What tool should you use?",options:["Cloud Trace to visualize request flow and identify latency bottlenecks across services","Cloud Monitoring CPU dashboards","Cloud Logging error searches","Cloud Profiler for CPU analysis"],correct:0,explanation:"Cloud Trace provides distributed tracing showing how requests flow through microservices, where time is spent, and which services contribute to latency.",wrongExplanations:{1:"CPU metrics show resource usage but don't reveal request flow timing or latency distribution.",2:"Logs show events but don't visualize request flow timing.",3:"Profiler analyzes code-level CPU/memory within a service, not request flow across multiple services."}},{id:92,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Key management",question:"You discovered a service account key in a public GitHub repository. What should you do immediately?",options:["Delete the service account key in GCP and rotate all credentials immediately","Make the repository private","Change the service account's IAM roles","Contact GitHub to remove the file"],correct:0,explanation:"Once exposed, keys must be immediately revoked. Delete key in GCP, rotate others, investigate access. Repository changes don't remove key from Git history.",wrongExplanations:{1:"Making repo private doesn't remove key from Git history. Anyone who cloned/forked already has it.",2:"Changing roles doesn't invalidate the exposed key which can still be used with original permissions.",3:"Contacting GitHub is too slow. Immediate key deletion is critical for security."}},{id:93,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC - Config Connector",question:"Your team manages Kubernetes resources and wants to manage GCP resources using same tools and workflows. What should you use?",options:["Config Connector to manage GCP resources as Kubernetes Custom Resources","Terraform with Kubernetes provider","gcloud commands in Kubernetes Jobs","Manual GCP resource creation"],correct:0,explanation:"Config Connector extends Kubernetes to manage GCP resources as CRDs, enabling kubectl and GitOps workflows for both Kubernetes and GCP infrastructure.",wrongExplanations:{1:"Terraform with Kubernetes provider manages Kubernetes from Terraform (reverse direction).",2:"gcloud in Jobs is imperative and doesn't integrate with Kubernetes' declarative model.",3:"Manual creation defeats infrastructure-as-code and creates operational inconsistency."}},{id:94,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Cloud Storage versioning",question:"You accidentally deleted critical files 4 days ago from Cloud Storage. How can you recover them?",options:["List object versions and restore previous version if Object Versioning is enabled","Contact Google Cloud Support for restore","Check Cloud Storage trash folder","Recover from local backups only"],correct:0,explanation:"If Object Versioning is enabled, deleted objects become non-current versions that can be restored. Without versioning, deletions are permanent after grace period.",wrongExplanations:{1:"Google Support cannot restore deleted Cloud Storage objects. Storage is customer-managed.",2:"Cloud Storage doesn't have a trash folder. Deletions are permanent without versioning.",3:"Without Object Versioning, files cannot be recovered from Cloud Storage."}},{id:95,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - BigQuery optimization",question:"Your BigQuery query returns 'Resources exceeded during query execution'. What should you try first?",options:["Add WHERE clauses limiting data scanned or partition the table","Request BigQuery quota increase","Switch to Dataflow for processing","Export to Cloud Storage and process locally"],correct:0,explanation:"Limiting scanned data through WHERE clauses or partitioning reduces resource usage. Partitioned tables enable BigQuery to prune unnecessary data, dramatically improving performance.",wrongExplanations:{1:"Quota increases don't fix poorly optimized queries scanning too much data unnecessarily.",2:"Dataflow is overkill for query optimization. BigQuery handles large datasets efficiently with proper design.",3:"Exporting loses BigQuery's power and creates complexity. Fix query/table structure instead."}},{id:96,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - GKE CrashLoopBackOff",question:"A Pod in GKE is stuck in 'CrashLoopBackOff'. What's the first diagnostic step?",options:["Check Pod logs using 'kubectl logs <pod-name>' to see application errors","Delete the Pod and recreate it","Scale deployment to zero and back","Restart the entire cluster"],correct:0,explanation:"Pod logs contain application output and error messages explaining why container is crashing. Always first diagnostic step for CrashLoopBackOff.",wrongExplanations:{1:"Deleting Pod might mask issue temporarily but doesn't solve underlying problem. New Pod will likely crash same way.",2:"Scaling down doesn't provide diagnostic information - just different way of deleting Pods. Investigate first.",3:"Restarting cluster is extremely disruptive and won't fix application-level issues. CrashLoopBackOff indicates Pod problem."}},{id:97,domain:"Configuring access and security",subdomain:"4.1 IAM - Predefined roles",question:"A developer needs to deploy to App Engine but not modify IAM policies. What role should you grant?",options:["App Engine Deployer role at project level","App Engine Admin role","Editor role at project level","Project Owner role"],correct:0,explanation:"App Engine Deployer allows deploying applications without modifying App Engine settings or IAM, following least privilege for developer needs.",wrongExplanations:{1:"App Engine Admin can modify settings including IAM policies - violates requirements.",2:"Editor grants broad permissions including IAM modifications across many services - far exceeds needs.",3:"Owner includes full IAM control and billing - most privileged role, completely violates least privilege."}},{id:98,domain:"Configuring access and security",subdomain:"4.1 IAM - Resource hierarchy inheritance",question:"You granted Editor role at folder level. What access does this provide?",options:["Editor access to all projects within that folder and its subfolders","Editor access only to folder itself, not projects","Editor access to entire organization","Read-only access to projects in folder"],correct:0,explanation:"IAM permissions inherit down the resource hierarchy. Folder-level roles automatically apply to all projects and resources within that folder and subfolders.",wrongExplanations:{1:"Folders are containers. IAM roles on folders automatically apply to contained resources through inheritance.",2:"Folder-level roles don't grant organization-level access - scoped to folder and children.",3:"Editor grants modification permissions. Question specifically asked about Editor role."}},{id:99,domain:"Configuring access and security",subdomain:"4.2 VPC Service Controls",question:"You need to prevent data exfiltration from sensitive BigQuery datasets. What security control should you implement?",options:["VPC Service Controls to create security perimeter around BigQuery project","IAM conditions limiting access by IP","Cloud Armor security policies","VPC firewall rules"],correct:0,explanation:"VPC Service Controls create security perimeters preventing data from leaving defined GCP resources, even if someone has IAM permissions. Designed specifically for preventing exfiltration.",wrongExplanations:{1:"IAM conditions restrict access by attributes but don't prevent data copying to authorized locations.",2:"Cloud Armor protects against external web attacks but doesn't control data movement between GCP resources.",3:"VPC firewall rules control network traffic to Compute Engine, not API-level BigQuery access or data exfiltration."}},{id:100,domain:"Configuring access and security",subdomain:"4.2 Binary Authorization",question:"You want to ensure only container images from your approved registry can deploy to GKE. What should you implement?",options:["Binary Authorization with policies requiring images from specific registries","Container Analysis API for scanning","VPC Service Controls around GKE","IAM policies restricting deployers"],correct:0,explanation:"Binary Authorization enforces deployment policies requiring images from specific registries, be signed, or pass vulnerability scans before GKE deployment.",wrongExplanations:{1:"Container Analysis scans for vulnerabilities but doesn't enforce source or prevent unapproved deployments.",2:"VPC Service Controls limit data exfiltration but don't control which container images can deploy.",3:"IAM controls who can deploy but not what they can deploy. Authorized users could still deploy unapproved images."}},{id:101,domain:"Configuring access and security",subdomain:"4.1 IAM - Conditions",question:"You need to grant contractor temporary access to Cloud Storage that auto-expires after 30 days. How should you configure this?",options:["Grant IAM role with condition that expires after 30 days using temporal constraints","Manually revoke access after 30 days","Use service account you'll delete after 30 days","Set calendar reminder to remove permissions"],correct:0,explanation:"IAM conditions support temporal constraints with automatic expiration on specified date without manual intervention. Reliable and automated.",wrongExplanations:{1:"Manual revocation is error-prone. People forget, get busy, or leave company. Automated expiration is more reliable.",2:"Service account deletion is disruptive if being used. IAM conditions allow fine-grained automatic expiration.",3:"Calendar reminders depend on humans. IAM conditions provide automated, enforced expiration not relying on manual processes."}},{id:102,domain:"Configuring access and security",subdomain:"4.3 Data encryption - CMEK",question:"Compliance requires control over encryption keys for Cloud Storage with ability to revoke access independently. What should you use?",options:["Customer-Managed Encryption Keys (CMEK) using Cloud KMS","Default Google-managed keys","Customer-Supplied Encryption Keys (CSEK)","Client-side encryption"],correct:0,explanation:"CMEK with Cloud KMS gives control over key management while Google handles cryptographic operations. Disabling key revokes access without deleting objects. Balances security, compliance, and operational simplicity.",wrongExplanations:{1:"Google-managed keys don't give control. Can't revoke by disabling keys or meet compliance for customer-controlled keys.",2:"CSEK requires providing keys with every operation, managing storage yourself, and creates operational burden.",3:"Client-side encryption adds app complexity, loses Google Cloud features, has performance overhead, riskier if implemented incorrectly."}},{id:103,domain:"Configuring access and security",subdomain:"4.4 Audit logging",question:"You need to track all IAM policy changes organization-wide for security auditing. What should you use?",options:["Admin Activity audit logs which are always enabled and log IAM changes","Set up custom Cloud Logging sinks","Enable Data Access audit logs","Use Cloud Asset Inventory"],correct:0,explanation:"Admin Activity logs automatically track administrative actions including IAM policy changes. Enabled by default, don't incur charges, provide detailed IAM change tracking.",wrongExplanations:{1:"Custom logging solutions are unnecessary when Admin Activity logs already capture IAM changes.",2:"Data Access logs track who accessed data, not who changed IAM policies.",3:"Cloud Asset Inventory tracks resource state but isn't designed for real-time audit logging."}},{id:104,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Error Reporting",question:"Your Cloud Run application has intermittent errors. You want automatic grouping and notifications. What should you use?",options:["Error Reporting to automatically group and track errors with Cloud Logging integration","Set up custom log-based metrics","Query Cloud Logging manually","Use Cloud Trace for errors"],correct:0,explanation:"Error Reporting automatically groups similar errors from Cloud Logging, provides real-time notifications, shows trends, and integrates with alerting - perfect for tracking application errors.",wrongExplanations:{1:"Log-based metrics require manual configuration and don't provide automatic error grouping or detailed analysis.",2:"Manual querying is time-consuming, reactive, doesn't provide automatic grouping or notifications.",3:"Cloud Trace is for distributed tracing and latency analysis, not error detection and grouping."}},{id:105,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Secrets management",question:"Your Cloud Run service needs database password. How should you securely provide this credential?",options:["Store password in Secret Manager and mount as environment variable in Cloud Run","Hard-code password in container image","Pass password as query parameter in URL","Store in Cloud Storage bucket"],correct:0,explanation:"Secret Manager provides secure storage with encryption, versioning, and audit logging. Cloud Run can directly access secrets as environment variables or volume mounts.",wrongExplanations:{1:"Hard-coding in images exposes passwords to anyone with image access. Images should never contain secrets.",2:"Query parameters are logged and visible in URLs - extremely insecure for sensitive data.",3:"Cloud Storage lacks specialized secret management features like rotation, versioning, and audit logging that Secret Manager offers."}},{id:106,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing Compute Engine - Maintenance",question:"You need to perform maintenance on a Compute Engine instance without losing its ephemeral IP. What should you do?",options:["Stop the instance, perform maintenance, then start it - IPs are preserved","Delete and recreate with same configuration","Use live migration","Take snapshot and restore to new instance"],correct:0,explanation:"Stopping and starting preserves ephemeral IP addresses. Instance retains same internal and external IPs when restarted.",wrongExplanations:{1:"Deleting and recreating assigns new IP addresses. Ephemeral IP is released when instance deleted.",2:"Live migration is for Google's maintenance, not user-initiated. Can't trigger manually.",3:"Restoring to new instance creates different instance with different IPs. Original IPs aren't transferred."}},{id:107,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Live migration",question:"Google announces maintenance for your VM's zone. What happens by default?",options:["Instance is live-migrated to another host in same zone with no downtime","Instance is automatically terminated","Instance is moved to another zone","Nothing - you must manually migrate"],correct:0,explanation:"Compute Engine live migration moves running instances to different physical hosts during maintenance automatically with no downtime for most instance types.",wrongExplanations:{1:"Instances aren't terminated by default during maintenance. Only if maintenance policy set to 'TERMINATE'.",2:"Instances stay in same zone during live migration. Cross-zone would change internal IP and isn't done automatically.",3:"Live migration is automatic for most instances. No action needed unless using types that don't support live migration."}},{id:108,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - GKE node pools",question:"You need to run CPU-intensive and memory-intensive workloads on GKE. How should you configure your cluster?",options:["Create multiple node pools with different machine types optimized for each workload, use node selectors","Use single node pool with largest available machine type","Create separate clusters for each workload type","Use Autopilot which doesn't allow custom configuration"],correct:0,explanation:"Multiple node pools allow workload-optimized machine types. Node selectors/affinity ensure pods schedule on appropriate nodes, optimizing cost and performance.",wrongExplanations:{1:"Single large machine type wastes resources. CPU-intensive workloads don't need excessive memory and vice versa, increasing costs.",2:"Separate clusters add management overhead, cost more (multiple control planes), and complicate cross-workload communication.",3:"Autopilot does allow resource specification through Pod requests. However, question is about Standard GKE where multiple node pools is correct approach."}},{id:109,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Monitoring - Uptime checks",question:"You want to monitor if your web application is accessible from different global locations. What should you configure?",options:["Cloud Monitoring uptime checks from multiple geographic locations","Write Cloud Function to ping application every minute","Set up Compute Engine instances worldwide to test","Use Cloud Trace for availability"],correct:0,explanation:"Uptime checks are designed for monitoring endpoint availability from multiple global locations. Integrate with alerting and are included with Cloud Monitoring.",wrongExplanations:{1:"Custom Cloud Functions add complexity and costs. Uptime checks provide this functionality natively with better integration.",2:"Compute Engine instances for monitoring are expensive and complex to manage. Uptime checks provide same functionality at lower cost.",3:"Cloud Trace is for distributed tracing and latency analysis, not availability monitoring or uptime status."}},{id:110,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Log sinks",question:"You need to retain audit logs for 5 years for compliance but Cloud Logging default retention is 30 days. What should you do?",options:["Create log sink to export logs to Cloud Storage with appropriate retention policies","Increase Cloud Logging retention to 5 years","Manually export logs monthly","Use BigQuery for all logging"],correct:0,explanation:"Log sinks export logs to Cloud Storage, BigQuery, or Pub/Sub. Cloud Storage with bucket retention policies provides cost-effective long-term retention for compliance.",wrongExplanations:{1:"Cloud Logging supports custom retention (up to 10 years) but it's more expensive than Cloud Storage for long-term retention.",2:"Manual export is unreliable and may miss logs during outages or human error. Automated log sinks ensure continuous export.",3:"BigQuery is optimized for queries, not cold storage. Cloud Storage is more cost-effective for compliance retention with infrequent access."}},{id:111,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Budget best practices",question:"You set up billing budget with 100% threshold alert but still exceed budget. What's the issue?",options:["Budget alerts only notify - they don't stop spending. You need automated responses or manual intervention","Alert wasn't properly configured","Budget limits automatically stop resource creation","Google Cloud doesn't enforce budgets"],correct:0,explanation:"Billing budgets are informational only - send alerts but don't prevent spending. You must take action (manual or automated via Cloud Functions/Pub/Sub) to control costs.",wrongExplanations:{1:"While misconfiguration is possible, more common issue is expecting budgets to enforce limits. Budgets alert, they don't stop spending.",2:"Budget limits don't automatically stop anything. Google Cloud will continue billing regardless of budget settings.",3:"Google tracks spending and sends budget alerts, but budgets are advisory, not enforced limits. You remain responsible for managing costs."}},{id:112,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Shared VPC",question:"Your organization has multiple projects needing private communication and shared network resources. What networking architecture should you implement?",options:["Shared VPC where host project shares VPC networks with service projects","VPC Network Peering between all projects","Separate VPCs with Cloud VPN connections","Public IPs for cross-project communication"],correct:0,explanation:"Shared VPC allows centralized network administration where service projects use networks from host project. Simplifies management and maintains private connectivity.",wrongExplanations:{1:"VPC Peering works but creates mesh topology that becomes complex with many projects. Shared VPC provides better centralized management.",2:"Cloud VPN is for connecting to on-premises or other clouds, not internal GCP project connectivity. Adds unnecessary complexity.",3:"Public IPs expose services to internet and incur egress charges. Private connectivity through Shared VPC is more secure and cost-effective."}},{id:113,domain:"Planning and implementing a cloud solution",subdomain:"2.4 IaC - Terraform state",question:"You're using Terraform for production infrastructure. Where should you store the state file?",options:["Cloud Storage bucket with versioning enabled and appropriate IAM controls","Local file on workstation","Commit to Git repository","Store in Cloud SQL database"],correct:0,explanation:"Cloud Storage with versioning provides durable, shared state storage with history. IAM controls limit access. Enables team collaboration and disaster recovery.",wrongExplanations:{1:"Local state files don't allow team collaboration and can be lost. Only suitable for personal testing, never production.",2:"State files contain sensitive information. Committing to Git exposes data and creates merge conflicts with team collaboration.",3:"Cloud SQL adds unnecessary complexity for Terraform state. Cloud Storage is recommended backend with built-in versioning and locking support."}},{id:114,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - SSL certificates",question:"You're setting up HTTPS load balancer and need SSL certificates with automatic renewal. What should you use?",options:["Google-managed SSL certificates which automatically provision and renew via Let's Encrypt","Self-signed certificates","Manual Let's Encrypt certificates","Third-party CA certificates"],correct:0,explanation:"Google-managed certificates automatically provision and renew SSL certificates for your domains. Eliminates manual certificate management overhead.",wrongExplanations:{1:"Self-signed certificates trigger browser warnings and aren't trusted by clients. Only suitable for testing, never production.",2:"Manual Let's Encrypt certificates require renewal every 90 days. Google-managed certificates automate this completely.",3:"Third-party certificates cost money and require manual renewal and upload. Google-managed certificates are free and fully automated."}},{id:115,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Database selection for MySQL",question:"You're migrating MySQL database supporting financial analytics with complex queries requiring strong consistency and ACID transactions. Which database should you choose?",options:["Cloud SQL for PostgreSQL with high availability configuration","Firestore in Datastore mode","BigQuery with streaming inserts","Cloud Spanner with PostgreSQL interface"],correct:0,explanation:"Cloud SQL for PostgreSQL provides managed PostgreSQL service with ACID transactions, strong consistency, full SQL support. HA configuration ensures reliability.",wrongExplanations:{1:"Firestore is NoSQL database that doesn't support complex SQL queries or PostgreSQL compatibility. Designed for document-based, real-time applications.",2:"BigQuery is analytics data warehouse, not OLTP database. Optimized for analytical queries, not transactional workloads with frequent updates.",3:"Spanner is for globally distributed, horizontally scalable workloads. More expensive and complex than needed for PostgreSQL migration."}},{id:116,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - VPC design for on-premises",question:"You need to connect on-premises data center to GCP with predictable bandwidth and low latency without traversing public internet. What should you use?",options:["Dedicated Interconnect or Partner Interconnect depending on proximity to Google facilities","Cloud VPN with high-bandwidth tunnels","Direct peering with Google","Public internet with VPN encryption"],correct:0,explanation:"Dedicated or Partner Interconnect provides private, high-bandwidth, low-latency connections between on-premises and GCP without public internet. Choose based on proximity to Google facilities.",wrongExplanations:{1:"Cloud VPN traverses public internet (encrypted), doesn't meet 'not traverse public internet' requirement and has variable latency.",2:"Direct peering is for accessing Google services, not private connections to your VPC. Doesn't provide needed private connectivity.",3:"Public internet explicitly violates requirement. Even with VPN encryption, traffic still goes over public internet with variable performance."}},{id:117,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Lifecycle management",question:"You have log files in Cloud Storage that must be retained 7 years but are rarely accessed after 90 days. How should you optimize costs?",options:["Create lifecycle policy to move objects to Archive storage after 90 days","Manually move old files to Archive quarterly","Keep everything in Standard storage","Delete files after 90 days and restore from backups if needed"],correct:0,explanation:"Lifecycle policies automatically transition objects between storage classes. Archive storage provides lowest cost for long-term retention while maintaining compliance.",wrongExplanations:{1:"Manual processes are error-prone, don't scale, and may miss files. Lifecycle policies automate transitions reliably.",2:"Different storage classes have same durability and compliance. Standard storage is much more expensive for rarely-accessed data.",3:"Deleting files violates compliance requirements. Archive storage maintains data for required retention at much lower cost."}},{id:118,domain:"Configuring access and security",subdomain:"4.1 IAM - Best practices",question:"Your Cloud Function needs to write to BigQuery. Following best practices, how should you configure access?",options:["Create dedicated service account with only BigQuery Data Editor role and assign to function","Use default App Engine service account","Use your personal user account","Grant function Owner role for simplicity"],correct:0,explanation:"Dedicated service accounts with minimal required permissions (least privilege) are best practice. BigQuery Data Editor allows writing data without unnecessary permissions.",wrongExplanations:{1:"Default App Engine service account has Editor permissions across project - far more than needed. Violates least privilege.",2:"Personal user accounts shouldn't be used for services. Breaks automation and creates security/auditing issues.",3:"Owner role grants full control over all resources - massive security risk. Always follow least privilege."}},{id:119,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Best practices",question:"Your Cloud Function needs to write to Storage and publish to Pub/Sub. How should you configure the service account?",options:["Create dedicated service account with only Storage Object Creator and Pub/Sub Publisher roles","Use default App Engine service account","Create service account with Owner role","Use personal account for deployment"],correct:0,explanation:"Dedicated service account with only required permissions follows least privilege and provides clear audit trails for function's actions.",wrongExplanations:{1:"Default App Engine service account has Editor-level permissions - far more than needed. Violates least privilege.",2:"Owner role grants full control creating massive security risks. If compromised, attackers have complete project access.",3:"Personal accounts break automation, make auditing difficult, tie permissions to person rather than service requirement."}},{id:120,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - Project creation",question:"Your company is launching a new product line and needs an isolated environment. You want to ensure proper billing attribution and separate IAM policies. What is the best approach?",options:["Create a new project within your existing organization for the product line","Create a new organization for the product line","Use the existing project with labels to separate resources","Create a separate billing account only"],correct:0,explanation:"Creating a new project within the existing organization provides proper isolation while maintaining centralized governance. Projects are the primary isolation boundary for resources, IAM policies, and billing attribution. The organization provides centralized policy control while projects enable separation of resources and billing.",wrongExplanations:{1:"Organizations are typically limited to one per company domain and are meant for the entire company, not individual product lines. Creating separate organizations prevents centralized governance and adds unnecessary complexity.",2:"Using labels within a single project doesn't provide IAM isolation or true resource separation. Labels are for organization and cost attribution but don't enforce security boundaries or separate IAM policies.",3:"Separate billing account alone doesn't provide resource or IAM isolation. Projects are needed for proper separation. Billing accounts are for payment and cost management, not resource isolation."}},{id:121,domain:"Setting up a cloud solution environment",subdomain:"1.1 Setting up cloud projects - API management",question:"You need to disable the Compute Engine API for a project to prevent new VM creation while keeping existing VMs running. What happens when you disable the API?",options:["New VMs cannot be created, but existing VMs continue running normally","All existing VMs are immediately terminated","The API cannot be disabled if VMs exist","Existing VMs stop but are not deleted"],correct:0,explanation:"Disabling an API prevents new resource creation using that API but does not affect existing resources. Existing VMs continue to run, can be accessed via SSH, and can be managed through gcloud commands for start/stop operations. This is useful for preventing new resource creation while maintaining existing workloads.",wrongExplanations:{1:"Google Cloud never terminates existing resources when an API is disabled. This would cause major production outages and is not how API management works. Disabling APIs only prevents new resource creation.",2:"APIs can be disabled even if resources exist. Google Cloud allows API disablement at any time and simply prevents new resource creation via that API.",3:"Disabling an API doesn't affect running resources. VMs remain in their current state (running or stopped) and continue operating normally."}},{id:122,domain:"Setting up a cloud solution environment",subdomain:"1.2 Managing billing - Committed use discounts",question:"Your production workload runs 24/7 on n2-standard-8 instances in us-central1. You want to reduce costs. What pricing model should you choose?",options:["Purchase 1-year or 3-year committed use discounts for predictable 24/7 workloads","Use Spot VMs for production workloads","Use on-demand pricing and rely on sustained use discounts","Purchase reservations for peak usage only"],correct:0,explanation:"Committed use discounts (CUDs) provide up to 57% discount for 1-year or 70% for 3-year commitments on compute resources. They're ideal for steady-state, predictable workloads running continuously. You commit to a certain amount of vCPU and memory in a region and get significant discounts. This is perfect for 24/7 production workloads with predictable resource needs.",wrongExplanations:{1:"Spot VMs can be preempted with 30-second notice and are unsuitable for production workloads requiring high availability. While they offer 60-91% discount, the interruption risk makes them inappropriate for 24/7 production systems.",2:"Sustained use discounts apply automatically but only provide up to 30% discount. For 24/7 workloads, CUDs offer much better savings (up to 57-70%) and are specifically designed for predictable, continuous usage.",3:"Reservations ensure capacity availability in specific zones but don't provide the same cost savings as CUDs. Reservations are for guaranteeing capacity during high-demand periods, while CUDs are for cost optimization on consistent usage."}},{id:123,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Subnet expansion",question:"Your subnet is running out of IP addresses. The current range is 10.0.1.0/24 (256 IPs). You need to expand it to accommodate 500 IPs. What should you do?",options:["Expand the subnet to 10.0.1.0/23 which provides 512 IPs","Delete the subnet and create a new one with larger range","Create a second subnet in the same region","Use alias IP ranges for additional addresses"],correct:0,explanation:"Google Cloud allows expanding subnets by modifying the CIDR mask to be smaller (encompassing more IPs). You can expand 10.0.1.0/24 to 10.0.1.0/23 which doubles the available IPs to 512. Expansion is done in place without downtime or resource recreation. Command: 'gcloud compute networks subnets expand-ip-range SUBNET --prefix-length=23'. This is non-disruptive and maintains all existing resources.",wrongExplanations:{1:"Deleting subnet would require recreating all resources (VMs, forwarding rules, etc.) in that subnet. Subnet expansion can be done in place without disruption. Never delete subnets when expansion is possible.",2:"Creating a second subnet doesn't help existing resources that are bound to the first subnet. Resources can't automatically move between subnets. Expanding the existing subnet is the correct solution.",3:"Alias IP ranges are for assigning multiple IPs to a single VM interface (useful for containers/pods), not for expanding overall subnet capacity. They consume IPs from the same subnet range."}},{id:124,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Instance groups",question:"You need to deploy a stateless web application across multiple zones in a region with automatic scaling based on CPU utilization. What should you use?",options:["Regional managed instance group with autoscaling based on CPU metrics","Zonal managed instance group in single zone","Unmanaged instance group","Multiple single VM instances"],correct:0,explanation:"Regional managed instance groups (MIGs) automatically distribute instances across multiple zones in a region, providing high availability and automatic zone failover. With autoscaling, the MIG automatically adds or removes instances based on CPU utilization. This provides both availability (multi-zone) and scalability (autoscaling) for stateless applications. Regional MIGs are Google's recommended approach for production applications.",wrongExplanations:{1:"Zonal MIG only deploys in a single zone, creating single point of failure for zone outages. For production applications requiring high availability, regional MIGs are preferred as they distribute across multiple zones automatically.",2:"Unmanaged instance groups are just collections of VMs without autoscaling, health checking, or automatic instance recreation. They require manual management and don't provide the automation needed for scaling based on CPU.",3:"Multiple single VMs require manual management, don't autoscale, have no automated health checking, and require custom load balancing configuration. Managed instance groups provide all these features automatically."}},{id:125,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud Spanner",question:"Your global application needs a SQL database with ACID transactions, strong consistency, and ability to scale horizontally. The application serves users across US, Europe, and Asia with single-digit millisecond latency. What should you use?",options:["Cloud Spanner configured as a multi-region instance spanning the required continents","Cloud SQL with read replicas in each region","Multiple regional Cloud SQL instances with application-level replication","Firestore in Datastore mode"],correct:0,explanation:"Cloud Spanner is Google's globally distributed, horizontally scalable relational database designed exactly for this use case. It provides: 1) Full SQL support with ACID transactions, 2) Strong consistency across regions, 3) Automatic sharding and horizontal scaling, 4) Single-digit millisecond latency through strategic multi-region placement, 5) 99.999% availability SLA. Multi-region configuration automatically replicates data across continents while maintaining consistency.",wrongExplanations:{1:"Cloud SQL read replicas use asynchronous replication causing potential data inconsistency. They don't provide the strong consistency requirement. Additionally, Cloud SQL doesn't scale horizontally like Spanner - it's limited to vertical scaling within a single primary instance.",2:"Multiple Cloud SQL instances with application-level replication creates enormous complexity in maintaining consistency, handling conflicts, and ensuring ACID properties across regions. This essentially means building a distributed database yourself, which is what Spanner provides as a managed service.",3:"Firestore is a NoSQL document database that doesn't support SQL or traditional relational schemas. While it can scale globally, it doesn't meet the SQL and relational database requirements."}},{id:126,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Persistent disks",question:"Your application requires high IOPS for database workloads. You need sustained performance of 30,000 IOPS. What disk type should you choose?",options:["SSD persistent disk which provides up to 100,000 IOPS per disk","Standard persistent disk for cost savings","Local SSD for maximum performance","Balanced persistent disk as a compromise"],correct:0,explanation:"SSD persistent disks provide high IOPS (up to 100,000 read IOPS and 100,000 write IOPS per disk) and are designed for high-performance database workloads. They offer the best balance of performance, durability, and features. SSDs provide persistent storage with snapshots, encryption, and the ability to resize or attach to different VMs. For 30,000 IOPS requirement, SSD persistent disk is the appropriate choice providing headroom for growth.",wrongExplanations:{1:"Standard persistent disks are backed by hard drives and provide much lower IOPS (around 3,000 IOPS max). They're designed for throughput-oriented workloads, not IOPS-intensive database operations requiring 30,000 IOPS.",2:"Local SSDs provide highest IOPS (up to 900,000) but are ephemeral - data is lost when VM is stopped or deleted. For database workloads requiring data persistence, local SSDs are inappropriate. They're for temporary caching, not database storage.",3:"Balanced persistent disks offer moderate performance (up to 80,000 IOPS) and cost between standard and SSD. While cheaper than SSD, they don't provide the consistent high performance needed for demanding database workloads requiring 30,000+ IOPS."}},{id:127,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Network tiers",question:"Your global web application needs to minimize latency for users worldwide by using Google's global network. What should you configure?",options:["Premium Network Tier which routes traffic through Google's global network","Standard Network Tier for cost savings","Configure Cloud CDN only","Use multiple regional load balancers"],correct:0,explanation:"Premium Network Tier routes traffic through Google's global private network, providing: 1) Lower latency by avoiding public internet hops, 2) Better reliability and performance, 3) Single global IP address (anycast), 4) Traffic enters Google network at nearest point of presence to user, 5) Optimized routing within Google's network. This is Google's recommended tier for latency-sensitive global applications and is default for most services.",wrongExplanations:{1:"Standard Network Tier routes traffic over public internet, resulting in higher latency and less predictable performance. While cheaper, it doesn't meet the requirement of minimizing latency through Google's global network. Standard tier uses regular internet routing, not Google's private network.",2:"Cloud CDN alone caches static content at edge locations but doesn't address routing for dynamic content or application logic. While CDN helps with static assets, Premium Tier is needed for overall application traffic to use Google's global network.",3:"Multiple regional load balancers create management complexity and don't provide the global network benefits. Premium Tier with global load balancer provides single global IP and automatic routing through Google's network to nearest backend."}},{id:128,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Snapshots",question:"You need to create backup of a persistent disk attached to a running VM. The disk contains a database with active writes. What is the best approach?",options:["Create a snapshot while the VM is running - snapshots are crash-consistent by default","Stop the VM, create snapshot, then restart","Detach the disk, create snapshot, reattach disk","Copy data to Cloud Storage instead"],correct:0,explanation:"Google Cloud snapshots can be created from attached disks while VMs are running. Snapshots are crash-consistent by default, meaning they capture disk state as if the system crashed. For databases, this is often acceptable as databases are designed to recover from crashes. For production databases, you can flush writes to disk before snapshot or use database-specific backup tools. Creating snapshots from running VMs is standard practice and causes no downtime.",wrongExplanations:{1:"Stopping VM creates unnecessary downtime. Snapshots can be created from running VMs without stopping. While stopping ensures no active writes, it's not required and introduces service interruption for most workloads.",2:"Detaching disk requires stopping VM (disks can't be detached from running instances for boot disks). This creates even more downtime than stopping the VM. Snapshots are designed to work with attached disks.",3:"Copying to Cloud Storage is manual process that doesn't leverage Compute Engine's integrated snapshot features. Snapshots are incremental, compressed, and provide fast restore to persistent disks. Manual copying loses these benefits."}},{id:129,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Object lifecycle",question:"Your application generates reports stored in Cloud Storage. Reports are accessed daily for 30 days, then rarely. After 365 days, they can be deleted. How should you configure this?",options:["Create lifecycle policy with age condition to transition to Nearline at 30 days and delete at 365 days","Manually archive files monthly","Use Cloud Scheduler to delete old files","Store in Archive class from the start"],correct:0,explanation:"Object Lifecycle Management policies automate storage class transitions and deletions based on age, creation date, or other conditions. Configuration: 1) Transition to Nearline storage after 30 days (occasional access, lower storage cost), 2) Delete objects after 365 days. This optimizes costs automatically without manual intervention. Lifecycle policies are free and execute automatically across all objects matching conditions.",wrongExplanations:{1:"Manual archival doesn't scale, is error-prone, requires ongoing operational effort, and may miss files. Lifecycle policies handle this automatically and reliably across millions of objects without human intervention.",2:"Cloud Scheduler triggering deletion adds complexity and requires custom implementation. Lifecycle policies provide this functionality natively with better integration and reliability. Don't build what's provided as a managed feature.",3:"Archive storage has high retrieval costs and 365-day minimum storage duration. Storing reports there from start would incur expensive retrieval costs for the frequent first-30-day access period."}},{id:130,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Managing networking - Static IPs",question:"Your VM needs to retain its external IP address permanently, even after stop/start cycles or deletion/recreation. What should you configure?",options:["Reserve a static external IP address and assign it to the VM","Use ephemeral IP address which persists through stop/start","Configure DNS to always point to the VM","Use Cloud NAT for persistent addressing"],correct:0,explanation:"Static (reserved) external IP addresses remain associated with your project until explicitly released. They persist through VM stop/start cycles and can be reassigned to different VMs. Reserve with 'gcloud compute addresses create' then assign to VM. Static IPs incur small hourly charge when not attached to running resources but provide persistent public addressing.",wrongExplanations:{1:"Ephemeral external IPs persist through stop/start of the SAME VM but are released when VM is deleted. They're not truly permanent and can't be moved between VMs. For permanent addressing, static IPs are required.",2:"DNS pointing doesn't make the underlying IP address persistent. If VM is deleted or IP changes, DNS will point to wrong address. You need static IP for the address itself to be permanent.",3:"Cloud NAT provides outbound connectivity for VMs without external IPs. It doesn't provide persistent inbound addressing or external IPs for VMs."}},{id:131,domain:"Configuring access and security",subdomain:"4.1 IAM - Policy troubleshooting",question:"A user reports they cannot create VMs despite having Compute Instance Admin role at project level. What should you check first?",options:["Check if organizational policies restrict VM creation or resource locations","Grant them Owner role instead","Verify their Google account is active","Check VPC firewall rules"],correct:0,explanation:"Organizational policies can override IAM permissions. Policies like 'constraints/compute.vmExternalIpAccess' or 'constraints/gcp.resourceLocations' can prevent VM creation even with proper IAM roles. Always check organizational policies when users have appropriate IAM but can't perform actions. Use 'gcloud resource-manager org-policies list' to view active policies.",wrongExplanations:{1:"Owner role doesn't solve organizational policy restrictions. Policies apply even to Owners. Granting excessive permissions doesn't fix policy-based denials and violates least privilege.",2:"If account was inactive, user couldn't log in at all. Since they're reporting inability to create VMs specifically, account is clearly active. Account status wouldn't cause this specific issue.",3:"Firewall rules control network traffic between resources, not VM creation permissions. Firewall rules have nothing to do with IAM or ability to create VMs."}},{id:132,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - GPU instances",question:"Your machine learning workload requires GPUs for training models. What must you do before creating GPU-attached VMs?",options:["Request GPU quota increase for your project in the specific region and zone","Enable the GPU API","Create a custom machine type","Install GPU drivers before attaching GPU"],correct:0,explanation:"GPU quota is separate from CPU quota and defaults to 0 in most projects/regions. You must request GPU quota increase through IAM & Admin > Quotas page, specifying GPU type (K80, P4, T4, V100, A100, etc.) and region/zone. Once granted, you can create GPU-attached VMs. GPU availability also varies by zone, so check availability for your target zone.",wrongExplanations:{1:"There's no separate 'GPU API' to enable. GPUs are part of Compute Engine API. If you can create VMs, the necessary APIs are already enabled.",2:"Custom machine types are for CPU/memory combinations. GPU attachment is independent of machine type customization. You can attach GPUs to predefined or custom machine types.",3:"GPU drivers are installed AFTER VM creation, not before. You create the VM with GPU attached, then install appropriate drivers (NVIDIA CUDA, etc.) in the OS. Google provides GPU-optimized images with drivers pre-installed."}},{id:133,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Cloud SQL backup",question:"Your Cloud SQL instance needs automated backups with ability to restore to any point in time within the last 7 days. What should you configure?",options:["Enable automated backups and enable point-in-time recovery (binary logging)","Create manual snapshots daily via cron job","Export database to Cloud Storage daily","Enable Cloud SQL high availability only"],correct:0,explanation:"Cloud SQL automated backups + point-in-time recovery (PITR) provide: 1) Automatic daily backups, 2) Binary logs (MySQL) or write-ahead logs (PostgreSQL) enabling restore to any second within retention period, 3) 7-day retention by default (configurable up to 365 days), 4) Automated and reliable. PITR requires additional storage for logs but provides granular recovery not possible with daily backups alone.",wrongExplanations:{1:"Manual snapshots via cron require custom implementation, can fail without alerting, don't provide point-in-time recovery between snapshots, and require ongoing maintenance. Cloud SQL's native backup solution is superior.",2:"Exporting to Cloud Storage provides data copy but requires custom restore process, doesn't integrate with Cloud SQL restore features, and doesn't provide point-in-time recovery capability.",3:"High availability prevents downtime from zone failures but doesn't provide backup/restore capabilities. HA is for availability, not disaster recovery. You need both HA and backups for complete protection."}},{id:134,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Custom metrics",question:"Your application tracks business metrics (orders per minute, revenue) that you want to monitor and alert on. How should you send these metrics to Cloud Monitoring?",options:["Use Cloud Monitoring API or client libraries to write custom metrics from your application","Write metrics to Cloud Logging and create log-based metrics","Store metrics in BigQuery and query periodically","Use Cloud Functions to scrape application endpoints"],correct:0,explanation:"Cloud Monitoring API and client libraries (available in Python, Java, Node.js, Go, etc.) allow applications to write custom metrics directly. Create custom metric descriptors, then write time series data points. Custom metrics integrate with all Cloud Monitoring features: dashboards, alerting, anomaly detection. This is the recommended approach for application-specific business metrics.",wrongExplanations:{1:"Log-based metrics work but are less efficient than native custom metrics. They require parsing log entries and have higher latency. Custom metrics via API are more performant and have better integration with monitoring features.",2:"BigQuery is for data warehousing and analysis, not real-time monitoring. Querying BigQuery periodically adds latency, complexity, and cost. Cloud Monitoring is purpose-built for metrics collection and alerting.",3:"Cloud Functions scraping adds latency, requires custom implementation, creates scaling challenges, and is less reliable than direct instrumentation. Applications should push metrics directly to Cloud Monitoring."}},{id:135,domain:"Configuring access and security",subdomain:"4.3 Security - Secret Manager",question:"Your application needs to access database credentials, API keys, and certificates. What is the recommended approach for managing these secrets?",options:["Store secrets in Secret Manager and access them programmatically using client libraries with automatic encryption and audit logging","Store secrets in environment variables","Commit secrets to version control encrypted with GPG","Store secrets in Cloud Storage with encryption"],correct:0,explanation:"Secret Manager is Google Cloud's purpose-built service for managing sensitive data: 1) Automatic encryption at rest and in transit, 2) Versioning of secrets, 3) Audit logging of all access, 4) IAM integration for access control, 5) Automatic rotation support, 6) Replication options. Access via API or client libraries. This is the recommended Google Cloud approach for centralized secret management.",wrongExplanations:{1:"Environment variables are visible to anyone with access to the VM/container and can be accidentally logged or exposed. They lack encryption, versioning, audit logging, and rotation capabilities that Secret Manager provides.",2:"Version control, even with encryption, is inappropriate for secrets: 1) Secrets remain in Git history permanently, 2) Anyone with repo access can decrypt, 3) No audit trail of secret access, 4) Difficult to rotate secrets, 5) Version control is for code, not secrets.",3:"Cloud Storage lacks specialized secret management features: no native versioning, requires custom access patterns, doesn't provide automatic audit logging of secret access, no built-in rotation support. Use purpose-built Secret Manager instead."}},{id:136,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Container options",question:"You have a containerized application that needs to scale to zero when idle, startup in under 1 second, and you want minimal operational overhead. What should you use?",options:["Cloud Run for fully managed container execution with automatic scale-to-zero","GKE Autopilot with custom scaling configuration","Cloud Functions repackaging your container","Compute Engine with container-optimized OS"],correct:0,explanation:"Cloud Run is specifically designed for this use case: 1) Fully managed (no infrastructure management), 2) Automatic scale-to-zero (no charges when idle), 3) Fast cold starts (typically under 1 second), 4) Supports any container, 5) Pay only for request processing time, 6) Built-in HTTPS endpoints, 7) Request-based autoscaling. Cloud Run is the simplest option for stateless containerized applications.",wrongExplanations:{1:"GKE Autopilot can scale to zero but requires more complex configuration and has Kubernetes overhead. For simple containerized apps, Cloud Run provides simpler solution with same scale-to-zero benefits.",2:"Cloud Functions requires specific language runtimes and doesn't directly support existing containers. You'd need to repackage your application, losing the benefit of existing container images.",3:"Compute Engine with containers requires managing VMs, doesn't scale to zero automatically, requires manual autoscaling configuration, and has much longer startup times than Cloud Run."}},{id:137,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Custom images",question:"You need to deploy 100 identical VMs with pre-installed software and configurations. What is the most efficient approach?",options:["Create a custom image from a configured VM, then create new VMs from that image","Use startup scripts to install software on each VM","Manually configure each VM after creation","Use Cloud Deployment Manager with inline scripts"],correct:0,explanation:"Custom images capture entire disk state including OS, software, and configurations. Create base VM, install software, create image, then use image to create new VMs. Benefits: 1) Fastest deployment (no installation time per VM), 2) Guaranteed consistency across all VMs, 3) Reduced bandwidth (no repeated downloads), 4) Lower error rate than repeated installations. Command: 'gcloud compute images create IMAGE --source-disk=SOURCE_DISK'.",wrongExplanations:{1:"Startup scripts install software on every VM boot, increasing deployment time significantly. For 100 VMs, this means 100x downloads and installations. Slower, consumes more bandwidth, and risks installation failures on some VMs.",2:"Manual configuration of 100 VMs is time-consuming, error-prone, not reproducible, and doesn't scale. Custom images eliminate manual work and ensure consistency.",3:"Deployment Manager with scripts still requires running installation on each VM. While automated, it's slower than using pre-configured images and consumes more resources."}},{id:138,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Load balancer SSL policies",question:"Your HTTPS load balancer needs to support only modern browsers with TLS 1.2+ and strong cipher suites for security compliance. What should you configure?",options:["Create and apply an SSL policy with MODERN or RESTRICTED profile to the load balancer","SSL policies are automatic and cannot be customized","Configure SSL policies on backend VMs","Use Cloud Armor for SSL configuration"],correct:0,explanation:"SSL policies define minimum TLS version and cipher suites for HTTPS load balancers. Google provides profiles: COMPATIBLE (broadest support, TLS 1.0+), MODERN (TLS 1.2+ with secure ciphers), RESTRICTED (TLS 1.2+ with most secure ciphers only), CUSTOM (define specific ciphers). Create policy: 'gcloud compute ssl-policies create POLICY --profile MODERN' then apply to load balancer. This enforces TLS requirements at load balancer level before reaching backends.",wrongExplanations:{1:"SSL policies are customizable and should be configured to meet security requirements. Default policy may allow older TLS versions not meeting compliance requirements. Explicit policy configuration is needed.",2:"SSL policies apply to load balancer (SSL termination point), not backend VMs. Load balancer handles SSL/TLS negotiation with clients, backends typically receive HTTP (unencrypted) traffic from load balancer.",3:"Cloud Armor provides DDoS protection and WAF features, not SSL/TLS configuration. SSL policies are separate feature in load balancing configuration."}},{id:139,domain:"Configuring access and security",subdomain:"4.1 IAM - Service account keys",question:"Your on-premises application needs to access Google Cloud APIs. What is the recommended authentication approach?",options:["Create service account key file, download JSON, and use in application with Application Default Credentials","Use user account credentials with OAuth","Create API key in Cloud Console","Use no authentication for internal APIs"],correct:0,explanation:"For applications running outside Google Cloud (on-premises, other clouds), service account keys are the recommended authentication method. Download JSON key file, store securely, set GOOGLE_APPLICATION_CREDENTIALS environment variable pointing to key file, use client libraries with Application Default Credentials. While keys require careful management, they're the standard approach for external applications. Consider Workload Identity Federation for even better security.",wrongExplanations:{1:"User account credentials tie authentication to individual humans, not services. This breaks when user leaves, makes auditing difficult, and violates principle of service identity. Services should use service accounts.",2:"API keys provide only identification, not authentication or authorization. They're for quota management and shouldn't be used for accessing protected resources. Service accounts provide proper authentication and IAM integration.",3:"All Google Cloud API access requires authentication. 'Internal' APIs still need proper authentication to enforce IAM policies and provide audit trails. Never leave APIs unauthenticated."}},{id:140,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - DNS",question:"You need to host your company's DNS zone (example.com) in Google Cloud with automatic DNSSEC. What should you use?",options:["Cloud DNS managed zone with DNSSEC enabled","Compute Engine VM running BIND","Use third-party DNS only","Cloud Load Balancer for DNS"],correct:0,explanation:"Cloud DNS is Google's managed, authoritative DNS service providing: 1) High availability (100% SLA), 2) Low latency via global anycast, 3) Automatic DNSSEC support, 4) Support for public and private zones, 5) Geo-routing and weighted routing, 6) API-driven for automation, 7) Integration with other Google Cloud services. Create managed zone: 'gcloud dns managed-zones create ZONE --dns-name=example.com --dnssec-state=on'.",wrongExplanations:{1:"Self-managed DNS on Compute Engine requires: VM management, software updates, HA configuration, backup strategy, monitoring, and security hardening. Cloud DNS eliminates all operational overhead while providing better reliability and performance.",2:"Third-party DNS providers work but don't integrate with Google Cloud features and may have higher latency for users accessing Cloud resources. Cloud DNS provides native integration and Google's network benefits.",3:"Load balancers distribute traffic to backends, they don't provide DNS hosting services. Cloud DNS and load balancers serve different purposes - DNS resolves names to IPs, load balancers distribute traffic."}},{id:141,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Transfer Service",question:"You need to migrate 500TB of data from AWS S3 to Cloud Storage. What is the recommended approach for large-scale transfer?",options:["Use Storage Transfer Service which handles large-scale transfers efficiently between cloud providers","Use gsutil rsync command from local workstation","Write custom multi-threaded application","Download to local disk then upload to GCS"],correct:0,explanation:"Storage Transfer Service is designed for large-scale data transfers: 1) Optimized for cloud-to-cloud transfers (AWS S3, Azure Blob, Google Cloud Storage), 2) Parallel transfers for high throughput, 3) Handles retries automatically, 4) Scheduled and one-time transfers, 5) Filters by filename, prefix, date, 6) Integrity verification, 7) Optional deletion of source objects after transfer. For 500TB, this is far more reliable and efficient than manual methods.",wrongExplanations:{1:"gsutil from local workstation limits transfer speed to your internet bandwidth. For 500TB, this could take weeks or months. Additionally, local workstation reliability issues could interrupt transfer. Storage Transfer Service uses Google's network backbone for much faster transfer.",2:"Custom applications require significant development, testing, error handling, resumption logic, and monitoring. Storage Transfer Service provides all this as managed service. Never build what's available as managed service.",3:"Downloading 500TB to local disk then uploading doubles transfer time and requires massive local storage. Storage Transfer Service performs direct cloud-to-cloud transfer using high-bandwidth cloud networks, avoiding local bottleneck entirely."}},{id:142,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Filestore",question:"Your application running on GKE needs shared file system (NFS) accessible by multiple pods simultaneously for read/write. What should you use?",options:["Cloud Filestore which provides managed NFS file shares accessible from GKE","Persistent volumes with ReadWriteOnce access mode","Cloud Storage FUSE mount","Host path volumes"],correct:0,explanation:"Cloud Filestore is Google's managed NFS service providing: 1) POSIX-compliant file system, 2) Multiple concurrent readers/writers (ReadWriteMany in K8s), 3) Low latency suitable for application data, 4) Snapshots and backups, 5) Native integration with GKE. Create Filestore instance, mount as PersistentVolume with ReadWriteMany access mode. Perfect for shared application state, home directories, or content management systems.",wrongExplanations:{1:"ReadWriteOnce persistent volumes can only be attached to one node at a time. Multiple pods on same node can access it, but not across nodes. This doesn't meet requirement for multiple pods across cluster to access simultaneously.",2:"Cloud Storage FUSE provides object storage semantics, not true file system. Has limitations: high latency, eventual consistency for some operations, limited POSIX compliance. Not suitable for applications requiring true shared file system with concurrent writes.",3:"Host path volumes are node-local storage, not shared across nodes. Each node has different host path content. Can't be used for shared data across multiple pods on different nodes."}},{id:143,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Impersonation",question:"You need to grant a developer temporary ability to act as a service account for testing without giving them the service account key. What should you do?",options:["Grant the developer Service Account Token Creator role to impersonate the service account","Give developer the service account key file","Add developer's user account to the service account","Create new service account for developer"],correct:0,explanation:"Service account impersonation allows users to temporarily assume service account identity without keys. Grant 'roles/iam.serviceAccountTokenCreator' on the service account. Developer can then use 'gcloud --impersonate-service-account' or API calls to act as the service account. Benefits: 1) No key files to manage, 2) Auditable (logs show who impersonated), 3) Can be revoked instantly, 4) Time-limited tokens. This is much more secure than distributing keys.",wrongExplanations:{1:"Distributing key files creates security risks: keys can be stolen, don't rotate automatically, remain valid until deleted, harder to audit. Impersonation is more secure as it uses short-lived tokens and maintains audit trail.",2:"You cannot add user accounts 'to' service accounts. Service accounts are separate identities. Impersonation is the mechanism to allow users to act as service accounts.",3:"Creating new service account per developer defeats purpose of service account (representing service, not individual). Multiplies IAM management overhead and doesn't solve testing problem of using actual service account identity."}},{id:144,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Sole-tenant nodes",question:"Your workload has regulatory requirements that VMs must run on dedicated physical servers not shared with other Google customers. What should you use?",options:["Sole-tenant nodes which provide dedicated physical servers for your VMs","Preemptible VMs for isolation","Custom machine types","Shielded VMs for security"],correct:0,explanation:"Sole-tenant nodes are physical Compute Engine servers dedicated exclusively to your project. Provides: 1) Physical isolation from other customers, 2) Ability to group VMs on same hardware, 3) Access to same machine types as regular VMs, 4) Meet regulatory requirements for dedicated hardware, 5) Useful for BYOL scenarios requiring host affinity. Create node group, then create VMs specifying node group. Costs more but provides required physical isolation.",wrongExplanations:{1:"Preemptible VMs are for cost savings, not isolation. They run on shared infrastructure alongside other customers' VMs, just like regular VMs. They provide no additional physical isolation.",2:"Custom machine types affect CPU/memory configuration but don't provide dedicated hardware. Custom machines still run on shared physical infrastructure with other customers.",3:"Shielded VMs provide boot security features (Secure Boot, vTPM, integrity monitoring) but run on shared infrastructure. They protect against boot-level attacks, not provide physical isolation from other customers."}},{id:145,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - BigQuery costs",question:"Your BigQuery costs are high due to analysts running ad-hoc queries scanning large amounts of data. How can you provide cost controls while maintaining query access?",options:["Create custom cost controls by setting query cost limits per user and implementing query quotas","Switch to flat-rate pricing for everyone","Disable BigQuery access","Only allow scheduled queries"],correct:0,explanation:"BigQuery provides cost control features: 1) Custom query quotas per user/project, 2) Maximum bytes billed settings, 3) Required table partitioning, 4) Query costs preview before execution, 5) Cost controls in Data Studio/Looker, 6) IAM policies limiting query capabilities. Set project-level or user-level quotas: 'bq update --project_id=PROJECT --max_bytes_billed=BYTES'. Analysts can still query but within cost boundaries.",wrongExplanations:{1:"Flat-rate pricing ($40k-$170k/year minimum) only makes sense for very high query volumes. For teams with analysts running ad-hoc queries, on-demand pricing with quotas is more cost-effective and flexible.",2:"Disabling access defeats the purpose of having BigQuery. Analytics teams need query access. The solution is controlling costs, not eliminating access.",3:"Scheduled queries are useful but don't address ad-hoc analysis needs. Analysts need interactive query capability for exploration and investigation. Cost controls allow both scheduled and ad-hoc queries within budget."}},{id:146,domain:"Configuring access and security",subdomain:"4.3 Security - Cloud Armor",question:"Your public-facing application needs protection against DDoS attacks and application-layer attacks. What should you configure?",options:["Cloud Armor security policies on your load balancer with rate limiting and WAF rules","VPC firewall rules only","Cloud IDS for intrusion detection","Binary Authorization for container security"],correct:0,explanation:"Cloud Armor provides: 1) DDoS protection (L3/L4 and L7), 2) WAF rules for OWASP Top 10 protection, 3) Rate limiting per IP/region, 4) Geo-based access control, 5) Custom rules based on request attributes, 6) Bot management, 7) Adaptive protection using machine learning. Applied to HTTP(S) load balancers. Essential for protecting public applications from malicious traffic.",wrongExplanations:{1:"VPC firewall rules provide basic L4 filtering but lack L7 application awareness, rate limiting, WAF rules, or DDoS mitigation capabilities. They can't inspect HTTP requests or detect application-layer attacks.",2:"Cloud IDS provides network intrusion detection but is designed for internal traffic inspection, not DDoS protection or WAF for public applications. It detects intrusions but doesn't actively block them like Cloud Armor.",3:"Binary Authorization ensures only approved containers run in GKE. It's for supply chain security, not runtime protection against network attacks or malicious users. Completely different security layer."}},{id:147,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Profiler",question:"Your application has high CPU usage and you need to identify which functions are consuming most CPU time. What tool should you use?",options:["Cloud Profiler to analyze CPU and memory usage at the function level with flame graphs","Cloud Monitoring CPU metrics only","Cloud Trace for latency analysis","Cloud Logging for application logs"],correct:0,explanation:"Cloud Profiler is a continuous profiling tool that: 1) Shows which functions consume CPU and memory, 2) Provides flame graphs visualizing call stacks, 3) Low overhead (safe for production), 4) Compares performance across versions, 5) Works with multiple languages (Java, Go, Python, Node.js), 6) Identifies hotspots and optimization opportunities. Install profiler agent in application code. Perfect for optimizing application performance at code level.",wrongExplanations:{1:"Cloud Monitoring CPU metrics show overall resource usage but don't identify which code/functions are responsible. You see symptoms (high CPU) but not root cause (which functions). Profiler provides the function-level detail needed for optimization.",2:"Cloud Trace shows request flow and latency between services but doesn't profile CPU usage within individual functions. Trace is for distributed system latency, Profiler is for CPU/memory optimization.",3:"Cloud Logging captures application output but logs don't show CPU profiling data unless you instrument code to log profiling info (complex and high overhead). Profiler is purpose-built for performance analysis."}},{id:148,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Confidential VMs",question:"Your workload processes sensitive data and requires memory encryption. What type of VM should you use?",options:["Confidential VMs which provide memory encryption using AMD SEV","Shielded VMs for boot security","Standard VMs with encrypted persistent disks","Sole-tenant nodes for isolation"],correct:0,explanation:"Confidential VMs provide: 1) Memory encryption using AMD SEV (Secure Encrypted Virtualization), 2) Encryption keys generated in hardware, not accessible to Google, 3) Protection of data in use (in memory), 4) Built on N2D machine types with AMD EPYC processors, 5) Minimal performance overhead. Create with '--confidential-compute' flag. Complements encryption at rest (disks) and in transit (network) with encryption of data in use.",wrongExplanations:{1:"Shielded VMs provide boot security (Secure Boot, vTPM, integrity monitoring) but don't encrypt memory. They protect boot process and detect malware at boot, but data in memory is not encrypted.",2:"Encrypted persistent disks protect data at rest but not data in memory while being processed. Disk encryption and memory encryption solve different problems and are complementary.",3:"Sole-tenant nodes provide physical isolation but don't encrypt memory. VMs on sole-tenant nodes process data in clear memory just like regular VMs. Physical isolation  memory encryption."}},{id:149,domain:"Configuring access and security",subdomain:"4.4 Compliance - Cloud Asset Inventory",question:"You need to track all resources in your organization for security audits and compliance. What tool provides inventory and history of all resources?",options:["Cloud Asset Inventory which tracks resource configurations and changes over time","Cloud Monitoring for resource tracking","Cloud Logging export to BigQuery","Manual spreadsheet of resources"],correct:0,explanation:"Cloud Asset Inventory: 1) Maintains inventory of all GCP resources, 2) Tracks IAM policies, 3) Provides resource history and change tracking, 4) Supports querying and searching across organization, 5) Exports to BigQuery for analysis, 6) Real-time feed of asset changes, 7) Essential for compliance, security analysis, and auditing. Query with 'gcloud asset search-all-resources'.",wrongExplanations:{1:"Cloud Monitoring tracks metrics and logs but isn't designed for comprehensive resource inventory. It shows operational state but not complete resource catalog with configurations and policies.",2:"Cloud Logging captures events but doesn't maintain comprehensive current inventory of all resources. You'd need complex queries to reconstruct resource inventory from logs, and historical log data may not be retained.",3:"Manual spreadsheets can't keep pace with dynamic cloud environments. Resources are created and deleted programmatically. Manual tracking is outdated immediately and prone to errors. Cloud Asset Inventory provides automated, always-current inventory."}},{id:150,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your web application needs automatic scaling, zero server management, and supports Python. Traffic varies from 10 to 10,000 requests per second. What should you use?",options:["App Engine Standard with automatic scaling","App Engine Flexible with manual scaling","Compute Engine with autoscaling","Cloud Functions"],correct:0,explanation:"App Engine Standard provides true zero-ops with automatic scaling from 0 to thousands of instances within seconds, built-in load balancing, and pay-per-use pricing.",wrongExplanations:{1:"Flexible with manual scaling doesn't autoscale and requires specifying instance count, defeating the purpose for variable traffic.",2:"Compute Engine requires managing VMs, configuring autoscaling, and load balancers - much more overhead than App Engine Standard.",3:"Cloud Functions is for event-driven workloads and short executions, not full web applications needing longer request handling."}},{id:151,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"Your application needs sub-millisecond latency for cache lookups with Redis compatibility. What should you use?",options:["Memorystore for Redis providing fully managed Redis with high availability","Cloud SQL with query caching","BigQuery with BI Engine","Persistent disk for cache storage"],correct:0,explanation:"Memorystore for Redis is Google's managed Redis service providing sub-millisecond latency, automatic failover, backup/restore, and Redis compatibility.",wrongExplanations:{1:"Cloud SQL is a relational database, not an in-memory cache. Query times are milliseconds to seconds, not sub-millisecond.",2:"BigQuery is for analytics, not application caching. BI Engine accelerates queries but isn't a general-purpose cache.",3:"Persistent disks have millisecond latencies, not sub-millisecond. Disk-based storage can't match in-memory cache performance."}},{id:152,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Private Service Connect",question:"You need to access a Google API privately without traffic leaving your VPC to the internet. What should you configure?",options:["Private Service Connect endpoint for Google APIs","Cloud NAT for API access","VPC Peering with Google","Public internet with SSL"],correct:0,explanation:"Private Service Connect allows accessing Google APIs through internal IP addresses in your VPC without internet traversal, providing security and lower latency.",wrongExplanations:{1:"Cloud NAT provides internet egress but traffic still goes to public Google API endpoints, not staying within VPC network.",2:"You cannot VPC peer with Google's production networks. Private Service Connect is the mechanism for private API access.",3:"Public internet access doesn't meet the requirement of keeping traffic within VPC network."}},{id:153,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Metadata server",question:"Your application running on Compute Engine needs to retrieve its own external IP address. What is the recommended method?",options:["Query the metadata server at http://metadata.google.internal/computeMetadata/v1/","Parse output of ifconfig command","Hard-code the IP in application configuration","Use external service to detect IP"],correct:0,explanation:"Metadata server provides instance information including external IP, zone, tags, service accounts. Query with 'Metadata-Flavor: Google' header. Standard method for VMs to discover their configuration.",wrongExplanations:{1:"ifconfig shows internal network interfaces, not external IP assigned by Google Cloud.",2:"Hard-coding IPs breaks when VMs are recreated or IPs change. Metadata server provides current values dynamically.",3:"External services add latency and dependency. Metadata server is local and reliable."}},{id:154,domain:"Configuring access and security",subdomain:"4.1 IAM - Resource hierarchy permissions",question:"A user has Editor role at organization level and Viewer role at project level. What effective permissions does the user have in the project?",options:["Editor permissions - roles granted at higher levels override lower level restrictions","Viewer permissions - lower level takes precedence","No access - conflicting roles cancel out","Custom combination of both roles"],correct:0,explanation:"IAM uses union of permissions - more permissive role wins. Organization-level Editor includes all Viewer permissions and more. Permissions inherit down and accumulate, cannot be reduced at lower levels.",wrongExplanations:{1:"Lower levels cannot restrict permissions granted at higher levels. Organization-level Editor provides Editor access everywhere below.",2:"IAM doesn't cancel permissions. If conflicting roles grant different access, the union of all permissions applies.",3:"Roles combine additively, not as custom mix. User gets maximum permissions granted at any level."}},{id:155,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Bigtable",question:"Your IoT application generates 1 million writes per second with low-latency read requirements. Data is time-series sensor readings. What database should you use?",options:["Cloud Bigtable designed for high-throughput, low-latency time-series data","Cloud SQL for relational data","Cloud Storage for object storage","Firestore for document storage"],correct:0,explanation:"Bigtable handles millions of writes per second with single-digit millisecond latency. Perfect for time-series data, IoT, analytics. Scales horizontally, integrates with data processing tools.",wrongExplanations:{1:"Cloud SQL maxes at thousands of writes per second, not millions. Designed for OLTP, not massive time-series ingestion.",2:"Cloud Storage is for objects/files, not high-throughput database operations with low-latency reads.",3:"Firestore handles thousands of writes per second, not millions. Better for mobile/web apps than IoT time-series at this scale."}},{id:156,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Object holds",question:"You must prevent deletion of critical objects in Cloud Storage for legal reasons even by users with delete permissions. What should you configure?",options:["Object holds (event-based or temporary) prevent deletion regardless of IAM permissions","Remove delete permissions from all users","Make bucket requester pays","Enable versioning only"],correct:0,explanation:"Object holds override IAM permissions. Event-based holds prevent deletion until hold is removed. Temporary holds last until specific date. Critical for legal/compliance requirements where even admins can't delete.",wrongExplanations:{1:"Removing delete permissions doesn't work if users regain permissions or use elevated accounts. Holds enforce retention regardless of permissions.",2:"Requester pays affects billing for access, not deletion protection. Completely unrelated to preventing deletion.",3:"Versioning retains deleted object versions but doesn't prevent deletion. Objects can still be deleted, they just remain as non-current versions."}},{id:157,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Batch workloads",question:"You have nightly data processing jobs taking 6 hours. Jobs can be paused and resumed. How should you minimize costs?",options:["Use Spot VMs with checkpointing to handle interruptions and achieve 60-91% cost savings","Use committed use discounts for 24/7 capacity","Run on App Engine Flexible","Use Cloud Functions with maximum timeout"],correct:0,explanation:"Spot VMs perfect for batch jobs that can tolerate interruptions. Implement checkpointing to save progress. 60-91% discount vs on-demand. For 6-hour nightly jobs, massive savings with acceptable interruption risk.",wrongExplanations:{1:"CUDs require 24/7 commitment but jobs only run 6 hours daily (25% utilization). Spot VMs are cheaper for intermittent workloads.",2:"App Engine Flexible isn't designed for long-running batch processing. More expensive and complex than Compute Engine for this use case.",3:"Cloud Functions has 60-minute maximum timeout. Can't run 6-hour jobs. Wrong tool for long batch processing."}},{id:158,domain:"Configuring access and security",subdomain:"4.3 Security - VPC Service Controls",question:"You need to prevent data exfiltration from BigQuery to external locations. What should you implement?",options:["VPC Service Controls perimeter around BigQuery project allowing only authorized access","IAM deny policies for external destinations","Firewall rules blocking egress","Remove all user permissions"],correct:0,explanation:"VPC Service Controls create security perimeter preventing data leaving defined resources even with IAM permissions. Blocks copying to external projects, on-premises, or internet. Primary defense against exfiltration.",wrongExplanations:{1:"IAM controls who can access but can't prevent authorized users from copying data to external locations. VPC Service Controls enforce perimeter restrictions.",2:"Firewall rules apply to Compute Engine network traffic, not API-level data access like BigQuery queries or exports.",3:"Removing permissions breaks legitimate use. VPC Service Controls allow normal operations while preventing data leaving perimeter."}},{id:159,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - SLOs",question:"You want to track that 99.9% of requests complete in under 500ms. What Cloud Monitoring feature should you use?",options:["Create Service Level Objective (SLO) with latency SLI and error budget tracking","Create alerting policy only","Use uptime checks","Export metrics to spreadsheet"],correct:0,explanation:"SLOs in Cloud Monitoring track service reliability against targets. Define SLI (latency <500ms), set target (99.9%), monitor error budget. Provides visibility into service health and reliability trends.",wrongExplanations:{1:"Alerts notify when threshold crossed but don't track reliability over time or error budget consumption. SLOs provide comprehensive reliability tracking.",2:"Uptime checks monitor availability, not request latency percentiles. Can't track 99.9% of requests meeting latency target.",3:"Manual spreadsheet tracking is error-prone and doesn't provide real-time monitoring or error budget alerts."}},{id:160,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud Interconnect",question:"You need 10 Gbps private connection to Google Cloud that doesn't traverse internet. Your office is 500 miles from nearest Google facility. What should you use?",options:["Partner Interconnect which provides 10 Gbps through supported service provider","Dedicated Interconnect requiring colocation","Cloud VPN maxing at 3 Gbps","Public internet connection"],correct:0,explanation:"Partner Interconnect provides enterprise-grade connectivity through service provider network when you can't colocate. Supports 50 Mbps to 50 Gbps. Doesn't require Google facility proximity. Private connection without internet traversal.",wrongExplanations:{1:"Dedicated Interconnect requires physical connection in Google colocation facility. At 500 miles away, not practical. Partner Interconnect solves distance issue.",2:"Cloud VPN maxes at 3 Gbps per tunnel and traverses internet. Doesn't meet 10 Gbps or private connection requirements.",3:"Public internet doesn't provide private connection or bandwidth guarantees. Violates requirements."}},{id:161,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Short-lived credentials",question:"Your application needs service account token valid for only 5 minutes. How should you generate it?",options:["Use Service Account Credentials API to generate short-lived access token with custom lifetime","Create service account key and rotate every 5 minutes","Use user credentials instead","No way to create tokens shorter than 1 hour"],correct:0,explanation:"Service Account Credentials API generates OAuth access tokens with configurable lifetime (as short as 5 minutes). Use 'iam.serviceAccounts.generateAccessToken'. Much more secure than long-lived keys. Automatic expiration.",wrongExplanations:{1:"Rotating keys every 5 minutes is operationally complex and unnecessary. Keys are meant to be long-lived. Short-lived tokens are the proper solution.",2:"User credentials tie authentication to individuals, not services. Doesn't solve need for short-lived service tokens.",3:"Service Account Credentials API explicitly supports creating tokens with custom expiration as short as 1 second."}},{id:162,domain:"Setting up a cloud solution environment",subdomain:"1.3 Setting up networking - Routes",question:"Your VMs need to route traffic to on-premises 10.1.0.0/16 network through Cloud VPN. What should you configure?",options:["Custom route with destination 10.1.0.0/16 and next-hop as VPN tunnel","Firewall rule allowing 10.1.0.0/16","NAT rule for translation","VPC Peering configuration"],correct:0,explanation:"Routes control traffic forwarding. Create custom route: 'gcloud compute routes create vpn-route --destination-range=10.1.0.0/16 --next-hop-vpn-tunnel=TUNNEL'. BGP can exchange routes automatically, but static routes work too.",wrongExplanations:{1:"Firewall rules permit/deny traffic but don't control routing. Even with allow rule, traffic won't reach on-premises without route.",2:"NAT translates IP addresses, doesn't route traffic. VPN connectivity requires proper routing, not NAT.",3:"VPC Peering connects two VPCs in Google Cloud. Can't peer with on-premises networks. VPN requires route configuration."}},{id:163,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Serial console",question:"Your VM is unresponsive to SSH. How can you troubleshoot if it's booting correctly?",options:["Access serial console output to view boot messages and system logs","Delete and recreate the VM","Wait for automatic recovery","Check Cloud Monitoring only"],correct:0,explanation:"Serial console provides access to VM's serial port output showing boot messages, kernel logs, login prompts. Essential for troubleshooting when SSH unavailable. View with 'gcloud compute instances get-serial-port-output'.",wrongExplanations:{1:"Deleting VM loses ability to diagnose issue and may destroy evidence of root cause. Serial console enables troubleshooting without destruction.",2:"VMs don't automatically recover from many issues. Serial console helps identify problem so you can fix it.",3:"Monitoring shows metrics but not boot logs or system errors visible in serial console."}},{id:164,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Transfer Appliance",question:"You need to transfer 500TB to Google Cloud but have limited bandwidth (10 Mbps). What is the most practical approach?",options:["Request Transfer Appliance, ship physical device with your data to Google","Use gsutil over internet connection","Use Storage Transfer Service","Mail hard drives directly"],correct:0,explanation:"Transfer Appliance is physical device Google ships for large offline data transfer. Load data locally at high speed, ship back to Google. Practical for multi-TB transfers with limited bandwidth. 10 Mbps would take years for 500TB.",wrongExplanations:{1:"gsutil over 10 Mbps for 500TB would take approximately 12 years. Completely impractical. Physical transfer is only viable option.",2:"Storage Transfer Service requires internet connectivity and is subject to same bandwidth limitations. No benefit over gsutil for this scenario.",3:"Google doesn't accept random hard drives. Transfer Appliance is the official physical transfer solution with encryption, tracking, and support."}},{id:165,domain:"Configuring access and security",subdomain:"4.1 IAM - Domain restricted sharing",question:"You need to ensure resources can only be shared with users from your organization's domain. What should you configure?",options:["Organization policy constraint 'iam.allowedPolicyMemberDomains' limiting to your domain","Manually review all IAM bindings","Remove all external users","Enable Cloud Identity only"],correct:0,explanation:"The 'iam.allowedPolicyMemberDomains' organizational policy prevents sharing resources with users outside specified domains. Enforces domain restriction across organization. Preventive control.",wrongExplanations:{1:"Manual review doesn't prevent future violations and doesn't scale. Organizational policy enforces rule automatically.",2:"Removing external users is reactive and one-time. New external users could be added. Policy prevents addition in first place.",3:"Cloud Identity provides user management but doesn't restrict resource sharing. Policy enforcement needed."}},{id:166,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Regional replication",question:"Your Cloud Storage bucket must replicate data to another region for disaster recovery. What should you configure?",options:["Use dual-region or multi-region bucket for automatic replication","Create script to copy objects between regions","Use gsutil rsync scheduled job","Enable versioning only"],correct:0,explanation:"Dual-region buckets automatically replicate between two specific regions. Multi-region buckets replicate across multiple regions. Provides geo-redundancy automatically. Can't convert single-region to dual-region - must create new bucket.",wrongExplanations:{1:"Scripts require maintenance, can fail, have replication lag, and require monitoring. Dual/multi-region provides automatic, reliable replication.",2:"gsutil rsync works but is manual, has lag, costs more (egress charges), and requires automation infrastructure. Dual-region is simpler.",3:"Versioning preserves object history but doesn't replicate to other regions. Different feature solving different problem."}},{id:167,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Windows licensing",question:"You want to use your existing Windows Server licenses on Compute Engine. What should you do?",options:["Use sole-tenant nodes with Bring Your Own License (BYOL) for Windows Server","Use regular VMs with Google-provided licenses","Install Windows without license","Use App Engine for Windows workloads"],correct:0,explanation:"BYOL requires sole-tenant nodes for licensing compliance. Allows using existing licenses, avoiding Google's Windows licensing costs. Must have license mobility rights. Configure with '--node-group' specifying sole-tenant group.",wrongExplanations:{1:"Regular VMs include Windows licensing in per-second pricing. Can't bring your own licenses to shared infrastructure. Must use sole-tenant for BYOL.",2:"Running unlicensed Windows violates licensing terms and creates legal/compliance issues. Not a valid option.",3:"App Engine doesn't support Windows. It's for web applications in specific languages, not Windows Server workloads."}},{id:168,domain:"Configuring access and security",subdomain:"4.3 Security - Shielded VMs",question:"You need to ensure VMs boot only with verified firmware and kernel. What feature should you enable?",options:["Shielded VM with Secure Boot, vTPM, and integrity monitoring","Confidential VM for encryption","Sole-tenant nodes for isolation","Custom image with hardening"],correct:0,explanation:"Shielded VMs provide: 1) Secure Boot (verified bootloader/kernel), 2) vTPM (virtual Trusted Platform Module), 3) Integrity monitoring (detect boot-level changes). Protects against rootkits and bootkits. Enable with '--shielded-secure-boot'.",wrongExplanations:{1:"Confidential VMs encrypt memory but don't verify boot integrity. Different security layer for data in use, not boot security.",2:"Sole-tenant nodes provide physical isolation but don't verify boot integrity. VMs on sole-tenant nodes can still have compromised bootloaders.",3:"Custom hardening helps but doesn't provide hardware-backed boot verification that Shielded VMs offer through Secure Boot."}},{id:169,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Notification channels",question:"Your alerting policy needs to notify multiple teams via different methods (email, Slack, PagerDuty). How should you configure this?",options:["Add multiple notification channels to the alerting policy","Create separate alerting policies for each team","Use Cloud Functions to fan out notifications","Send emails only and let teams forward"],correct:0,explanation:"Alerting policies support multiple notification channels. Configure email, Slack, PagerDuty, SMS, webhooks, Pub/Sub. Each channel delivers to different team/system. Simple configuration, no custom code.",wrongExplanations:{1:"Separate policies for same condition creates management overhead. One policy can notify multiple channels. Duplicating policies causes confusion.",2:"Cloud Functions add unnecessary complexity. Cloud Monitoring natively supports multiple notification channels without custom code.",3:"Manual forwarding is unreliable and defeats purpose of automated alerting. Direct notification ensures timely delivery."}},{id:170,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - AlloyDB",question:"You need PostgreSQL-compatible database with 4x faster transaction performance than standard PostgreSQL. What should you use?",options:["AlloyDB for PostgreSQL which provides high-performance PostgreSQL with Google innovations","Cloud SQL for PostgreSQL","Cloud Spanner with PostgreSQL interface","Self-managed PostgreSQL on Compute Engine"],correct:0,explanation:"AlloyDB is Google's fully managed PostgreSQL-compatible database offering 4x faster transactions, 100x faster analytics queries, built-in caching, and column store. Combines transactional and analytical workloads. More advanced than Cloud SQL.",wrongExplanations:{1:"Cloud SQL PostgreSQL offers standard performance. AlloyDB provides significantly better performance through proprietary optimizations.",2:"Spanner PostgreSQL interface provides global scale but doesn't offer the same transaction performance or PostgreSQL compatibility as AlloyDB.",3:"Self-managed PostgreSQL on Compute Engine provides standard PostgreSQL performance without AlloyDB's proprietary optimizations and managed features."}},{id:171,domain:"Configuring access and security",subdomain:"4.1 IAM - Deny policies",question:"You need to prevent all users including Owners from deleting production VMs. What should you configure?",options:["IAM deny policy blocking compute.instances.delete for all principals on production VMs","Remove delete permissions from all users","Organization policy only","Firewall rules"],correct:0,explanation:"IAM deny policies override allow policies. Can block specific actions even for Owners. Apply to resources with tags/labels. 'gcloud iam policies create' with deny rules for compute.instances.delete. Enforces constraints that allow policies can't.",wrongExplanations:{1:"Removing permissions doesn't prevent Owners who have all permissions by default. Deny policies can override even Owner role.",2:"Organization policies provide constraints but IAM deny policies offer more granular control over specific actions. Both can work together.",3:"Firewall rules control network traffic, not IAM permissions for VM deletion. Completely different security layer."}},{id:172,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Instance templates",question:"You need to standardize VM configurations for development teams to ensure consistency. What should you create?",options:["Instance template defining machine type, boot disk, network, metadata for standardized VM creation","Custom image only","Documentation with manual steps","Startup script"],correct:0,explanation:"Instance templates are immutable VM configuration blueprints. Define all VM properties: machine type, image, disks, network, service account, labels. Used to create individual VMs or instance groups. Ensures consistency, simplifies deployment.",wrongExplanations:{1:"Custom images define disk content but not VM configuration (machine type, networking, etc.). Templates include image plus infrastructure configuration.",2:"Documentation requires manual following, error-prone, not enforced. Templates programmatically enforce standard configurations.",3:"Startup scripts configure software after boot but don't define VM infrastructure settings. Templates define complete VM specification."}},{id:173,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning networking - Cloud CDN",question:"Your global website has static assets (images, CSS, JavaScript) accessed by users worldwide. How can you reduce latency?",options:["Enable Cloud CDN on your load balancer to cache content at Google edge locations globally","Deploy VMs in every region","Use Cloud Storage only","Enable Premium Network Tier only"],correct:0,explanation:"Cloud CDN caches content at Google's 100+ edge locations worldwide. Dramatically reduces latency for static content. Enable on HTTP(S) load balancer with cache-mode. Supports custom cache keys, TTLs, cache invalidation.",wrongExplanations:{1:"Deploying VMs globally is expensive and complex. CDN provides caching without infrastructure in every location. VMs still needed for origin content.",2:"Cloud Storage alone doesn't provide edge caching. CDN caches Storage content at edge locations, reducing latency and Storage egress costs.",3:"Premium Tier improves network routing but doesn't cache content at edge. CDN and Premium Tier complement each other."}},{id:174,domain:"Configuring access and security",subdomain:"4.4 Compliance - Data residency",question:"Regulations require customer data stay within EU. How do you enforce this in Google Cloud?",options:["Set organization policy 'gcp.resourceLocations' restricting resources to EU regions only","Train developers to use EU regions only","Manually audit resources monthly","Use IAM policies"],correct:0,explanation:"Organization policy 'constraints/gcp.resourceLocations' enforces data residency by limiting resource creation to specified regions. Set to 'in:eu-locations' to restrict to EU. Preventive control, not reactive.",wrongExplanations:{1:"Training is good but not enforceable. Developers can make mistakes. Policy prevents creation outside EU automatically.",2:"Manual audit is reactive and allows violations to exist for up to a month. Policy prevents violations from occurring.",3:"IAM controls who can do what, not where resources can be created. Location restrictions require organizational policies."}},{id:175,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Bucket locking",question:"You need to prevent anyone including admins from reducing Cloud Storage bucket retention period once set. What should you enable?",options:["Retention policy with bucket lock to make retention period immutable","Object versioning","Lifecycle management","IAM policy only"],correct:0,explanation:"Bucket lock makes retention policy immutable. Once locked, retention period can only be increased, never decreased or removed. Critical for regulatory compliance. Lock is permanent and irreversible.",wrongExplanations:{1:"Versioning preserves object history but doesn't prevent retention policy changes. Different feature.",2:"Lifecycle management automates object lifecycle but doesn't lock retention policies. Can be modified or removed.",3:"IAM policies can be changed by admins. Bucket lock prevents even project owners from reducing retention."}},{id:176,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Accelerators",question:"Your video transcoding workload could benefit from hardware acceleration. What should you attach to VMs?",options:["GPU (T4, P4) for video encoding/decoding acceleration","TPU for machine learning","More vCPUs only","Local SSD for speed"],correct:0,explanation:"GPUs like NVIDIA T4 have hardware video encoders/decoders (NVENC/NVDEC) dramatically accelerating transcoding. Much faster and more power-efficient than CPU transcoding. Attach GPU to VM and use encoding software supporting hardware acceleration.",wrongExplanations:{1:"TPUs are specifically for machine learning tensor operations, not video transcoding. Wrong accelerator type.",2:"More vCPUs help but can't match GPU hardware encoders' efficiency. GPU transcoding is 10-100x faster than CPU.",3:"Local SSD speeds up storage I/O but doesn't accelerate actual encoding/decoding computations. Need GPU for video acceleration."}},{id:177,domain:"Configuring access and security",subdomain:"4.2 Service accounts - Automatic creation",question:"You enable Compute Engine API and notice a service account was automatically created. What is this account?",options:["Default Compute Engine service account used by VMs when no custom service account specified","Your user account","Google's service account","Random account requiring deletion"],correct:0,explanation:"Google Cloud automatically creates default service accounts when enabling certain APIs. Compute Engine default SA has Editor role. VMs use it by default. Best practice: create custom service accounts with minimal permissions instead.",wrongExplanations:{1:"Default service accounts are separate from user accounts. They're for resources, not humans.",2:"These aren't Google's operational accounts. They belong to your project for resource use.",3:"Default service accounts are intentional, not errors. Don't delete unless you've assigned custom accounts to all resources using them."}},{id:178,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.4 Monitoring - Dashboards",question:"You need to create custom dashboard showing VM CPU, memory, and application metrics together. What should you use?",options:["Cloud Monitoring dashboard with widgets displaying different metric types","BigQuery for visualization","Spreadsheet with manual data entry","Cloud Logging only"],correct:0,explanation:"Cloud Monitoring dashboards support multiple widget types: line charts, stacked area, heatmaps, gauges. Combine infrastructure and custom metrics. Share dashboards across team. Real-time auto-refresh.",wrongExplanations:{1:"BigQuery is for data analysis, not real-time operational monitoring. Dashboards provide real-time visibility.",2:"Manual spreadsheets can't show real-time data, don't auto-refresh, require constant updating. Monitoring dashboards are purpose-built solution.",3:"Cloud Logging shows logs, not metric visualizations. Monitoring provides metric dashboards and charts."}},{id:179,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Pub/Sub",question:"Your application components need asynchronous messaging with guaranteed delivery. What should you use?",options:["Pub/Sub for reliable, scalable messaging between independent components","Cloud Storage for message files","Direct HTTP calls between services","Shared database for queues"],correct:0,explanation:"Pub/Sub provides: 1) At-least-once delivery guarantee, 2) Automatic scaling, 3) Message retention (7 days), 4) Topic and subscription model, 5) Push and pull delivery, 6) Decouples publishers and subscribers. Perfect for event-driven architectures.",wrongExplanations:{1:"Cloud Storage isn't a message queue. No delivery guarantees, subscriber notification, or message ordering. Wrong tool for messaging.",2:"Direct HTTP calls couple services tightly, require handling failures/retries, and don't buffer when subscriber unavailable. Pub/Sub provides reliable async messaging.",3:"Shared database queues are DIY solution requiring complex implementation. Pub/Sub provides managed, scalable messaging without custom queue management."}},{id:180,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You need to process uploaded images automatically when they arrive in a Cloud Storage bucket. The processing takes 2-5 seconds per image and happens infrequently (a few times per hour). What is the most cost-effective solution?",options:["Deploy a Cloud Function (2nd gen) triggered by Cloud Storage events","Run a Compute Engine instance continuously polling the bucket","Deploy a GKE cluster with a CronJob checking the bucket every minute","Use Cloud Run with scheduled Cloud Scheduler checking the bucket"],correct:0,explanation:"Cloud Functions 2nd gen with Cloud Storage triggers provide event-driven processing with no infrastructure management. You only pay for execution time (seconds of processing), making it extremely cost-effective for infrequent, short-duration tasks. The function automatically scales to zero when not in use.",wrongExplanations:{1:"A continuously running Compute Engine instance wastes resources and costs money 24/7, even when no images are uploaded. This is far more expensive than serverless functions that only run on-demand.",2:"GKE is overkill for simple image processing. It requires cluster management, continuous node running costs, and is designed for complex containerized applications, not simple event-driven tasks.",3:"Cloud Scheduler with Cloud Run requires polling the bucket at regular intervals, processing whether files exist or not. Cloud Functions with storage triggers are more efficient and respond immediately to uploads."}},{id:181,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your Cloud Function needs to call multiple Google Cloud APIs and access Cloud SQL. The function execution time is unpredictable and can take up to 15 minutes. Which Cloud Functions generation should you use?",options:["Cloud Functions 2nd gen, which supports up to 60-minute timeouts and better networking","Cloud Functions 1st gen with maximum 9-minute timeout","Cloud Functions 2nd gen is not suitable; use Cloud Run instead","Split the work into multiple 1st gen functions chained together"],correct:0,explanation:"Cloud Functions 2nd gen (built on Cloud Run) supports execution timeouts up to 60 minutes, compared to 9 minutes for 1st gen. It also provides better VPC networking capabilities, making it ideal for database connections and long-running processes.",wrongExplanations:{1:"1st gen functions have a maximum timeout of 9 minutes, which is insufficient for this 15-minute workload. The function would be terminated before completing.",2:"While Cloud Run would work, Cloud Functions 2nd gen is built on Cloud Run infrastructure and provides the same capabilities with simpler deployment for function-style code. There's no need to avoid 2nd gen functions here.",3:"Chaining multiple functions adds unnecessary complexity, requires managing state between functions, and introduces potential failure points. 2nd gen functions handle this workload directly."}},{id:182,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You're deploying a Cloud Function that processes sensitive customer data. The function must not be publicly accessible and should only be invoked by authenticated services within your organization. What should you configure?",options:["Set the function to require authentication and grant roles/cloudfunctions.invoker to specific service accounts","Deploy the function with --allow-unauthenticated flag and use API keys","Put the function behind Cloud Armor for protection","Use VPC Service Controls without authentication"],correct:0,explanation:"Setting authentication requirements and granting roles/cloudfunctions.invoker only to specific service accounts follows the principle of least privilege. This ensures only authorized services can invoke the function, using Google Cloud's IAM for secure authentication.",wrongExplanations:{1:"The --allow-unauthenticated flag makes the function publicly accessible to anyone on the internet. API keys alone are not sufficient security for sensitive data and can be leaked or stolen.",2:"Cloud Armor protects against DDoS and web attacks but doesn't provide authentication or authorization. It's designed for load balancers, not for controlling function access.",3:"VPC Service Controls provide network perimeter security but don't replace authentication. You still need IAM-based authentication to control who can invoke the function."}},{id:183,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Cloud Functions",question:"Your Cloud Function is experiencing cold start latency issues, causing timeouts for the first request after periods of inactivity. Users report 5-10 second delays. What can you do to minimize cold starts?",options:["Configure minimum instances to keep at least one instance warm","Increase the function's memory allocation to speed up cold starts","Switch to 1st gen Cloud Functions which have faster cold starts","Add retry logic in the client application"],correct:0,explanation:"Setting minimum instances (available in Cloud Functions 2nd gen) keeps instances warm and ready to handle requests, eliminating cold start delays. This incurs a small cost for the idle instances but ensures consistent performance.",wrongExplanations:{1:"While more memory can slightly reduce cold start time, it doesn't eliminate cold starts entirely. You'll still experience delays when all instances have scaled to zero.",2:"1st gen Cloud Functions generally have longer cold starts than 2nd gen. Switching generations doesn't solve the cold start problem and moves you to older technology.",3:"Retry logic helps handle failures but doesn't prevent cold starts. Users would still experience the initial 5-10 second delay, followed by a retry with another potential cold start."}},{id:184,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You need to deploy a function that processes Pub/Sub messages containing order data. The function must handle up to 1000 messages per second during peak hours. What should you configure?",options:["Deploy Cloud Function 2nd gen with Pub/Sub trigger and configure max instances to handle peak load","Deploy Cloud Function 1st gen with default settings","Deploy multiple separate functions to distribute the load","Use Compute Engine with a Pub/Sub pull subscription instead"],correct:0,explanation:"Cloud Functions 2nd gen with Pub/Sub triggers automatically scale to handle message volume. Configuring max instances prevents runaway costs while ensuring capacity for 1000 messages/second. Each function instance can handle concurrent requests, efficiently processing the message stream.",wrongExplanations:{1:"1st gen functions have more limited concurrency and scaling capabilities compared to 2nd gen. For high-throughput scenarios like 1000 messages/second, 2nd gen provides better performance and concurrency handling.",2:"Deploying multiple functions adds unnecessary complexity and requires custom load distribution logic. Cloud Functions automatically scale horizontally to handle load without manual distribution.",3:"Compute Engine requires managing infrastructure, autoscaling configuration, and deployment complexity. Cloud Functions provide automatic scaling and simpler management for event-driven workloads."}},{id:185,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Cloud Functions",question:"Your Cloud Function needs to write data to BigQuery and read secrets from Secret Manager. Following least privilege, what should you do?",options:["Create a dedicated service account with roles/bigquery.dataEditor and roles/secretmanager.secretAccessor, assign it to the function","Use the default App Engine service account which has broad permissions","Grant roles/owner to the function's service account","Store BigQuery and Secret Manager credentials in environment variables"],correct:0,explanation:"Creating a dedicated service account with only the specific roles needed (BigQuery data editor and Secret Manager accessor) follows the principle of least privilege. This limits the blast radius if the function is compromised.",wrongExplanations:{1:"The default App Engine service account has roles/editor at the project level, which is far too broad. This violates least privilege and could allow the function to modify resources it shouldn't access.",2:"roles/owner grants full control over the project, including ability to delete resources, modify IAM policies, and access all data. This is the opposite of least privilege and represents a significant security risk.",3:"Environment variables are not secure for storing credentials. Secret Manager is specifically designed for this purpose. Also, you can't store service account credentials for BigQuery in environment variables - you need to use IAM service account authentication."}},{id:186,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You need to deploy a Cloud Function that connects to a Cloud SQL instance in a private VPC. The function should not be accessible from the internet. What configuration is required?",options:["Deploy Cloud Functions 2nd gen with VPC connector, configure Cloud SQL private IP, and disable public access to the function","Deploy Cloud Functions 1st gen with Cloud SQL proxy","Use public IP for Cloud SQL and restrict by IP allowlist","Deploy the function in GKE instead to access the VPC"],correct:0,explanation:"Cloud Functions 2nd gen supports VPC connectors for private networking, allowing direct connection to Cloud SQL private IP addresses. Disabling public access to the function ensures it can only be invoked by authenticated callers, not from the internet.",wrongExplanations:{1:"While Cloud SQL proxy works with 1st gen functions, 2nd gen functions with VPC connectors provide better VPC integration and networking capabilities, including support for private IPs without requiring the proxy.",2:"Using public IP for Cloud SQL exposes the database to the internet, requiring management of IP allowlists. This is less secure than private IP communication within a VPC.",3:"GKE is unnecessary overhead for a simple function. Cloud Functions 2nd gen with VPC connector provides the same private networking capabilities without the complexity of managing a Kubernetes cluster."}},{id:187,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Monitoring and logging - Cloud Functions",question:"Your Cloud Function is failing intermittently with timeout errors. You need to investigate the root cause. Where should you look first?",options:["Check Cloud Logging for function logs and Cloud Trace for request latency breakdown","Increase the function timeout and memory allocation","Restart the function deployment","Check the function's source code for syntax errors"],correct:0,explanation:"Cloud Logging captures function execution logs, including errors, warnings, and custom log messages. Cloud Trace shows detailed latency breakdown of function execution and downstream API calls, helping identify bottlenecks causing timeouts.",wrongExplanations:{1:"Increasing timeout and memory without understanding the root cause is treating symptoms, not the problem. The function might have an infinite loop, slow API calls, or inefficient code that needs fixing first.",2:"Restarting a function deployment doesn't fix intermittent timeout issues, which are typically caused by performance problems in the code or downstream dependencies, not deployment state.",3:"Syntax errors would cause the function to fail to deploy or fail immediately on every invocation, not intermittently. Timeouts suggest a performance or resource issue, not syntax problems."}},{id:188,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You have a Cloud Function triggered by HTTP requests that performs data validation. During load testing, you notice some requests are taking 30 seconds while others complete in 2 seconds. What is the most likely cause?",options:["Cold starts when new instances are created to handle increased load","Network latency to Google Cloud","The function code has a performance bug","Cloud Functions throttling your requests"],correct:0,explanation:"Cold starts occur when Cloud Functions creates new instances to handle load. The first request to a new instance experiences initialization latency (loading runtime, dependencies, code). Subsequent requests on warm instances are much faster. This explains the 30s vs 2s variance.",wrongExplanations:{1:"Network latency to Google Cloud is typically measured in milliseconds, not tens of seconds. Network issues wouldn't cause such dramatic variance between requests.",2:"While a performance bug could cause slow execution, it would affect most or all requests consistently. The dramatic difference (30s vs 2s) and occurrence during load testing specifically points to cold starts as instances scale up.",3:"Cloud Functions don't throttle by adding delay to requests. Throttling would result in rejected requests (429 errors) or rate limiting, not variable latency within successful requests."}},{id:189,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your team is debating between Cloud Functions and Cloud Run for a new microservice. The service processes webhook events, has predictable traffic patterns, and requires specific runtime dependencies. When would Cloud Functions be the better choice?",options:["When the code follows simple function patterns and you want minimal configuration and deployment simplicity","When you need custom Dockerfile for complex runtime dependencies","When you need fine-grained control over container configuration","Cloud Run is always better than Cloud Functions"],correct:0,explanation:"Cloud Functions excel for simple, function-based code with standard runtime dependencies (Node.js, Python, Go, Java, etc.). They offer the simplest deployment model with minimal configuration. For webhook processing with standard libraries, Functions provide easier development and deployment.",wrongExplanations:{1:"If you need custom Dockerfiles for complex dependencies, Cloud Run is the better choice. Cloud Functions support standard runtimes with requirements.txt or package.json for dependencies, but not custom container images.",2:"Cloud Run provides fine-grained control over container configuration, including CPU, memory, concurrency, and custom containers. Cloud Functions offer simpler deployment but less configuration control. This scenario favors Run, not Functions.",3:"This is false. Both services have valid use cases. Cloud Functions are better for simple event-driven functions, while Cloud Run is better for containerized applications, complex dependencies, or when you need more control."}},{id:190,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"You need to deploy a Cloud Function that runs on a schedule to generate daily reports. The function takes 5 minutes to execute and runs every day at 2 AM. What is the correct approach?",options:["Create a Cloud Scheduler job with Pub/Sub topic, configure Cloud Function with Pub/Sub trigger","Use a Compute Engine instance with crontab","Configure Cloud Function with HTTP trigger and call it manually each day","Use GKE CronJob instead"],correct:0,explanation:"Cloud Scheduler publishes messages to Pub/Sub on a cron schedule. The Cloud Function with Pub/Sub trigger automatically executes when messages arrive. This is the serverless, managed approach for scheduled function execution with no infrastructure to manage.",wrongExplanations:{1:"Compute Engine with crontab requires running and managing a VM 24/7 just to execute a 5-minute daily task. This is wasteful and expensive compared to serverless Cloud Functions that only consume resources during execution.",2:"Manual execution defeats the purpose of automation and is error-prone. Cloud Scheduler provides reliable, automated scheduling without manual intervention.",3:"GKE CronJob requires managing a Kubernetes cluster for a simple scheduled task. This is significant overhead compared to Cloud Functions + Cloud Scheduler, which require no infrastructure management."}},{id:191,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - Cloud Functions",question:"After deploying a new version of your Cloud Function, users report errors. You need to quickly roll back to the previous working version. What should you do?",options:["Use gcloud functions deploy with the previous version's code or redeploy from source control","Delete the function and create it again from scratch","Wait for automatic rollback to occur","Use gcloud functions rollback command"],correct:0,explanation:"Cloud Functions don't have built-in version management or automatic rollback. To roll back, you redeploy the previous version using 'gcloud functions deploy' with the old source code. Best practice is to maintain versioned source code in git for easy rollback.",wrongExplanations:{1:"Deleting and recreating the function causes downtime and loses configuration like environment variables, IAM bindings, and triggers. Redeploying the previous version maintains all configuration while only changing the code.",2:"Cloud Functions don't have automatic rollback capabilities. You must manually deploy the previous version. This is why version control and CI/CD practices are important for functions.",3:"There is no 'gcloud functions rollback' command. Cloud Functions don't maintain version history automatically. You must redeploy using the standard deploy command with previous source code."}},{id:192,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your Cloud Function needs to process large files (up to 500MB) uploaded to Cloud Storage. The processing involves reading the entire file into memory. What should you consider?",options:["Configure sufficient memory for the function (up to 32GB in 2nd gen) and consider streaming processing to reduce memory usage","Cloud Functions cannot handle files over 100MB","Split files before uploading using client-side logic","Use 1st gen functions which have better file handling"],correct:0,explanation:"Cloud Functions 2nd gen supports up to 32GB of memory, sufficient for 500MB files. However, best practice is to use streaming processing where possible to reduce memory footprint, improve performance, and reduce costs. Consider processing files in chunks rather than loading entirely into memory.",wrongExplanations:{1:"This is incorrect. Cloud Functions 2nd gen can handle files much larger than 100MB with appropriate memory configuration. The limit is based on available memory (up to 32GB), not file size.",2:"Splitting files adds complexity to both upload and processing logic. It's better to use the function's memory configuration and streaming processing. File splitting should be a last resort, not the first solution.",3:"1st gen functions have lower memory limits (up to 8GB) compared to 2nd gen (up to 32GB). For large file processing, 2nd gen provides better capabilities, not 1st gen."}},{id:193,domain:"Configuring access and security",subdomain:"4.2 Security best practices - Cloud Functions",question:"You need to provide a third-party service temporary access to invoke your Cloud Function for testing purposes. Following security best practices, what should you do?",options:["Create a service account with roles/cloudfunctions.invoker, generate a short-lived token, and revoke access after testing","Share your personal Google account credentials","Make the function public with --allow-unauthenticated","Create an API key and share it with the third party"],correct:0,explanation:"Creating a dedicated service account with only the necessary permission (cloudfunctions.invoker) follows least privilege. Using short-lived tokens and revoking access after testing limits the window of potential unauthorized access. Service accounts can be easily disabled or deleted.",wrongExplanations:{1:"Sharing personal credentials violates security best practices and may breach your organization's policies. Personal accounts often have broad permissions beyond just the function, creating security risks.",2:"Making the function public removes all access control, exposing it to anyone on the internet. This creates security vulnerabilities even if only temporarily, and you might forget to re-enable authentication.",3:"API keys alone don't provide sufficient security for Cloud Functions. They can be leaked, don't expire automatically, and don't integrate with IAM. Service accounts with short-lived tokens are more secure."}},{id:194,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your company processes payment transactions using Cloud Functions. The function must guarantee exactly-once processing of each transaction. How can you achieve this?",options:["Implement idempotency in the function code by tracking processed transaction IDs in Firestore or Cloud SQL","Cloud Functions automatically guarantee exactly-once processing","Use Pub/Sub with exactly-once delivery setting","Deploy multiple function instances for redundancy"],correct:0,explanation:"Exactly-once processing must be implemented at the application level through idempotency. Track processed transaction IDs in a database (Firestore, Cloud SQL, etc.) and check before processing. Cloud Functions themselves provide at-least-once guarantees, so your code must handle potential duplicates.",wrongExplanations:{1:"Cloud Functions provide at-least-once guarantees, not exactly-once. In failure scenarios, a function may be retried, causing duplicate invocations. Your application code must implement idempotency to handle this.",2:"Pub/Sub delivers messages at-least-once, not exactly-once. Even with Pub/Sub, your function code must implement idempotency checks to ensure transactions are processed only once.",3:"Multiple instances increase availability but don't guarantee exactly-once processing. In fact, they increase the chance of duplicate processing if requests are retried. Idempotency logic is still required."}},{id:195,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Monitoring and logging - Cloud Functions",question:"You want to monitor the error rate and execution time of your Cloud Functions to detect performance degradation. What should you set up?",options:["Create Cloud Monitoring dashboards with function execution metrics and set up alerting policies for error rate and latency thresholds","Read Cloud Logging logs manually every day","Use third-party monitoring tools only","Cloud Functions don't provide performance metrics"],correct:0,explanation:"Cloud Monitoring automatically collects Cloud Functions metrics including execution count, error count, execution time, and memory usage. Creating dashboards visualizes these metrics, and alerting policies proactively notify you when thresholds are exceeded, enabling quick response to issues.",wrongExplanations:{1:"Manual log review is time-consuming, error-prone, and reactive. You won't detect issues until you happen to check the logs. Cloud Monitoring provides automated, proactive alerting.",2:"While third-party tools can augment monitoring, Cloud Monitoring provides native integration with Cloud Functions metrics at no additional cost. It's best to use the built-in monitoring capabilities first.",3:"This is false. Cloud Functions automatically export detailed metrics to Cloud Monitoring, including execution times, error rates, instance counts, and resource usage."}},{id:196,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - Cloud Functions",question:"Your Cloud Function needs to fan out work to multiple downstream services based on event data. For example, when an order is placed, you need to notify inventory, billing, and shipping services. What pattern should you use?",options:["Have the function publish to multiple Pub/Sub topics, with each downstream service subscribing to its relevant topic","Make synchronous HTTP calls to all services from within the function","Store events in a database and have services poll for new events","Deploy separate functions for each downstream service"],correct:0,explanation:"Publishing to multiple Pub/Sub topics decouples services and provides reliable, asynchronous delivery. Each service independently subscribes to relevant topics and processes messages at its own pace. This follows event-driven architecture best practices and allows services to scale independently.",wrongExplanations:{1:"Synchronous HTTP calls create tight coupling between services and increase function execution time. If any downstream service is slow or unavailable, the function is delayed or fails. This doesn't scale well and reduces reliability.",2:"Database polling is inefficient, adds latency (waiting for next poll cycle), and creates unnecessary database load. Pub/Sub provides immediate event delivery without polling overhead.",3:"Deploying separate functions for each downstream service creates unnecessary duplication of the fan-out logic. One function publishing to multiple topics is simpler and more maintainable."}},{id:197,domain:"Configuring access and security",subdomain:"4.3 Managing encryption - Cloud Functions",question:"Your Cloud Function processes sensitive healthcare data and must encrypt environment variables containing API keys. What is the recommended approach?",options:["Store secrets in Secret Manager and access them from the function code using the Secret Manager API","Encrypt environment variables manually before deployment","Store secrets in function source code","Use Cloud KMS to encrypt environment variables"],correct:0,explanation:"Secret Manager is designed specifically for storing and managing sensitive data like API keys. It provides encryption at rest, access logging, versioning, and fine-grained IAM control. Functions can retrieve secrets at runtime using the Secret Manager API with appropriate IAM permissions.",wrongExplanations:{1:"While you could manually encrypt environment variables, managing the encryption keys and decryption logic adds complexity. Secret Manager handles encryption, rotation, and access control automatically.",2:"Storing secrets in source code is a critical security violation. Source code is often committed to version control, shared among developers, and may be exposed in logs or error messages. Never store secrets in code.",3:"While Cloud KMS can encrypt data, Secret Manager is the preferred solution for secrets as it's purpose-built for this use case. It provides additional features like versioning, audit logging, and easier access patterns specifically designed for secrets."}},{id:198,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"You're migrating a Python web application to Google Cloud. The application requires automatic scaling, zero server management, and support for background workers. Traffic is variable with occasional spikes. Which deployment option is most suitable?",options:["App Engine Standard Environment with automatic scaling and task queues for background jobs","App Engine Flexible Environment with manual scaling","Compute Engine with managed instance groups","GKE with manual scaling configuration"],correct:0,explanation:"App Engine Standard provides automatic scaling with zero instance management, scales to zero when idle (cost-effective), and integrates with Cloud Tasks for background workers. It's ideal for web applications with variable traffic patterns and supports Python natively.",wrongExplanations:{1:"Flexible Environment doesn't scale to zero and requires at least one instance running always, increasing costs. Manual scaling defeats the purpose of serverless benefits for variable traffic.",2:"Compute Engine requires managing VMs, load balancers, autoscaling configuration, and OS updates. This adds operational overhead compared to App Engine's fully managed approach.",3:"GKE requires managing a Kubernetes cluster, which is unnecessary complexity for a standard web application. App Engine provides simpler deployment and management for typical web apps."}},{id:199,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine application needs to install custom system libraries and run containers that require root access during initialization. Which App Engine environment should you use?",options:["App Engine Flexible Environment, which supports custom Dockerfiles and root access","App Engine Standard Environment with buildpacks","App Engine Standard Environment is always the better choice","Neither; use Cloud Run instead"],correct:0,explanation:"App Engine Flexible Environment runs your application in Docker containers on Compute Engine VMs, allowing custom Dockerfiles, system library installation, and root access during initialization. Standard Environment runs in a sandbox with limited system access.",wrongExplanations:{1:"App Engine Standard Environment runs in a secure sandbox that doesn't allow custom system libraries or root access. It's designed for applications using standard runtime dependencies.",2:"This is incorrect. Both environments have valid use cases. Standard is better for simpler apps with standard dependencies, while Flexible is better for apps requiring custom system libraries or container configuration.",3:"While Cloud Run supports custom containers, if you're already using App Engine features like traffic splitting, versions, and integrated services, Flexible Environment provides those benefits with custom container support."}},{id:200,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - App Engine",question:"You deployed a new version of your App Engine application and users report errors. You need to quickly route all traffic back to the previous working version. What should you do?",options:["Use traffic splitting to route 100% of traffic to the previous version","Delete the new version","Redeploy the previous version code","App Engine automatically rolls back on errors"],correct:0,explanation:"App Engine maintains multiple versions simultaneously. Traffic splitting allows instant migration between versions without redeployment. You can route 100% of traffic back to the working version immediately, then debug the new version before migrating traffic again.",wrongExplanations:{1:"Deleting the version doesn't immediately fix the issue for users currently being served. You need to actively route traffic to the working version first. Deletion should happen after confirming the rollback works.",2:"Redeploying creates a new version and takes time. Traffic splitting provides instant rollback by leveraging existing deployed versions. This is faster and safer.",3:"App Engine does not automatically roll back on errors. You must manually manage traffic between versions. This gives you control over when and how to roll back."}},{id:201,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine Standard application needs to process uploaded files that take 15 minutes per file. Standard Environment has a 10-minute request timeout. What is the best solution?",options:["Use Cloud Tasks to queue the file processing job and have a separate service process it asynchronously","Switch to App Engine Flexible which has 60-minute timeout","Increase the timeout limit in App Engine Standard","Process files in the upload request handler"],correct:0,explanation:"Cloud Tasks provides asynchronous job queuing, allowing the upload request to complete quickly while processing happens separately. The task handler can run for up to 10 minutes, but you can chain tasks or use Cloud Functions 2nd gen for longer processing. This decouples upload from processing.",wrongExplanations:{1:"While Flexible Environment does support longer timeouts, it's more expensive (doesn't scale to zero) and overkill for this use case. Asynchronous processing is a better architectural pattern.",2:"App Engine Standard timeout limits cannot be increased beyond 10 minutes. This is a fundamental constraint of the Standard Environment.",3:"Processing long-running jobs in the request handler creates poor user experience (15-minute wait for upload), ties up instances, and will hit timeout limits. Asynchronous processing is the correct pattern."}},{id:202,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"You need to test a new version of your App Engine application with 10% of production traffic before full rollout. How can you achieve this?",options:["Deploy the new version without migrating traffic, then use traffic splitting to allocate 10% to the new version","Deploy to a separate project for testing","Use Compute Engine for canary testing instead","App Engine doesn't support partial traffic routing"],correct:0,explanation:"App Engine traffic splitting allows you to distribute traffic between versions by percentage, IP address, or cookie. Deploying with --no-promote keeps the new version available without receiving traffic, then you can gradually split traffic (10%, 50%, 100%) for canary testing.",wrongExplanations:{1:"Deploying to a separate project doesn't test with real production traffic and requires duplicate infrastructure. Traffic splitting within the same service provides true canary testing with production users.",2:"Compute Engine requires setting up load balancers and traffic management manually. App Engine provides built-in traffic splitting without infrastructure management.",3:"This is false. Traffic splitting is a core App Engine feature specifically designed for gradual rollouts, A/B testing, and canary deployments."}},{id:203,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - App Engine",question:"Your App Engine application needs to access Cloud SQL. Following security best practices, how should you configure authentication?",options:["Use the App Engine default service account with appropriate Cloud SQL IAM roles","Store Cloud SQL password in environment variables","Use Cloud SQL proxy with hardcoded credentials","Grant roles/owner to the App Engine service account"],correct:0,explanation:"App Engine applications automatically use the App Engine default service account for authentication. Grant this service account the Cloud SQL Client role (roles/cloudsql.client) for secure, password-free connection to Cloud SQL instances.",wrongExplanations:{1:"Storing passwords in environment variables exposes them in configuration and logs. IAM-based authentication with service accounts is more secure and doesn't require managing passwords.",2:"Hardcoded credentials in code or configuration are a critical security vulnerability. They can be exposed in version control, logs, or error messages. Always use IAM-based authentication.",3:"roles/owner grants far too many permissions beyond Cloud SQL access. This violates least privilege principle and could allow the application to modify or delete any project resources."}},{id:204,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - App Engine",question:"Your App Engine application experiences high latency during cold starts. Users report slow initial page loads. What can you do to minimize this issue?",options:["Configure minimum instances to keep instances warm and ready to serve requests","Switch to manual scaling","Increase the instance class size","Cold starts are unavoidable in App Engine"],correct:0,explanation:"Minimum instances (min_instances) keeps the specified number of instances running and warm, eliminating cold start latency for those instances. This incurs a cost for idle instances but ensures consistent performance. This is available in App Engine Standard.",wrongExplanations:{1:"Manual scaling keeps instances running but defeats autoscaling benefits. Automatic scaling with minimum instances provides the best of both worlds: warmth and scalability.",2:"Instance class affects performance but doesn't eliminate cold starts. Cold starts are caused by instance initialization, not resource constraints. Minimum instances solve this by keeping instances warm.",3:"While cold starts exist, they can be minimized through minimum instances, optimizing startup code, reducing dependencies, and using warmup requests. Multiple strategies exist to address cold starts."}},{id:205,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"You have microservices architecture where different components need independent scaling and deployment. Each service has different resource requirements. How should you structure your App Engine deployment?",options:["Deploy each microservice as a separate App Engine service, allowing independent scaling and deployment","Deploy all microservices in a single service with different versions","Use separate GCP projects for each microservice","App Engine doesn't support microservices"],correct:0,explanation:"App Engine services (formerly called modules) allow you to structure your application as microservices. Each service can have its own scaling configuration, instance class, runtime, and versions. Services are independently deployable and scalable while sharing the same project.",wrongExplanations:{1:"Versions within a service share scaling configuration and are meant for different versions of the same code, not different microservices. This doesn't provide the independent scaling needed for microservices.",2:"Separate projects add complexity in networking, IAM, billing, and service communication. App Engine services provide microservices benefits within a single project.",3:"This is false. App Engine services are specifically designed for microservices architecture, allowing multiple independently deployable and scalable services within one application."}},{id:206,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine Standard application needs to connect to a Cloud Memorystore Redis instance for caching. Redis is only accessible via private IP. What configuration is required?",options:["Configure Serverless VPC Access connector for your App Engine service to connect to the VPC","App Engine Standard cannot connect to private IPs","Use public IP for Redis instance","Switch to App Engine Flexible Environment"],correct:0,explanation:"Serverless VPC Access connector enables App Engine Standard (and other serverless services) to connect to resources in your VPC via private IP addresses, including Cloud Memorystore, Compute Engine VMs, and other internal resources.",wrongExplanations:{1:"This is incorrect. App Engine Standard can connect to private IPs using Serverless VPC Access connectors. This is a supported configuration for accessing VPC resources.",2:"Memorystore doesn't support public IPs for security reasons. It's designed for private VPC access only. Serverless VPC Access connector is the correct solution.",3:"While Flexible Environment has VPC networking capabilities, Standard Environment with Serverless VPC Access connector is more cost-effective (scales to zero) and simpler for applications that otherwise fit Standard's model."}},{id:207,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Monitoring and logging - App Engine",question:"Your App Engine application is experiencing performance issues. You need to identify which handler functions are slowest and causing timeouts. What should you use?",options:["Cloud Trace to analyze request latency and identify slow operations in your application","Cloud Logging to read logs manually","Cloud Monitoring for CPU metrics only","App Engine doesn't provide performance profiling"],correct:0,explanation:"Cloud Trace automatically collects latency data from App Engine applications, showing detailed breakdowns of request processing time, including time spent in different functions, API calls, and database queries. This pinpoints performance bottlenecks.",wrongExplanations:{1:"While logs contain some timing information, manually reading logs is inefficient for performance analysis. Cloud Trace provides automated latency analysis and visualization specifically designed for identifying bottlenecks.",2:"CPU metrics show resource utilization but don't identify which specific code paths or operations are slow. Cloud Trace provides request-level detail showing where time is spent.",3:"This is false. App Engine integrates with Cloud Trace and Cloud Profiler for detailed performance analysis, latency tracking, and code-level profiling."}},{id:208,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine application uses environment variables for configuration. You need to update a configuration value without redeploying the application. Is this possible?",options:["No, environment variables are set at deployment time and require redeployment to change","Yes, use gcloud app update-env command","Yes, update them in the Cloud Console","Yes, environment variables auto-sync from Secret Manager"],correct:0,explanation:"App Engine environment variables are part of the version configuration and are baked in at deployment time. To change them, you must redeploy. For runtime-updatable configuration, use Secret Manager, Cloud Storage, or Firestore to store configuration that your app reads at runtime.",wrongExplanations:{1:"This command doesn't exist. App Engine environment variables cannot be updated without redeployment. Consider using external configuration sources for values that change frequently.",2:"Environment variables cannot be updated through the Console without creating a new version. They're part of the immutable version configuration.",3:"Environment variables don't auto-sync with Secret Manager. While Secret Manager is recommended for sensitive configuration, your application code must explicitly fetch secrets at runtime using the API."}},{id:209,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"You need to deploy a Node.js application that requires a specific Node.js version not available in the current App Engine Standard runtimes. What should you do?",options:["Use App Engine Flexible Environment with a custom runtime specified in Dockerfile","Downgrade your application to use an available Node.js version","App Engine Standard supports all Node.js versions","Deploy to Compute Engine instead"],correct:0,explanation:"App Engine Flexible Environment allows custom runtimes using Dockerfiles, enabling you to specify any Node.js version. Standard Environment only supports specific runtime versions. Flexible provides flexibility for custom runtime requirements while maintaining App Engine benefits.",wrongExplanations:{1:"Downgrading may not be feasible if your application depends on features or security patches in newer Node.js versions. Using Flexible Environment maintains your version requirements.",2:"This is false. App Engine Standard provides specific, managed runtime versions. While Google regularly updates them, not all versions are available. Flexible Environment supports custom versions.",3:"Compute Engine requires managing VMs, load balancing, autoscaling, and deployments manually. App Engine Flexible provides these features while supporting custom runtimes."}},{id:210,domain:"Planning and implementing a cloud solution",subdomain:"2.1 Planning compute - App Engine",question:"Your App Engine application serves static assets (images, CSS, JavaScript). These assets don't change frequently but are requested on every page load. How can you optimize delivery and reduce costs?",options:["Configure app.yaml to serve static files directly and use Cloud CDN for caching","Serve all files through application handlers","Store static files in Cloud Storage and link directly","Use Cloud Functions to serve static files"],correct:0,explanation:"App Engine's app.yaml allows defining static file handlers that serve files directly without invoking application code, reducing latency and costs. Enabling Cloud CDN caches these assets at edge locations globally, further improving performance and reducing origin requests.",wrongExplanations:{1:"Serving static files through application handlers wastes instance resources and increases latency. Static handlers bypass the runtime, serving files directly more efficiently.",2:"While Cloud Storage is good for static files, App Engine's static handlers with CDN provide integrated deployment and caching. For simple static assets bundled with your app, static handlers are simpler.",3:"Cloud Functions for static file serving adds unnecessary complexity and cost. App Engine's static handlers are specifically designed for this use case and are more efficient."}},{id:211,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - App Engine",question:"You need to restrict access to your App Engine application so only users in your organization can access it. What should you configure?",options:["Enable Identity-Aware Proxy (IAP) and configure access policies to allow only your organization's users","Add firewall rules to block external IPs","Configure Cloud Armor security policies","Use VPC Service Controls"],correct:0,explanation:"Identity-Aware Proxy (IAP) provides application-level access control based on user identity and context. You can restrict access to users in your organization using Google Workspace or Cloud Identity, without requiring VPN or complex network configuration.",wrongExplanations:{1:"App Engine Standard doesn't support VPC firewall rules in the traditional sense. It's a serverless platform where network security is managed differently. IAP provides the application-level access control needed.",2:"Cloud Armor protects against DDoS and web attacks but doesn't provide user authentication or organization-based access control. IAP provides identity-based access control.",3:"VPC Service Controls create security perimeters for GCP resources but are designed for preventing data exfiltration, not user authentication. IAP provides the user-level access control needed here."}},{id:212,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing compute - App Engine",question:"Your App Engine application has automatic scaling enabled. You notice instances are being created and destroyed frequently during traffic fluctuations, causing cold starts. How can you optimize this behavior?",options:["Configure target CPU utilization and max concurrent requests to tune scaling behavior, and set min_idle_instances","Switch to basic scaling instead","Disable autoscaling entirely","Increase max_instances limit"],correct:0,explanation:"Automatic scaling in App Engine can be tuned using target_cpu_utilization, target_throughput_utilization, max_concurrent_requests, and min_idle_instances. These settings control when new instances are created and how many idle instances are kept warm, balancing cost and performance.",wrongExplanations:{1:"Basic scaling doesn't provide automatic scaling based on load. It creates instances on-demand and shuts them down after idle time. This doesn't solve the cold start problem for traffic fluctuations.",2:"Disabling autoscaling defeats the purpose of serverless benefits and cost optimization. The goal is to tune autoscaling, not disable it.",3:"Increasing max_instances doesn't prevent frequent creation/destruction of instances based on traffic. It just sets an upper limit. Tuning scaling thresholds and keeping minimum idle instances addresses the cold start issue."}},{id:213,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"You need to process streaming data from Pub/Sub, perform windowed aggregations, and write results to BigQuery in real-time. The processing logic requires complex transformations. What should you use?",options:["Cloud Dataflow with Apache Beam for streaming ETL pipeline","BigQuery scheduled queries","Cloud Functions triggered by Pub/Sub","Dataproc with Spark Streaming"],correct:0,explanation:"Cloud Dataflow is Google's fully managed service for stream and batch processing using Apache Beam. It excels at streaming data processing with windowing, complex transformations, and has native Pub/Sub and BigQuery connectors. It autoscales and requires no cluster management.",wrongExplanations:{1:"BigQuery scheduled queries work on data already in BigQuery, not streaming data from Pub/Sub. They can't perform real-time processing of incoming events.",2:"Cloud Functions are designed for simple event processing, not complex streaming analytics with windowing and aggregations. They lack the streaming primitives (windows, triggers, watermarks) needed for sophisticated stream processing.",3:"While Dataproc with Spark Streaming can handle this, it requires managing clusters. Dataflow is serverless, autoscaling, and purpose-built for this use case with better BigQuery integration."}},{id:214,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"Your team has existing Spark and Hadoop jobs that need to run in Google Cloud. The jobs run periodically (hourly) and complete within 20-30 minutes. What is the most cost-effective approach?",options:["Use Dataproc with ephemeral clusters created per job, then deleted after completion","Run a persistent Dataproc cluster 24/7","Rewrite all jobs to use Dataflow","Use Compute Engine with manual Hadoop installation"],correct:0,explanation:"Dataproc supports ephemeral clusters that spin up for a job and delete automatically afterward. For periodic workloads, this is highly cost-effective as you only pay for cluster time during job execution. Dataproc clusters start in 90 seconds, making this pattern practical.",wrongExplanations:{1:"A persistent cluster running 24/7 for jobs that only run hourly for 30 minutes wastes resources. You'd pay for 23.5 hours of idle time per day. Ephemeral clusters eliminate this waste.",2:"Rewriting existing Spark/Hadoop jobs to Dataflow requires significant development effort and may not be necessary. Dataproc provides a lift-and-shift path for existing Spark/Hadoop workloads.",3:"Manual Hadoop installation on Compute Engine requires managing cluster configuration, software updates, and scaling manually. Dataproc provides a fully managed Hadoop/Spark environment."}},{id:215,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"Your Dataflow pipeline processes batch data from Cloud Storage. You need to minimize costs while accepting longer processing times. What should you configure?",options:["Use Dataflow Shuffle service and enable FlexRS (Flexible Resource Scheduling) for batch jobs","Use the smallest machine type available","Reduce the number of workers to 1","Process data manually with Cloud Functions"],correct:0,explanation:"FlexRS (Flexible Resource Scheduling) provides advanced scheduling for batch jobs at reduced cost (up to 40% savings) by using a mix of normal and preemptible resources with intelligent retries. Dataflow Shuffle service (included with FlexRS) improves performance and reduces costs for large batch jobs.",wrongExplanations:{1:"Using the smallest machine type doesn't optimize costs effectively. It may actually increase costs if jobs run much longer. FlexRS provides better cost optimization through resource scheduling.",2:"Reducing workers to 1 severely limits parallelism, making jobs extremely slow. Dataflow's autoscaling with FlexRS provides better cost/performance balance.",3:"Cloud Functions are unsuitable for large-scale batch data processing. They have memory and execution time limits. Dataflow is purpose-built for large batch processing."}},{id:216,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You're running Apache Spark jobs on Dataproc that process data stored in Cloud Storage. The jobs frequently fail due to worker node failures. How can you improve reliability?",options:["Enable Enhanced Flexibility Mode (EFM) which uses preemptible VMs with automatic recovery","Disable preemptible workers entirely","Increase the number of worker nodes","Switch to Dataflow instead"],correct:0,explanation:"Enhanced Flexibility Mode (EFM) in Dataproc allows using a mix of preemptible and regular VMs with intelligent workload placement and automatic recovery. It provides cost savings from preemptible VMs while maintaining reliability through smart failure handling.",wrongExplanations:{1:"Disabling preemptible workers increases costs significantly. EFM allows safe use of preemptible workers with reliability through automatic recovery mechanisms.",2:"Simply increasing nodes doesn't address the root cause of node failures. EFM provides failure recovery mechanisms that make the cluster more resilient.",3:"While Dataflow is more resilient for some workloads, if you have existing Spark jobs, EFM provides a cost-effective way to improve Dataproc reliability without rewriting code."}},{id:217,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataprep",question:"Your data analysts need to clean and prepare messy CSV files for analysis in BigQuery, but they don't have programming skills. They need a visual interface to explore data and create transformation recipes. What should you use?",options:["Cloud Dataprep by Trifacta for visual data preparation and cleaning","Write Python scripts in Cloud Functions","Use BigQuery SQL for data cleaning","Use Cloud Dataflow with Apache Beam"],correct:0,explanation:"Cloud Dataprep provides a visual, no-code interface for data cleaning and preparation. Analysts can interactively explore data, detect anomalies, and build transformation recipes that are executed as Dataflow jobs. It's designed specifically for non-technical users.",wrongExplanations:{1:"Python scripts require programming skills, which the analysts lack. Dataprep provides a visual interface specifically designed for non-technical users.",2:"While BigQuery SQL can clean data, it's less interactive and doesn't provide the data profiling, anomaly detection, and recipe-building features that Dataprep offers for exploratory data cleaning.",3:"Cloud Dataflow requires programming with Apache Beam (Python or Java). It's powerful but not suitable for analysts without programming skills. Dataprep generates Dataflow jobs automatically."}},{id:218,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing data processing - Dataflow",question:"Your Dataflow streaming pipeline is experiencing high latency. The pipeline reads from Pub/Sub, processes data, and writes to BigQuery. How can you identify the bottleneck?",options:["Check the Dataflow monitoring UI for system lag, data freshness metrics, and per-step processing times","Increase worker count immediately","Restart the pipeline","Dataflow doesn't provide performance metrics"],correct:0,explanation:"Dataflow's monitoring UI provides detailed metrics including system lag (backlog of unprocessed data), data freshness (age of oldest unprocessed data), and per-step execution time. These metrics pinpoint whether bottlenecks are in reading, processing, or writing stages.",wrongExplanations:{1:"Blindly increasing workers without identifying the bottleneck wastes resources. The bottleneck might be in a non-parallelizable step or external dependency, where more workers won't help.",2:"Restarting doesn't fix performance issues. It might even make things worse if there's backlog, as the pipeline needs to reprocess buffered data.",3:"This is false. Dataflow provides comprehensive monitoring including system lag, data freshness, throughput, worker utilization, and per-step metrics."}},{id:219,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You need to run a Dataproc cluster with HDFS for temporary storage during Spark jobs, but want to minimize costs. What storage strategy should you use?",options:["Use Cloud Storage as primary storage and local HDFS only for shuffle/temp data with smaller persistent disk","Use large persistent disks for HDFS storage","Use only in-memory storage without any disk","Use Cloud Filestore for HDFS replacement"],correct:0,explanation:"Cloud Storage provides durable, cost-effective storage for data with excellent integration with Dataproc (HDFS-compatible connector). Use HDFS on smaller local disks only for temporary shuffle data during jobs. This approach minimizes storage costs while maintaining performance.",wrongExplanations:{1:"Large persistent disks for HDFS are expensive and unnecessary when Cloud Storage provides durable, cheaper storage. Persistent disks should be sized for temp/shuffle data only.",2:"In-memory storage is limited by VM memory and ephemeral. Spark jobs need persistent storage for data and temp files. This approach won't work for most real workloads.",3:"Cloud Filestore is expensive network-attached storage meant for applications needing shared NFS. Cloud Storage with HDFS connector is more cost-effective for Hadoop/Spark workloads."}},{id:220,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"Your Dataflow pipeline needs to join streaming data from Pub/Sub with reference data in BigQuery. The reference data changes infrequently (once daily). What pattern should you use?",options:["Use Dataflow side input to periodically reload reference data from BigQuery and join in memory","Query BigQuery for each streaming record","Duplicate reference data in Pub/Sub messages","Store reference data in Dataflow worker memory statically"],correct:0,explanation:"Dataflow side inputs allow loading reference data periodically (e.g., hourly or daily) and making it available to all workers for in-memory joins. This is efficient for slowly changing reference data, avoiding per-record BigQuery queries while keeping data reasonably fresh.",wrongExplanations:{1:"Querying BigQuery for each streaming record creates massive overhead, latency, and BigQuery costs. For slowly changing reference data, periodic bulk loads with side inputs are far more efficient.",2:"Duplicating reference data in each Pub/Sub message wastes bandwidth and increases message size. It also creates consistency problems when reference data changes.",3:"Static worker memory doesn't update when reference data changes. Side inputs support periodic refreshes, keeping reference data current without pipeline restarts."}},{id:221,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You need to run both Spark and Presto workloads on the same dataset. Managing two separate clusters is operationally expensive. What Dataproc feature should you use?",options:["Use Dataproc Component Gateway to install multiple big data components (Spark, Presto, Hive) on one cluster","Run separate Dataproc clusters for each component","Dataproc only supports Spark","Use GKE with custom containers instead"],correct:0,explanation:"Dataproc supports optional components including Presto, Hive, HBase, Jupyter, and more through initialization actions and Component Gateway. You can run multiple components on a single cluster, reducing operational overhead and costs while providing web UIs for each component.",wrongExplanations:{1:"Running separate clusters increases costs and operational complexity. Dataproc's component support allows consolidation when workloads share data and resource requirements.",2:"This is false. Dataproc is based on Apache Hadoop and supports a full ecosystem including Spark, Hadoop MapReduce, Pig, Hive, Presto, Flink, and more through optional components.",3:"While GKE provides flexibility, managing big data components in Kubernetes adds significant complexity. Dataproc provides a fully managed Hadoop ecosystem with integrated component support."}},{id:222,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing data processing - Dataproc",question:"Your Dataproc cluster job failed, but the cluster was configured to delete after job completion. You need to troubleshoot the failure. How can you access logs?",options:["Check Cloud Logging which retains Dataproc logs even after cluster deletion","Logs are permanently lost when cluster is deleted","SSH into the cluster master node","Logs are only available in HDFS on the cluster"],correct:0,explanation:"Dataproc automatically exports logs to Cloud Logging (formerly Stackdriver Logging). These logs persist after cluster deletion, allowing post-mortem troubleshooting of failed jobs on ephemeral clusters. Logs include job output, YARN logs, and system logs.",wrongExplanations:{1:"This is false. Dataproc integrates with Cloud Logging to preserve logs even after clusters are deleted. This is essential for ephemeral cluster troubleshooting.",2:"You can't SSH into a deleted cluster. Cloud Logging provides access to logs after cluster deletion without requiring cluster access.",3:"HDFS is local to the cluster and lost when cluster is deleted. Cloud Logging provides durable log storage independent of cluster lifecycle."}},{id:223,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"You're building a Dataflow pipeline that processes financial transactions. You need to ensure exactly-once processing semantics when writing to BigQuery. What should you do?",options:["Use Dataflow's BigQueryIO with built-in exactly-once semantics and idempotent writes","Implement deduplication logic in your pipeline code","Use Pub/Sub exactly-once delivery","Dataflow cannot guarantee exactly-once semantics"],correct:0,explanation:"Dataflow's BigQueryIO connector provides exactly-once write semantics to BigQuery using streaming inserts with deduplication or using BigQuery Storage Write API. This is built into the connector and handles retries and failures transparently, ensuring financial transactions aren't duplicated.",wrongExplanations:{1:"While you can implement deduplication logic, BigQueryIO already provides this capability. Using the built-in feature is simpler, more reliable, and follows best practices.",2:"Pub/Sub provides at-least-once delivery, not exactly-once. Even with exactly-once delivery (where available), you still need idempotent sinks. BigQueryIO provides this for BigQuery writes.",3:"This is false. Dataflow with appropriate sinks (like BigQueryIO) provides exactly-once processing semantics, which is critical for financial and other sensitive data processing."}},{id:224,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You need to process data using Spark ML library for machine learning training. The training jobs require GPUs. Can you use Dataproc for this, and if so, how?",options:["Yes, configure Dataproc clusters with GPU-enabled machine types and appropriate GPU drivers","No, Dataproc doesn't support GPUs; use Vertex AI instead","Yes, but only on the master node","No, Spark doesn't support GPU acceleration"],correct:0,explanation:"Dataproc supports GPU-enabled machine types for worker nodes. You can configure clusters with NVIDIA GPUs and drivers for GPU-accelerated Spark workloads, including Spark ML training. This is useful for distributed GPU computing with existing Spark code.",wrongExplanations:{1:"While Vertex AI is excellent for managed ML training, Dataproc does support GPUs for users who want to run Spark-based ML workflows with GPU acceleration. Both services have valid use cases.",2:"GPUs can be attached to worker nodes where processing happens, not just the master node. Worker nodes need GPUs for distributed computation.",3:"This is false. Spark 3.x includes GPU support through Rapids accelerator. Dataproc can run GPU-accelerated Spark workloads for ETL and ML."}},{id:225,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"Your streaming Dataflow pipeline needs to handle late-arriving data (events that arrive hours after their event time). What Dataflow feature addresses this?",options:["Configure windowing with allowed lateness and triggers to handle late data appropriately","Late data cannot be handled in streaming pipelines","Increase the pipeline parallelism","Use batch processing instead"],correct:0,explanation:"Dataflow provides sophisticated late data handling through watermarks, allowed lateness, and triggers. You can specify how long to wait for late data, configure triggers to emit multiple results per window, and handle late data gracefully without losing events.",wrongExplanations:{1:"This is false. Handling late data is a core capability of Apache Beam and Dataflow. Watermarks and allowed lateness are designed specifically for this scenario.",2:"Parallelism affects throughput, not late data handling. Late data is handled through windowing and watermark configuration, independent of parallelism.",3:"Batch processing doesn't eliminate late data issues; it just changes the problem. Streaming with proper windowing and late data handling is often more appropriate for real-time use cases."}},{id:226,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing data processing - Dataflow",question:"Your Dataflow pipeline autoscaling is not responding to load increases. Workers remain at minimum despite growing backlog. What might be the issue?",options:["The pipeline has a non-parallelizable bottleneck or maxNumWorkers is set too low","Autoscaling is disabled by default","Dataflow doesn't support autoscaling","Increase the worker machine type"],correct:0,explanation:"Dataflow autoscales based on backlog and work distribution. If a pipeline has operations that aren't parallelizable (e.g., global aggregations, single-threaded operations), autoscaling can't help. Also, check maxNumWorkers setting which caps scaling. Identify bottleneck steps in monitoring UI.",wrongExplanations:{1:"Autoscaling is enabled by default in Dataflow. If it's not working, there's typically a configuration issue or a non-parallelizable bottleneck, not that it's disabled.",2:"This is false. Autoscaling is a core Dataflow feature. Dataflow automatically scales workers based on backlog and resource utilization.",3:"Machine type affects per-worker capacity but doesn't address autoscaling issues. If workers aren't scaling out, adding more powerful machines doesn't solve the parallelization problem."}},{id:227,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataproc",question:"You're migrating on-premises Hadoop workloads to Google Cloud. The workflows use custom Python scripts that interact with HDFS commands. What is the easiest migration path?",options:["Use Dataproc with Cloud Storage HDFS connector; most HDFS commands work with gs:// paths transparently","Rewrite all scripts to use Cloud Storage API directly","Use Filestore to replicate HDFS exactly","Keep HDFS on Dataproc persistent disks"],correct:0,explanation:"Dataproc includes the Cloud Storage connector that implements the HDFS interface for gs:// paths. Many HDFS commands and Hadoop filesystem API calls work with Cloud Storage transparently, enabling lift-and-shift migration with minimal code changes.",wrongExplanations:{1:"Rewriting scripts requires significant development effort. The Cloud Storage HDFS connector allows most scripts to work with minimal changes by supporting HDFS APIs.",2:"Filestore is expensive network storage meant for NFS use cases. It's not a suitable HDFS replacement. Cloud Storage with HDFS connector provides better cost and performance.",3:"Using HDFS on persistent disks increases costs and loses Cloud Storage benefits (durability, no storage management). Cloud Storage with HDFS connector is recommended for most workloads."}},{id:228,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"You need to update a production Dataflow streaming pipeline with new transformation logic without losing in-flight data or creating downtime. What should you do?",options:["Use Dataflow's update feature to perform an in-place update of the running pipeline","Stop the current pipeline, then start a new one","Run both pipelines in parallel, then switch","Streaming pipelines cannot be updated"],correct:0,explanation:"Dataflow supports updating running streaming pipelines in-place using the --update flag. This preserves pipeline state, in-flight data, and subscription positions without downtime. Dataflow determines which transforms changed and migrates state appropriately.",wrongExplanations:{1:"Stopping and restarting loses in-flight data and resets subscription positions. You'd need to reprocess data or accept data loss. Dataflow updates provide a better solution.",2:"Running parallel pipelines wastes resources and creates complexity in managing two pipelines and avoiding duplicate processing. In-place updates are cleaner.",3:"This is false. Dataflow specifically supports updating streaming pipelines in-place, which is a major operational advantage for production systems."}},{id:229,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Dataproc",question:"Your Dataproc cluster needs to read data from Cloud Storage and write results to BigQuery. Following least privilege, what roles should you grant the cluster's service account?",options:["roles/storage.objectViewer for source bucket and roles/bigquery.dataEditor for destination dataset","roles/editor at the project level","roles/owner for full access","No roles; Dataproc has automatic access"],correct:0,explanation:"Dataproc clusters use a service account for accessing GCP resources. Grant only the minimum required permissions: objectViewer (or objectUser) for reading Cloud Storage, and dataEditor for writing to BigQuery. This follows the principle of least privilege.",wrongExplanations:{1:"roles/editor grants broad project-level permissions far beyond what's needed for reading storage and writing to BigQuery. This violates least privilege and creates security risks.",2:"roles/owner grants full control over the project, including IAM modifications and resource deletion. This is extremely excessive for a data processing cluster.",3:"Dataproc clusters use a service account that requires appropriate IAM roles. Without proper roles, the cluster cannot access Cloud Storage or BigQuery."}},{id:230,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"Your organization wants to use Dataflow but developers are more familiar with Python than Java. Can you use Dataflow with Python, and are there any limitations?",options:["Yes, Apache Beam Python SDK is fully supported in Dataflow with feature parity for most use cases","No, Dataflow only supports Java","Yes, but Python pipelines can't use streaming","Yes, but Python has no access to GCP connectors"],correct:0,explanation:"Dataflow fully supports Apache Beam Python SDK with connectors for Pub/Sub, BigQuery, Cloud Storage, and more. Both streaming and batch processing are supported. Python and Java SDKs have near feature parity for most use cases.",wrongExplanations:{1:"This is false. Dataflow supports Apache Beam Java, Python, and Go SDKs. Python is a first-class citizen with full support.",2:"This is false. Python SDK fully supports streaming pipelines. Many production streaming pipelines use Python with Dataflow.",3:"This is false. Python SDK includes I/O connectors for all major GCP services including BigQuery, Pub/Sub, Cloud Storage, Bigtable, and more."}},{id:231,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing data processing - Dataproc",question:"You need to apply custom initialization scripts to install Python libraries on all Dataproc cluster nodes before jobs run. What should you use?",options:["Use Dataproc initialization actions to run scripts during cluster creation","SSH into each node manually after cluster creation","Include installation commands in each job submission","Initialization scripts are not supported"],correct:0,explanation:"Dataproc initialization actions are scripts that run on all nodes during cluster creation. They're used to install custom software, configure settings, or download data. Scripts can be stored in Cloud Storage and specified at cluster creation time.",wrongExplanations:{1:"Manual SSH doesn't scale and must be repeated for each cluster, especially problematic with autoscaling or ephemeral clusters. Initialization actions automate this.",2:"Including installation in each job wastes time and resources. Libraries should be installed once during cluster initialization, not repeatedly for each job.",3:"This is false. Initialization actions are a core Dataproc feature for customizing cluster configuration during creation."}},{id:232,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning data processing - Dataflow",question:"You need to migrate from batch to streaming processing for near real-time analytics. Your existing batch code is written in Apache Beam. How much code rewrite is required?",options:["Minimal changes: update the pipeline to read from Pub/Sub instead of batch source and specify windowing strategy","Complete rewrite: batch and streaming are completely different programming models","Partial rewrite of 50% of the code","Impossible: must use different technology for streaming"],correct:0,explanation:"Apache Beam's unified batch/streaming model allows most batch pipelines to convert to streaming with minimal changes: swap the source (e.g., Cloud Storage  Pub/Sub) and add windowing/triggering configuration. The core transformation logic often remains unchanged.",wrongExplanations:{1:"This is false. Apache Beam's key design principle is unified batch and streaming. The same transforms work for both, making batch-to-streaming migration much easier than with other frameworks.",2:"While some changes are needed (particularly around windowing), 50% is an overestimate. Most transformation logic remains the same; main changes are source, windowing, and possibly triggering.",3:"This is false. Apache Beam on Dataflow specifically enables using the same code for batch and streaming with configuration changes, not technology replacement."}},{id:233,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"Your application needs a low-latency caching layer for frequently accessed database queries. Cache hit rates are critical for performance. Which service should you use?",options:["Cloud Memorystore for Redis as a managed in-memory cache","Cloud Storage with lifecycle management","BigQuery materialized views","Persistent Disk attached to Compute Engine"],correct:0,explanation:"Cloud Memorystore for Redis provides fully managed, highly available Redis instances with sub-millisecond latency. It's specifically designed for caching use cases, session storage, and real-time analytics requiring fast in-memory data access.",wrongExplanations:{1:"Cloud Storage is object storage with milliseconds to seconds latency, not suitable for sub-millisecond caching requirements. It's designed for file storage, not in-memory caching.",2:"BigQuery materialized views cache query results but are designed for analytical queries in BigQuery, not application-level caching with sub-millisecond latency requirements.",3:"Persistent Disk is block storage for VMs with millisecond latency. For caching, in-memory systems like Memorystore provide much faster access (microseconds vs milliseconds)."}},{id:234,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"You need to deploy Memorystore for Redis with high availability across zones. What tier and configuration should you use?",options:["Standard Tier with automatic failover to a replica in a different zone","Basic Tier which provides built-in HA","Standard Tier with manual backups only","Basic Tier with multiple instances"],correct:0,explanation:"Memorystore Standard Tier provides a Redis instance with a replica in a different zone for high availability. Automatic failover ensures minimal downtime if the primary instance fails. Basic Tier doesn't provide replication or automatic failover.",wrongExplanations:{1:"Basic Tier is a single instance without replication or automatic failover. It's suitable for development/testing but not for production HA requirements.",2:"Standard Tier with only manual backups doesn't provide high availability during failures. HA requires automatic failover to a replica, not just backups.",3:"Running multiple Basic Tier instances doesn't provide automatic failover or data replication. Standard Tier with built-in replication is the correct solution for HA."}},{id:235,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Memorystore",question:"Your Memorystore Redis instance is approaching maximum memory capacity. What are your options to prevent out-of-memory errors?",options:["Scale up the instance to a larger memory size or configure eviction policies for automatic key removal","Memorystore automatically scales memory","Add more instances and manually shard data","Delete the instance and create a larger one"],correct:0,explanation:"Memorystore supports vertical scaling (increasing memory size) and configurable eviction policies (LRU, LFU, etc.) to automatically remove less-used keys when memory is full. Standard Tier supports scaling with minimal downtime.",wrongExplanations:{1:"Memorystore does not automatically scale memory. You must manually scale up the instance or configure eviction policies to manage memory usage.",2:"While you can create multiple instances, Memorystore doesn't provide automatic sharding. Manual sharding adds complexity. Scaling up or using eviction policies is simpler for most use cases.",3:"Deleting and recreating causes downtime and data loss. Standard Tier supports scaling up with minimal disruption. This should be the last resort, not the first option."}},{id:236,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"Your application running on GKE needs to connect to Memorystore Redis. Both are in the same project and region. What networking configuration is required?",options:["Ensure GKE and Memorystore are in the same VPC; Memorystore is accessible via private IP","Configure Cloud VPN between GKE and Memorystore","Use Memorystore's public IP endpoint","Deploy a proxy service in front of Memorystore"],correct:0,explanation:"Memorystore instances are created in a VPC with private IP addresses. GKE clusters in the same VPC can directly connect to Memorystore using the private IP. No additional networking configuration is needed.",wrongExplanations:{1:"Cloud VPN is for connecting different networks or on-premises to GCP. Within the same VPC, resources communicate directly via private IPs without VPN.",2:"Memorystore doesn't provide public IP endpoints for security reasons. It's only accessible via private IP within the VPC.",3:"A proxy is unnecessary. GKE pods can directly connect to Memorystore via private IP when in the same VPC. Adding a proxy increases complexity and latency."}},{id:237,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Memorystore",question:"You need to choose between Memorystore for Redis and Memorystore for Memcached. Your application requires data persistence and complex data structures (lists, sets). Which should you choose?",options:["Memorystore for Redis, which supports persistence and rich data structures","Memorystore for Memcached, which is faster for all use cases","Either one; they have identical features","Memorystore for Memcached with custom persistence logic"],correct:0,explanation:"Redis supports data persistence (RDB snapshots, AOF logs) and rich data structures (strings, lists, sets, sorted sets, hashes). Memcached is a pure in-memory cache with only key-value strings and no persistence. For complex data structures and persistence, Redis is the correct choice.",wrongExplanations:{1:"Memcached can be faster for simple key-value operations but doesn't support complex data structures or persistence. The question requires both features, making Redis the only viable option.",2:"This is false. Redis and Memcached have different capabilities. Redis supports complex data structures, persistence, pub/sub, and transactions. Memcached is simpler with only key-value caching.",3:"Memcached doesn't support persistence at all. Adding custom persistence logic defeats the purpose of using a managed caching service. Redis provides built-in persistence."}},{id:238,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Filestore",question:"Your application running on multiple Compute Engine instances needs shared file storage with NFS protocol support. What should you use?",options:["Cloud Filestore for managed NFS file storage accessible by multiple instances","Persistent Disk attached to one instance and shared via custom NFS server","Cloud Storage FUSE mount","Local SSD on each instance"],correct:0,explanation:"Cloud Filestore provides fully managed NFS file shares that multiple Compute Engine instances (or GKE pods) can mount concurrently. It's designed for shared file storage use cases requiring NFS protocol.",wrongExplanations:{1:"While you can create an NFS server on Compute Engine, this requires managing the server, handling availability, and capacity planning. Filestore provides a fully managed solution.",2:"Cloud Storage FUSE provides object storage access via filesystem interface but has different semantics than NFS (eventual consistency, no file locking). For true NFS compatibility, Filestore is better.",3:"Local SSDs are attached to individual instances and can't be shared between instances. Each instance would have separate storage, not shared storage."}},{id:239,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Filestore",question:"Your application requires high-throughput file storage for video rendering workloads. Which Filestore tier should you choose?",options:["High Scale or Enterprise tier for high throughput and IOPS requirements","Basic HDD tier for maximum capacity","Basic SSD tier is always sufficient","Filestore doesn't support high throughput workloads"],correct:0,explanation:"Filestore High Scale and Enterprise tiers provide high throughput (up to 1200+ MB/s) and IOPS for demanding workloads like video rendering, HPC, and analytics. Basic tiers have lower performance limits suitable for general file sharing.",wrongExplanations:{1:"Basic HDD tier provides maximum capacity but low throughput (100 MB/s). For high-throughput video rendering, this would be a bottleneck.",2:"Basic SSD tier (up to 480 MB/s) may be sufficient for some workloads but video rendering often requires higher throughput. High Scale/Enterprise tiers provide better performance.",3:"This is false. Filestore High Scale and Enterprise tiers are specifically designed for high-throughput workloads and can deliver over 1200 MB/s."}},{id:240,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Managing storage - Filestore",question:"Your Filestore instance is running out of capacity. How can you increase storage without downtime?",options:["Scale up the instance capacity through the console or gcloud command; Filestore supports online resizing","Create a new larger instance and manually copy data","Filestore capacity cannot be changed after creation","Delete and recreate the instance with larger capacity"],correct:0,explanation:"Filestore supports online capacity increases without downtime. You can scale up (but not down) the instance size through the console or gcloud commands. The resize happens while the instance remains available to clients.",wrongExplanations:{1:"Creating a new instance and copying data causes disruption to applications and requires coordinating the migration. Online resizing is simpler and maintains availability.",2:"This is false. Filestore supports scaling up capacity online without downtime. This is a key operational feature for production environments.",3:"Deleting and recreating causes downtime and data loss if not backed up. Online scaling is available and maintains continuous availability."}},{id:241,domain:"Configuring access and security",subdomain:"4.1 Managing access - Filestore",question:"You need to control which Compute Engine instances can mount your Filestore instance. What security mechanism should you configure?",options:["Configure VPC firewall rules and Filestore's IP-based access control","Use IAM roles to control mount access","Filestore is automatically secure; no configuration needed","Configure Cloud Armor policies"],correct:0,explanation:"Filestore uses IP-based access control. You can restrict which IP addresses/ranges can mount the NFS share. Combined with VPC firewall rules, this controls access at the network level. IAM controls who can manage the Filestore instance, not who can mount it.",wrongExplanations:{1:"IAM roles control who can create, delete, or modify Filestore instances in GCP, but don't control NFS mount access. NFS uses IP-based access control, not IAM authentication.",2:"Filestore requires explicit access control configuration. By default, any instance in the VPC can potentially access it if firewall rules allow. IP-based restrictions add security.",3:"Cloud Armor is for protecting load-balanced HTTP(S) services from DDoS and web attacks. It doesn't apply to NFS file shares."}},{id:242,domain:"Planning and implementing a cloud solution",subdomain:"2.2 Planning storage - Filestore",question:"Your GKE application needs persistent shared storage for multiple pods to read and write simultaneously. What combination should you use?",options:["Cloud Filestore with ReadWriteMany (RWX) PersistentVolumeClaim in GKE","Persistent Disk with ReadWriteOnce (RWO)","Cloud Storage bucket with gcsfuse","Local storage in each pod"],correct:0,explanation:"Filestore with ReadWriteMany (RWX) PersistentVolumeClaim allows multiple pods to mount the same volume simultaneously with read and write access. This is ideal for shared file storage in Kubernetes. Persistent Disk only supports RWO (one pod at a time).",wrongExplanations:{1:"Persistent Disk with ReadWriteOnce can only be mounted by one pod at a time. For simultaneous access by multiple pods, ReadWriteMany with Filestore is required.",2:"While gcsfuse can work, it provides object storage semantics, not true filesystem semantics. File locking and consistency guarantees differ from NFS. Filestore provides true shared filesystem.",3:"Local storage in each pod creates separate storage per pod, not shared storage. Changes in one pod aren't visible to others."}},{id:243,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Cloud Build",question:"You need to automatically build and deploy your application to GKE whenever code is pushed to your GitHub repository. What should you configure?",options:["Set up Cloud Build triggers connected to GitHub repository with build steps to build container and deploy to GKE","Manually run gcloud commands after each code push","Use Jenkins on Compute Engine","Cloud Build doesn't integrate with GitHub"],correct:0,explanation:"Cloud Build integrates with GitHub (and other repositories) through triggers. When code is pushed, triggers automatically start builds defined in cloudbuild.yaml. Build steps can build containers, push to Artifact Registry, and deploy to GKE - creating a complete CI/CD pipeline.",wrongExplanations:{1:"Manual commands don't provide automation, are error-prone, and don't scale. Cloud Build triggers provide fully automated CI/CD.",2:"While Jenkins can work, it requires managing infrastructure (VMs), updates, and plugins. Cloud Build is serverless and fully managed with native GCP integration.",3:"This is false. Cloud Build has first-class integrations with GitHub, GitLab, Bitbucket, and Cloud Source Repositories through build triggers."}},{id:244,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Cloud Build",question:"Your Cloud Build pipeline needs to access secrets (API keys, passwords) during builds. What is the recommended approach?",options:["Store secrets in Secret Manager and grant Cloud Build service account access to retrieve them during builds","Hardcode secrets in cloudbuild.yaml","Store secrets in environment variables in the Cloud Build configuration","Use Cloud KMS to encrypt secrets in source code"],correct:0,explanation:"Secret Manager is designed for storing sensitive data like API keys and passwords. Cloud Build can retrieve secrets from Secret Manager during builds using the secretEnv field or gcloud commands. This keeps secrets out of source code and build configurations.",wrongExplanations:{1:"Hardcoding secrets in configuration files is a critical security violation. These files are often committed to source control, exposing secrets to anyone with repository access.",2:"While you can use environment variables, storing secret values directly in Cloud Build configuration exposes them. Secret Manager integration allows referencing secrets without exposing values.",3:"Encrypting secrets in source code still requires managing decryption keys and exposes encrypted values in version control. Secret Manager provides better security and secret lifecycle management."}},{id:245,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing CI/CD - Cloud Build",question:"Your Cloud Build pipeline is failing with permission errors when trying to deploy to GKE. What is the likely cause?",options:["The Cloud Build service account lacks required roles (Kubernetes Engine Developer or Container Developer)","Cloud Build cannot deploy to GKE","GKE is in a different project","The build configuration is invalid"],correct:0,explanation:"Cloud Build uses a service account to interact with GCP services. To deploy to GKE, this service account needs appropriate roles like roles/container.developer or roles/container.clusterAdmin. Without these roles, deployment commands will fail with permission errors.",wrongExplanations:{1:"This is false. Cloud Build is commonly used for GKE deployments. It can run kubectl commands, deploy manifests, and manage Kubernetes resources.",2:"Cloud Build can deploy to GKE in different projects if the service account has cross-project permissions. The issue is likely missing roles, not cross-project deployment.",3:"Invalid build configuration would cause different errors (syntax errors, missing steps). Permission errors specifically indicate IAM/role issues."}},{id:246,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Artifact Registry",question:"You need to store Docker container images and Maven packages for your organization. What service should you use?",options:["Artifact Registry which supports multiple artifact formats (Docker, Maven, npm, Python, etc.)","Container Registry for all artifact types","Cloud Storage for storing artifacts","Separate services for each artifact type"],correct:0,explanation:"Artifact Registry is Google's recommended solution for storing and managing artifacts. It supports Docker containers, Maven, npm, Python, apt, and other formats in a single service. It's the successor to Container Registry with additional features.",wrongExplanations:{1:"Container Registry only supports Docker images. For Maven packages and other formats, you'd need different solutions. Artifact Registry consolidates all artifact types.",2:"While Cloud Storage can store files, it doesn't provide artifact-specific features like vulnerability scanning, versioning, access controls, or integration with build tools that Artifact Registry provides.",3:"Using separate services increases management complexity. Artifact Registry provides a unified solution for all artifact types with consistent IAM, scanning, and policies."}},{id:247,domain:"Configuring access and security",subdomain:"4.1 Managing IAM - Artifact Registry",question:"You need to allow Cloud Build to push images to Artifact Registry and GKE to pull images from it. What roles should you configure?",options:["Grant Cloud Build service account roles/artifactregistry.writer and GKE service account roles/artifactregistry.reader","Grant both service accounts roles/owner","Use public repository without authentication","Artifact Registry doesn't use IAM"],correct:0,explanation:"Artifact Registry uses IAM for access control. Cloud Build needs writer role to push artifacts, while GKE needs reader role to pull images. This follows least privilege by granting only necessary permissions.",wrongExplanations:{1:"roles/owner grants full project control, far exceeding what's needed for pushing/pulling artifacts. This violates least privilege and creates security risks.",2:"Public repositories remove access control, allowing anyone on the internet to pull images. This exposes your artifacts and creates security vulnerabilities.",3:"This is false. Artifact Registry uses IAM for fine-grained access control. You can control who can read, write, or manage artifacts using IAM roles."}},{id:248,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Cloud Deploy",question:"You need to deploy applications to multiple GKE environments (dev, staging, prod) with progressive rollouts and automated promotion. What GCP service simplifies this?",options:["Cloud Deploy for managed continuous delivery with deployment pipelines and progressive delivery","Cloud Build alone handles multi-environment deployments","Manually deploy to each environment using kubectl","Use separate Cloud Build triggers for each environment"],correct:0,explanation:"Cloud Deploy provides managed continuous delivery with support for deployment pipelines (dev  staging  prod), progressive delivery strategies (canary, blue-green), automated rollbacks, and approval workflows. It simplifies multi-environment deployments.",wrongExplanations:{1:"Cloud Build handles CI (building and testing) but doesn't provide CD pipeline features like progressive rollouts, deployment strategies, or approval workflows. Cloud Deploy adds these CD capabilities.",2:"Manual deployment doesn't scale, lacks automation, and is error-prone. It doesn't provide progressive delivery, rollback capabilities, or audit trails.",3:"While multiple triggers can deploy to different environments, this doesn't provide pipeline orchestration, approval workflows, or progressive delivery strategies that Cloud Deploy offers."}},{id:249,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing CI/CD - Cloud Build",question:"Your Cloud Build builds are taking too long. You notice builds are downloading dependencies from the internet on every run. How can you improve build times?",options:["Use Cloud Build's built-in caching or explicitly cache dependencies in Cloud Storage between builds","Increase build machine size","Run builds in parallel","Build times cannot be optimized"],correct:0,explanation:"Cloud Build supports caching dependencies between builds. You can use Kaniko cache for Docker layers or explicitly save/restore dependencies to Cloud Storage. This avoids re-downloading unchanged dependencies, significantly improving build times.",wrongExplanations:{1:"Larger machines provide more resources but don't avoid downloading dependencies. Caching eliminates redundant downloads, providing better speedup for dependency-heavy builds.",2:"Parallel builds help with concurrent builds but don't speed up individual builds that are slow due to dependency downloads. Caching addresses the root cause.",3:"This is false. Build times can be significantly optimized through caching, dependency optimization, multi-stage builds, and build configuration tuning."}},{id:250,domain:"Planning and implementing a cloud solution",subdomain:"2.3 Planning CI/CD - Cloud Build",question:"You need to build Docker images in Cloud Build without requiring Docker daemon. What builder should you use?",options:["Kaniko or Cloud Native Buildpacks which build images without Docker daemon","Standard Docker builder is the only option","Cloud Build doesn't support building Docker images","Use docker-in-docker"],correct:0,explanation:"Kaniko and Cloud Native Buildpacks (pack) build container images without requiring Docker daemon, making them ideal for Cloud Build's containerized build environment. They're more secure and efficient than docker-in-docker approaches.",wrongExplanations:{1:"Cloud Build supports multiple Docker builders. While the docker builder works, Kaniko and Buildpacks are recommended for better security and caching in Cloud Build's environment.",2:"This is false. Cloud Build is commonly used for building Docker images and provides multiple builder options including docker, Kaniko, and Buildpacks.",3:"Docker-in-docker runs Docker daemon inside a container, which has security implications and complexity. Kaniko and Buildpacks are purpose-built for daemonless image building."}},{id:251,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Migrate for Compute Engine",question:"You need to migrate on-premises VMware VMs to Google Cloud with minimal downtime. What service should you use?",options:["Migrate for Compute Engine (formerly Velostrata) for live migration with minimal downtime","Manually export VMs and import to GCP","Rebuild VMs from scratch in GCP","Use gsutil to copy VM files"],correct:0,explanation:"Migrate for Compute Engine enables live migration of VMs from on-premises, AWS, or Azure to Compute Engine with minimal downtime. It uses streaming technology to run workloads in GCP while data migrates in the background.",wrongExplanations:{1:"Manual export/import requires downtime during export, transfer, and import. It's time-consuming and causes significant service disruption compared to live migration.",2:"Rebuilding VMs requires reconfiguring applications, installing software, and migrating data manually. This takes weeks/months and is error-prone. Migration tools automate this process.",3:"gsutil is for transferring files to Cloud Storage, not for migrating VMs. VM migration requires tools that handle disk images, configurations, and minimize downtime."}},{id:252,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Database Migration Service",question:"You need to migrate your PostgreSQL database from on-premises to Cloud SQL with continuous replication and minimal downtime. What should you use?",options:["Database Migration Service for continuous replication and minimal downtime migration","pg_dump and restore to Cloud SQL","Manually set up replication","Export to CSV and import"],correct:0,explanation:"Database Migration Service provides serverless, easy-to-use database migration with continuous replication. It supports PostgreSQL and MySQL to Cloud SQL with minimal downtime by keeping databases in sync until cutover.",wrongExplanations:{1:"pg_dump requires downtime during export and restore. For large databases, this can be hours or days. Database Migration Service provides continuous replication for minimal downtime.",2:"Manual replication setup is complex, requires expertise in PostgreSQL replication, and is error-prone. Database Migration Service automates and simplifies the process.",3:"CSV export/import causes significant downtime, doesn't preserve database schema fully, and is inefficient for large databases. Database Migration Service handles schema and data migration properly."}},{id:253,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Transfer Service",question:"You need to transfer 100TB of data from AWS S3 to Cloud Storage. What is the most efficient approach?",options:["Use Storage Transfer Service for automated, managed transfer from AWS S3 to Cloud Storage","Download data locally and upload to Cloud Storage","Use gsutil rsync","Manually copy files one by one"],correct:0,explanation:"Storage Transfer Service is designed for large-scale data transfers from AWS S3, Azure Blob Storage, or HTTP/HTTPS sources to Cloud Storage. It provides parallel transfers, scheduling, filtering, and automatic retries - ideal for 100TB transfers.",wrongExplanations:{1:"Downloading locally requires massive bandwidth and local storage. For 100TB, this is impractical and slow. Storage Transfer Service transfers directly between clouds.",2:"While gsutil rsync can work, it runs from a single machine (limited bandwidth) and requires managing the transfer process. Storage Transfer Service provides managed, parallel transfers.",3:"Manual copying is completely impractical for 100TB. It would take months and be extremely error-prone. Automated transfer services are essential for large datasets."}},{id:254,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.1 Managing migration - Migrate for Compute Engine",question:"During a migration using Migrate for Compute Engine, the migrated VM is experiencing performance issues. What could be the cause?",options:["The VM is still in streaming mode where some data is being read from source; performance improves after full data migration","Migrate for Compute Engine always reduces performance","The VM configuration is corrupted","Migration failed"],correct:0,explanation:"Migrate for Compute Engine uses streaming technology where VMs run in GCP while data migrates in background. During streaming, some disk reads may fetch from source, causing latency. Performance improves once migration completes and all data is local.",wrongExplanations:{1:"This is false. After migration completes, VMs run natively on Compute Engine with full performance. Temporary performance issues during streaming are expected and resolve post-migration.",2:"Configuration corruption would cause failures or errors, not just performance issues. Streaming mode specifically explains the performance characteristics during migration.",3:"If migration failed, the VM wouldn't be running. Performance issues during streaming mode are expected behavior, not failure."}},{id:255,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Transfer Appliance",question:"You need to transfer 1PB of data to Google Cloud but have limited internet bandwidth. What offline transfer option should you use?",options:["Transfer Appliance, a physical device shipped to you for loading data, then shipped to Google for upload","Storage Transfer Service over internet","Ship hard drives via regular mail","Use gsutil over slow connection"],correct:0,explanation:"Transfer Appliance is Google's solution for offline data transfer. Google ships a high-capacity storage device to your location. You load data locally (fast), ship the appliance back, and Google uploads data to your Cloud Storage bucket. Ideal for large datasets with limited bandwidth.",wrongExplanations:{1:"Storage Transfer Service requires internet connectivity. With limited bandwidth, transferring 1PB could take months or years. Transfer Appliance bypasses bandwidth limitations.",2:"Google doesn't support informal shipping of hard drives. Transfer Appliance is the official, secure offline transfer method with encryption and chain-of-custody tracking.",3:"gsutil over slow connection would take an impractical amount of time for 1PB. Transfer Appliance is specifically designed for large offline transfers."}},{id:256,domain:"Planning and implementing a cloud solution",subdomain:"2.4 Planning migration - Assessment",question:"You're planning a large-scale migration to Google Cloud and need to discover and assess your current infrastructure. What tool should you use?",options:["Use migration assessment tools like StratoZone or RVTools to discover infrastructure and plan migration","Manually document all infrastructure","Migrate everything immediately without assessment","Assessment is not necessary"],correct:0,explanation:"Migration assessment tools automatically discover on-premises infrastructure, analyze dependencies, estimate costs, and recommend migration strategies. StratoZone (Google's tool) and similar products provide data-driven migration planning.",wrongExplanations:{1:"Manual documentation is time-consuming, incomplete, and quickly becomes outdated. Automated discovery tools provide comprehensive, accurate inventory faster.",2:"Migrating without assessment leads to cost overruns, architectural problems, and migration failures. Assessment identifies dependencies, sizing requirements, and potential issues.",3:"This is false. Assessment is a critical first phase of migration. It informs migration strategy, prioritization, timeline, and budget. Skipping it leads to failed migrations."}},{id:257,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Committed Use Discounts",question:"Your Compute Engine workloads run consistently 24/7 with predictable resource needs. How can you reduce costs?",options:["Purchase Committed Use Discounts (CUDs) for 1 or 3 years to get up to 57% discount","Use Spot VMs for production workloads","Scale down instances daily","No cost optimization is possible"],correct:0,explanation:"Committed Use Discounts provide significant savings (up to 57% for 3-year commitments) for predictable workloads. You commit to use a certain amount of vCPUs and memory in a region for 1 or 3 years. Ideal for steady-state production workloads.",wrongExplanations:{1:"Spot VMs can be preempted at any time, making them unsuitable for production workloads requiring 24/7 availability. CUDs provide discounts without preemption risk.",2:"Scaling down 24/7 workloads defeats the purpose and may impact availability. For always-on workloads, CUDs provide cost savings without operational changes.",3:"This is false. Multiple cost optimization options exist: CUDs, sustained use discounts, rightsizing, Spot VMs for fault-tolerant workloads, and custom machine types."}},{id:258,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Recommender",question:"You want to identify idle resources and rightsizing opportunities across your GCP organization. What should you use?",options:["Active Assist Recommender which provides automated recommendations for cost optimization and resource efficiency","Manually review all resources in Cloud Console","Wait for billing alerts","GCP doesn't provide optimization recommendations"],correct:0,explanation:"Active Assist Recommender analyzes resource usage and provides automated recommendations including idle resource deletion, VM rightsizing, committed use discounts, and more. It uses machine learning to identify optimization opportunities across your organization.",wrongExplanations:{1:"Manual review doesn't scale, misses patterns, and is time-consuming. Recommender uses ML to analyze usage patterns and identify opportunities you might miss.",2:"Billing alerts notify you of spending but don't identify specific optimization opportunities. Recommender proactively suggests cost-saving actions.",3:"This is false. Google Cloud provides comprehensive recommendations through Active Assist Recommender for cost, performance, security, and reliability improvements."}},{id:259,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Cost optimization - Monitoring",question:"You notice unexpected cost increases in your billing. What tools should you use to investigate?",options:["Use Cloud Billing Reports with cost breakdown by service, SKU, and label; export to BigQuery for detailed analysis","Wait for monthly invoice","Guess which service is causing costs","Billing data is not available"],correct:0,explanation:"Cloud Billing Reports provide detailed cost breakdowns by project, service, SKU, label, and time period. Exporting to BigQuery enables custom analysis with SQL. This combination helps identify cost drivers and unexpected spending.",wrongExplanations:{1:"Monthly invoices provide totals but lack the granularity needed to investigate cost increases. Real-time billing reports enable proactive cost management.",2:"Guessing is ineffective and wasteful. Cloud Billing provides detailed data to identify exact sources of cost increases.",3:"This is false. GCP provides comprehensive billing data through Cloud Billing Reports, BigQuery export, budgets, and cost allocation using labels and projects."}},{id:260,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Rightsizing",question:"Your VM instances are consistently using only 20% of allocated CPU and memory. What should you do?",options:["Downsize to smaller machine types or use custom machine types to match actual resource needs","Keep overprovisioned to handle potential spikes","Add more VMs","Ignore utilization metrics"],correct:0,explanation:"Rightsizing reduces costs by matching instance size to actual usage. With 20% utilization, you're paying for 80% unused capacity. Downsizing or using custom machine types (exact vCPU/memory configuration) optimizes costs while meeting workload needs.",wrongExplanations:{1:"While some overhead is reasonable, 80% unused capacity is excessive. Autoscaling or load balancing can handle spikes more cost-effectively than constant overprovisioning.",2:"Adding VMs increases costs. The issue is oversized instances, not lack of instances. Rightsizing existing instances is the solution.",3:"Ignoring low utilization wastes money. Recommender provides rightsizing suggestions, and monitoring metrics guide optimization decisions."}},{id:261,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Spot VMs",question:"You have a batch processing job that can tolerate interruptions and restart from checkpoints. How can you minimize compute costs?",options:["Use Spot VMs which offer up to 90% discount but can be preempted when Google needs capacity","Use regular on-demand instances","Purchase committed use discounts","Use the smallest instance type"],correct:0,explanation:"Spot VMs (formerly preemptible VMs) offer significant discounts (up to 90%) for fault-tolerant, interruptible workloads. Since your job can checkpoint and restart, Spot VMs are ideal. Google can reclaim them with 30-second warning when capacity is needed.",wrongExplanations:{1:"On-demand instances cost more but provide no benefit for fault-tolerant workloads that can use Spot VMs. Spot VMs provide the same performance at much lower cost.",2:"CUDs provide discounts for committed usage but aren't as aggressive as Spot VM discounts. For batch jobs that tolerate interruption, Spot VMs offer better savings.",3:"The smallest instance might reduce costs but could make jobs run too slowly. Spot VMs provide the same instance types at 90% discount, giving better cost-performance balance."}},{id:262,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Storage classes",question:"You have log files that must be retained for compliance but are rarely accessed after 30 days. How can you reduce storage costs?",options:["Use Cloud Storage lifecycle policies to transition objects to Coldline or Archive storage after 30 days","Keep all files in Standard storage class","Delete files after 30 days","Compress files manually"],correct:0,explanation:"Cloud Storage lifecycle management automatically transitions objects between storage classes based on age or conditions. Coldline (30-90 day access) and Archive (yearly access) cost significantly less than Standard storage, perfect for compliance retention of rarely-accessed data.",wrongExplanations:{1:"Standard storage costs more than Coldline/Archive. Without lifecycle policies, you pay premium prices for rarely-accessed data, wasting money on retention.",2:"Deleting files violates compliance requirements. Lifecycle policies allow cost-effective retention by moving to cheaper storage classes, not deletion.",3:"While compression helps, it doesn't address storage class costs. Lifecycle policies to cheaper storage classes provide better cost reduction, and can be combined with compression."}},{id:263,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.2 Cost optimization - Budgets",question:"You want to automatically alert your team when project spending reaches 80% of monthly budget. What should you configure?",options:["Create a budget with threshold alert at 80% and configure email or Pub/Sub notification","Manually check billing daily","Wait for budget overrun","Budgets cannot send alerts"],correct:0,explanation:"Cloud Billing budgets allow setting spending thresholds with automated alerts. You can configure email notifications or Pub/Sub messages at specific thresholds (50%, 80%, 100%, etc.). This enables proactive cost management before budgets are exceeded.",wrongExplanations:{1:"Manual checking doesn't scale, can miss spending spikes, and is reactive. Automated budget alerts provide timely notifications without manual effort.",2:"Waiting for overrun means costs already exceeded budget. Proactive alerts at 80% allow time to investigate and take action before budget is fully consumed.",3:"This is false. Budget alerts are a core feature. You can configure multiple threshold alerts (percentage or absolute amounts) with email or Pub/Sub notifications."}},{id:264,domain:"Planning and implementing a cloud solution",subdomain:"2.5 Cost optimization - Labels",question:"Your organization has multiple teams and projects sharing a billing account. You need to track costs per team for chargeback. What should you implement?",options:["Apply labels to resources with team names and use billing reports filtered by labels for cost allocation","Create separate billing accounts per team","Manually allocate costs in spreadsheets","Cost allocation by team is not possible"],correct:0,explanation:"Labels are key-value pairs attached to resources (VMs, buckets, etc.). Billing exports include labels, enabling cost filtering and allocation. You can track costs by team, environment, cost center, or any dimension using labels. This enables accurate chargeback without separate billing accounts.",wrongExplanations:{1:"Separate billing accounts create administrative overhead and lose volume discounts. Labels provide cost allocation within a single billing account.",2:"Manual spreadsheet allocation is error-prone, time-consuming, and doesn't scale. Labels provide automated, accurate cost attribution in billing reports.",3:"This is false. Labels combined with billing reports and BigQuery export enable detailed cost allocation across any dimension you define."}},{id:265,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud NAT",question:"Your Compute Engine instances in a private subnet need to download updates from the internet but should not be directly accessible from the internet. What should you configure?",options:["Configure Cloud NAT to provide outbound internet access without assigning public IPs to instances","Assign public IPs to all instances","Use Cloud VPN for internet access","Private instances cannot access the internet"],correct:0,explanation:"Cloud NAT (Network Address Translation) provides managed outbound internet connectivity for private instances without exposing them to inbound internet traffic. Instances can download updates, access APIs, etc., while remaining private and secure.",wrongExplanations:{1:"Assigning public IPs exposes instances to inbound internet traffic, increasing attack surface. Cloud NAT provides outbound-only access, maintaining security.",2:"Cloud VPN connects to on-premises or other private networks, not the public internet. For internet access from private instances, Cloud NAT is the solution.",3:"This is false. Cloud NAT specifically enables private instances to access the internet for outbound connections while preventing inbound connections."}},{id:266,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - VPC Peering",question:"You have two VPC networks in the same organization and need to enable direct communication between resources in both VPCs. What should you configure?",options:["Set up VPC Network Peering to create direct network connectivity between the two VPCs","Use Cloud VPN to connect the VPCs","Assign public IPs and communicate over internet","VPCs cannot communicate"],correct:0,explanation:"VPC Network Peering creates direct, private connectivity between VPC networks within or across projects/organizations. Traffic stays on Google's network, providing low latency, high bandwidth, and security without internet or VPN gateways.",wrongExplanations:{1:"While Cloud VPN can connect VPCs, it adds latency and complexity. VPC Peering provides direct connectivity with better performance and simpler configuration for VPC-to-VPC connections.",2:"Using public IPs sends traffic over internet, increasing latency, costs (egress), and security risks. VPC Peering provides private, secure connectivity.",3:"This is false. VPC Peering is specifically designed for VPC-to-VPC connectivity within Google Cloud."}},{id:267,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Private Service Connect",question:"You need to access a Google-managed service (like BigQuery or Cloud SQL) from your VPC using private IP addresses without exposing traffic to the internet. What should you use?",options:["Private Service Connect or Private Google Access for private IP connectivity to Google services","Use public IPs for all Google services","VPN is required for Google services","Private access to Google services is not possible"],correct:0,explanation:"Private Service Connect enables private connectivity to Google APIs and services using internal IP addresses. Private Google Access allows VMs with only private IPs to reach Google services. Both keep traffic on Google's network without internet exposure.",wrongExplanations:{1:"Public IPs send traffic over internet, increasing latency and security risks. Private access keeps traffic on Google's private network.",2:"VPN is for connecting to on-premises or other external networks. Private Service Connect/Private Google Access provide direct private connectivity to Google services without VPN.",3:"This is false. Private Service Connect and Private Google Access specifically provide private IP connectivity to Google services."}},{id:268,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Shared VPC",question:"Your organization has multiple projects that need to share a common network while maintaining project-level isolation for billing and IAM. What should you implement?",options:["Use Shared VPC to share a VPC network across multiple projects with centralized network administration","Create identical VPC in each project","Use VPC Peering between all projects","Use a single project for all resources"],correct:0,explanation:"Shared VPC allows a central host project to share VPC networks with multiple service projects. Network admins manage networking centrally while project owners manage their resources. This provides centralized network control with project separation for billing and IAM.",wrongExplanations:{1:"Identical VPCs in each project create management overhead, inconsistency risk, and don't provide centralized control. Shared VPC centralizes network management.",2:"VPC Peering creates mesh connections between separate VPCs, requiring N(N-1)/2 connections. Shared VPC provides simpler hub-and-spoke model with centralized management.",3:"Single project loses project-level isolation for billing, IAM, and resource organization. Shared VPC provides network sharing while maintaining project separation."}},{id:269,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking troubleshooting - Connectivity Tests",question:"You need to verify that traffic can flow from a VM in one subnet to a VM in another subnet. What tool should you use to test connectivity?",options:["Network Connectivity Tests (formerly Network Intelligence Center) to simulate traffic and identify connectivity issues","Ping from the VM","Guess if firewall rules are correct","Connectivity testing is not available"],correct:0,explanation:"Network Connectivity Tests analyze your configuration (firewall rules, routes, VPC, etc.) and simulate traffic between endpoints without sending actual packets. It identifies misconfigurations blocking connectivity, providing detailed analysis of the network path.",wrongExplanations:{1:"Ping tests actual connectivity but doesn't explain why connection fails if it does. Connectivity Tests analyze configuration to identify issues before deploying or sending traffic.",2:"Guessing is inefficient and error-prone. Connectivity Tests provide automated analysis of firewall rules, routes, and network configuration.",3:"This is false. Network Connectivity Tests provide configuration analysis and traffic simulation to diagnose connectivity issues."}},{id:270,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud Interconnect",question:"You need high-bandwidth, low-latency connectivity between your on-premises datacenter and Google Cloud for production workloads. What should you use?",options:["Dedicated Interconnect (10 Gbps or 100 Gbps) or Partner Interconnect for high-bandwidth private connectivity","Cloud VPN for all use cases","Public internet","Interconnect is not available"],correct:0,explanation:"Cloud Interconnect provides private, high-bandwidth connectivity (10-100+ Gbps) with lower latency than internet. Dedicated Interconnect requires direct physical connection to Google. Partner Interconnect uses service provider connections. Both provide enterprise-grade connectivity for production workloads.",wrongExplanations:{1:"Cloud VPN provides up to 3 Gbps per tunnel over internet. For high-bandwidth requirements (10+ Gbps), Interconnect provides dedicated capacity and better performance.",2:"Public internet has variable latency, limited bandwidth, and security concerns. Interconnect provides private, dedicated connectivity with SLAs.",3:"This is false. Cloud Interconnect is Google's enterprise connectivity solution with Dedicated and Partner options for high-bandwidth private connectivity."}},{id:271,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud VPN",question:"You need to connect your on-premises network to Google Cloud VPC with automatic failover. What VPN configuration should you use?",options:["HA VPN with two tunnels to two GCP gateways providing 99.99% SLA with automatic failover","Classic VPN with single tunnel","Multiple independent Classic VPNs","VPN doesn't support failover"],correct:0,explanation:"HA VPN provides high availability with 99.99% SLA by using two VPN gateways and tunnels. Automatic failover occurs if one tunnel fails. Classic VPN offers 99.9% SLA with single gateway. HA VPN is recommended for production workloads requiring high availability.",wrongExplanations:{1:"Classic VPN with single tunnel provides 99.9% SLA without automatic failover. For HA requirements, HA VPN with redundant tunnels is the better choice.",2:"Multiple Classic VPNs can provide redundancy but require manual configuration and don't provide automatic failover. HA VPN provides built-in redundancy and automatic failover.",3:"This is false. HA VPN specifically provides automatic failover between redundant tunnels for high availability."}},{id:272,domain:"Configuring access and security",subdomain:"4.2 Network security - Firewall rules",question:"You need to allow SSH access to your Compute Engine instances only from your corporate IP range. What firewall rule should you create?",options:["Create ingress allow rule for TCP port 22 with source IP range matching your corporate IPs and target tags for your instances","Allow port 22 from 0.0.0.0/0 (all IPs)","Firewall rules cannot filter by source IP","SSH is automatically allowed"],correct:0,explanation:"VPC firewall rules can filter by source IP ranges. Creating an ingress allow rule for port 22 with your corporate IP range restricts SSH access to only your network. Using target tags applies the rule to specific instances, following least privilege.",wrongExplanations:{1:"Allowing SSH from all IPs (0.0.0.0/0) exposes instances to brute-force attacks and unauthorized access attempts. Restrict source IPs to known, trusted networks.",2:"This is false. Firewall rules support source IP filtering using CIDR ranges, making IP-based access control a core capability.",3:"SSH is not automatically allowed. Default VPC firewall rules allow some traffic, but custom VPCs require explicit rules. Even with default rules, restricting to corporate IPs improves security."}},{id:273,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud DNS",question:"Your application needs DNS resolution for services within your VPC using custom domain names. What should you configure?",options:["Create a Cloud DNS private zone for internal DNS resolution within your VPC","Use public DNS zones for internal names","Manually edit /etc/hosts on each VM","Internal DNS is not supported"],correct:0,explanation:"Cloud DNS private zones provide DNS resolution for resources within your VPC. They're not visible from the internet, making them ideal for internal service discovery and custom internal domain names. VMs in the VPC can resolve these names automatically.",wrongExplanations:{1:"Public DNS zones expose internal names to the internet, creating security risks. Private zones keep DNS resolution internal to your VPC.",2:"Manual /etc/hosts editing doesn't scale, is error-prone, and requires updating each VM when IPs change. Cloud DNS provides centralized, dynamic DNS management.",3:"This is false. Cloud DNS private zones specifically provide internal DNS resolution for VPC resources."}},{id:274,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Load balancing",question:"You need to load balance HTTPS traffic across multiple regions with automatic failover and content-based routing. What load balancer should you use?",options:["Global HTTP(S) Load Balancer with backend services in multiple regions and URL maps for content routing","Network Load Balancer","Internal Load Balancer","Manually distribute traffic with DNS"],correct:0,explanation:"Global HTTP(S) Load Balancer operates at L7, supports HTTPS, SSL termination, content-based routing (URL maps), and global distribution with automatic failover between regions. It routes users to the nearest healthy backend, providing optimal performance and availability.",wrongExplanations:{1:"Network Load Balancer operates at L4 (TCP/UDP), doesn't support HTTPS termination or content-based routing. For HTTP(S) with advanced routing, HTTP(S) Load Balancer is required.",2:"Internal Load Balancer is for VPC-internal traffic, not internet-facing global load balancing. Global HTTP(S) Load Balancer handles external traffic.",3:"Manual DNS-based distribution doesn't provide health checks, automatic failover, or content-based routing. Load balancers provide automated traffic distribution and failover."}},{id:275,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Flow Logs",question:"You need to troubleshoot network connectivity issues and analyze traffic patterns in your VPC. What should you enable?",options:["VPC Flow Logs to capture IP traffic information for network monitoring and forensics","Cloud Logging only","Firewall logs only","Network logging is not available"],correct:0,explanation:"VPC Flow Logs record samples of network flows sent/received by VM instances. They provide visibility into traffic patterns, help troubleshoot connectivity issues, perform security analysis, and optimize network usage. Logs can be analyzed in Cloud Logging or exported to BigQuery.",wrongExplanations:{1:"Cloud Logging captures application and system logs but doesn't capture network flow data. Flow Logs specifically provide network traffic visibility.",2:"Firewall logs show allowed/denied traffic based on rules but don't provide detailed flow information for all traffic. Flow Logs provide comprehensive traffic data.",3:"This is false. VPC Flow Logs specifically provide network traffic visibility for monitoring, troubleshooting, and security analysis."}},{id:276,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud CDN",question:"Your web application serves static content globally. Users far from your backend experience high latency. How can you improve performance?",options:["Enable Cloud CDN with your HTTP(S) Load Balancer to cache content at Google's edge locations worldwide","Deploy VMs in every region","Increase backend instance size","Content delivery optimization is not possible"],correct:0,explanation:"Cloud CDN caches content at Google's globally distributed edge locations. When enabled with HTTP(S) Load Balancer, static content is served from the edge location nearest to users, dramatically reducing latency and backend load. Cache headers control what's cached.",wrongExplanations:{1:"Deploying VMs in every region is expensive and complex. Cloud CDN provides global content distribution without managing infrastructure in multiple regions.",2:"Larger backend instances don't reduce network latency for distant users. CDN edge caching reduces latency by serving content close to users.",3:"This is false. Cloud CDN specifically optimizes content delivery by caching at edge locations worldwide."}},{id:277,domain:"Configuring access and security",subdomain:"4.2 Network security - Cloud Armor",question:"Your public-facing web application is experiencing DDoS attacks. What should you configure to protect it?",options:["Enable Cloud Armor with security policies on your HTTP(S) Load Balancer to block malicious traffic","Use firewall rules only","Increase instance count","DDoS protection is not available"],correct:0,explanation:"Cloud Armor provides DDoS protection, WAF capabilities, and customizable security policies for HTTP(S) Load Balancers. It can block traffic based on IP addresses, geolocation, request patterns, and more. Google's infrastructure absorbs attacks at the edge.",wrongExplanations:{1:"VPC firewall rules operate at L3/L4 and can't protect against application-layer (L7) attacks. Cloud Armor provides L7 protection for HTTP(S) traffic.",2:"Increasing instances helps with capacity but doesn't block attack traffic. Attackers can overwhelm any number of instances. Cloud Armor blocks malicious traffic before it reaches backends.",3:"This is false. Cloud Armor provides DDoS protection, WAF, and customizable security policies specifically for protecting HTTP(S) applications."}},{id:278,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Packet Mirroring",question:"You need to capture and analyze network traffic for security monitoring and compliance. What GCP feature should you use?",options:["Packet Mirroring to clone traffic from specified instances and send to monitoring tools","VPC Flow Logs only","Manual tcpdump on each instance","Packet capture is not supported"],correct:0,explanation:"Packet Mirroring clones traffic from specified instances and sends it to a collector instance running monitoring/security tools. This enables deep packet inspection, intrusion detection, and compliance monitoring without affecting production traffic.",wrongExplanations:{1:"Flow Logs provide metadata about traffic but not full packet capture. For deep packet inspection and protocol analysis, Packet Mirroring provides complete packet data.",2:"Manual tcpdump on instances requires installing software on each VM, affects performance, and doesn't scale. Packet Mirroring provides centralized, non-invasive capture.",3:"This is false. Packet Mirroring specifically provides network traffic cloning for security monitoring and analysis."}},{id:279,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Private Google Access",question:"Your VMs have only private IP addresses (no external IPs) but need to access Google Cloud services like Cloud Storage and BigQuery. What should you enable?",options:["Enable Private Google Access on the subnet to allow VMs with private IPs to reach Google services","Assign external IPs to all VMs","Use Cloud VPN","Private VMs cannot access Google services"],correct:0,explanation:"Private Google Access allows VMs with only private IPs to reach Google APIs and services (Cloud Storage, BigQuery, etc.) using internal routing. Traffic stays on Google's network without internet exposure. Enabled per subnet in VPC settings.",wrongExplanations:{1:"Assigning external IPs exposes VMs to internet, increasing security risks. Private Google Access provides service access while keeping VMs private.",2:"Cloud VPN is for connecting to on-premises networks, not for accessing Google Cloud services from VMs. Private Google Access is the solution for this use case.",3:"This is false. Private Google Access specifically enables private-only VMs to access Google Cloud services."}},{id:280,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - SSL Policies",question:"You need to ensure your HTTP(S) Load Balancer uses only strong TLS versions and cipher suites. What should you configure?",options:["Create and attach an SSL policy specifying minimum TLS version and allowed cipher suites","SSL configuration is automatic","Modify backend application code","TLS configuration is not available"],correct:0,explanation:"SSL policies define the TLS features (versions, cipher suites) that clients can use to connect to your load balancer. You can enforce minimum TLS 1.2, restrict weak ciphers, and meet compliance requirements. Policies attach to load balancer frontends.",wrongExplanations:{1:"While default SSL configuration is secure, custom requirements (compliance, security standards) often need specific TLS versions and ciphers. SSL policies provide this control.",2:"Backend applications don't handle SSL termination when using HTTP(S) Load Balancer. The load balancer terminates SSL, and SSL policies configure its behavior.",3:"This is false. SSL policies provide detailed control over TLS versions and cipher suites for HTTP(S) Load Balancers."}},{id:281,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Troubleshooting firewall",question:"You created a firewall rule to allow traffic but connections are still failing. What should you check?",options:["Verify rule priority, direction (ingress/egress), targets match your instances, and no higher-priority deny rules exist","Only check if rule exists","Firewall rules apply automatically","Delete all firewall rules"],correct:0,explanation:"Firewall rules are evaluated by priority (lower number = higher priority). A deny rule with higher priority overrides allow rules. Also verify direction (ingress/egress), source/destination filters, target tags/service accounts match your instances, and protocols/ports are correct.",wrongExplanations:{1:"Just checking if a rule exists isn't sufficient. Misconfigured priority, targets, or competing deny rules can prevent the allow rule from working.",2:"Firewall rules apply automatically when created, but misconfigurations can prevent them from working as expected. Systematic troubleshooting is needed.",3:"Deleting all rules creates default deny, making things worse. Systematic review of priority, targets, and rule logic identifies the issue."}},{id:282,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Service Directory",question:"You have microservices across multiple environments (GKE, Compute Engine, on-premises) and need unified service discovery. What should you use?",options:["Service Directory for centralized service registry across hybrid environments","Kubernetes service discovery only","Manual service endpoint management","Unified service discovery is not available"],correct:0,explanation:"Service Directory provides a unified service registry for hybrid and multi-cloud environments. Services from GKE, Compute Engine, Cloud Run, and on-premises can register, enabling centralized discovery and DNS-based resolution across environments.",wrongExplanations:{1:"Kubernetes service discovery only works within a K8s cluster. For services outside GKE or in multiple clusters, Service Directory provides cross-environment discovery.",2:"Manual endpoint management doesn't scale, is error-prone, and lacks automation. Service Directory provides automated registration and discovery.",3:"This is false. Service Directory specifically provides unified service discovery across hybrid and multi-cloud environments."}},{id:283,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud Router",question:"You're using Cloud VPN or Interconnect and need to dynamically exchange routes between your on-premises network and GCP VPC. What should you configure?",options:["Cloud Router with BGP to dynamically exchange routes between on-premises and VPC","Static routes only","Manual route updates","Dynamic routing is not supported"],correct:0,explanation:"Cloud Router uses BGP (Border Gateway Protocol) to dynamically exchange routes between your VPC and on-premises network via VPN or Interconnect. This eliminates manual route management when networks change, providing automatic failover and load distribution.",wrongExplanations:{1:"Static routes require manual updates whenever network topology changes. Cloud Router with BGP automates route exchange, providing dynamic adaptation to network changes.",2:"Manual route updates don't scale, are error-prone, and can't provide automatic failover. Cloud Router with BGP automates routing.",3:"This is false. Cloud Router with BGP specifically provides dynamic routing for VPN and Interconnect connections."}},{id:284,domain:"Configuring access and security",subdomain:"4.2 Network security - Hierarchical Firewall Policies",question:"You need to enforce organization-wide firewall rules across all VPCs and projects. What should you use?",options:["Hierarchical Firewall Policies at organization or folder level to enforce rules across multiple projects","Create identical VPC firewall rules in each project manually","Use organization policies","Cross-project firewall rules are not possible"],correct:0,explanation:"Hierarchical Firewall Policies apply at organization or folder level, enforcing rules across all VPCs in child projects. This enables centralized security policy management without duplicating rules in each project. Rules are inherited and can't be overridden by projects.",wrongExplanations:{1:"Creating identical rules in each project is error-prone, doesn't scale, and lacks centralized control. Hierarchical policies provide centralized enforcement.",2:"Organization policies control GCP resource configurations (e.g., allowed regions) but don't provide network firewall functionality. Hierarchical Firewall Policies handle network security.",3:"This is false. Hierarchical Firewall Policies specifically enable organization-wide network security policies across projects and VPCs."}},{id:285,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Internal Load Balancer",question:"You need to load balance traffic between VMs for a private internal application not exposed to the internet. What load balancer should you use?",options:["Internal TCP/UDP Load Balancer for private load balancing within your VPC","Global HTTP(S) Load Balancer","Network Load Balancer (external)","Internal load balancing is not available"],correct:0,explanation:"Internal TCP/UDP Load Balancer distributes traffic within your VPC using private IPs. It's not accessible from internet, making it ideal for internal services, microservices communication, and multi-tier applications. Supports regional backends with health checks.",wrongExplanations:{1:"Global HTTP(S) Load Balancer is for internet-facing applications with global backends. For private internal load balancing, Internal Load Balancer is the correct choice.",2:"Network Load Balancer (external) uses external IPs and is internet-facing. For internal applications, Internal Load Balancer with private IPs is appropriate.",3:"This is false. Internal TCP/UDP Load Balancer and Internal HTTP(S) Load Balancer specifically provide private load balancing within VPCs."}},{id:286,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Network metrics",question:"You need to monitor load balancer performance including latency, request count, and backend health. Where should you look?",options:["Cloud Monitoring metrics for load balancers showing request count, latency, error rates, and backend health","Load balancer logs only","Guess performance from application behavior","Load balancer metrics are not available"],correct:0,explanation:"Load balancers export comprehensive metrics to Cloud Monitoring including request/response counts, latency percentiles, error rates, bandwidth, and backend health status. These metrics enable performance monitoring, capacity planning, and alerting.",wrongExplanations:{1:"Logs provide request details but metrics provide aggregated performance data better suited for monitoring trends, alerting on thresholds, and dashboards.",2:"Guessing is ineffective. Cloud Monitoring provides detailed, real-time metrics for data-driven performance analysis.",3:"This is false. Load balancers export comprehensive metrics to Cloud Monitoring for performance monitoring and alerting."}},{id:287,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Cloud Domains",question:"You need to register a new domain name and manage DNS within Google Cloud. What service should you use?",options:["Cloud Domains for domain registration and integrated DNS management with Cloud DNS","Use external registrar only","Domain registration is not available in GCP","Cloud DNS can register domains"],correct:0,explanation:"Cloud Domains provides domain registration with integrated Cloud DNS management. You can register new domains, transfer existing domains, and manage DNS zones in one place. It simplifies domain management within GCP with built-in DNSSEC support.",wrongExplanations:{1:"While external registrars work, Cloud Domains provides integrated experience with Cloud DNS, GCP billing, and IAM. It simplifies management for GCP-hosted applications.",2:"This is false. Cloud Domains specifically provides domain registration services integrated with GCP.",3:"Cloud DNS provides DNS hosting and management but not domain registration. Cloud Domains handles registration, while Cloud DNS manages zones."}},{id:288,domain:"Configuring access and security",subdomain:"4.2 Network security - Private Service Connect endpoints",question:"You need to access a partner's service privately without exposing traffic to the internet. The partner has published their service via Private Service Connect. What should you create?",options:["Create a Private Service Connect endpoint in your VPC to access the partner's service privately","Use public internet connection","Set up VPN to partner's network","Private access to partner services is not possible"],correct:0,explanation:"Private Service Connect endpoints allow consuming services (from Google, partners, or other projects) via private IP addresses in your VPC. Traffic stays on Google's network without internet exposure. Provides secure, private connectivity to published services.",wrongExplanations:{1:"Public internet exposes traffic, increases latency, and creates security risks. Private Service Connect provides private connectivity.",2:"VPN requires coordination with partner and setup on both sides. Private Service Connect provides one-way private access without bilateral VPN setup when the service is PSC-published.",3:"This is false. Private Service Connect specifically enables private consumption of services published by other organizations or projects."}},{id:289,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Network Address Usage",question:"You're planning VPC subnet IP address ranges for a growing application. What should you consider?",options:["Plan subnet ranges with enough capacity for growth, avoid overlapping with other networks, and remember you can expand subnets later","Use smallest possible subnet ranges","Subnet size cannot be changed","IP planning is not necessary"],correct:0,explanation:"VPC subnets should be sized for current needs plus growth. Avoid overlaps with on-premises or other VPCs for peering/interconnect. Good news: subnets can be expanded (not shrunk) later, so start reasonable and expand as needed. Consider secondary ranges for GKE pods/services.",wrongExplanations:{1:"Smallest ranges may require frequent expansion or cause fragmentation. While expansion is possible, planning for growth reduces operational overhead.",2:"This is false. VPC subnets can be expanded (increasing CIDR range) without recreating the subnet or VMs. They cannot be shrunk, only expanded.",3:"IP planning prevents overlaps with other networks, ensures sufficient capacity, and enables VPC peering and interconnect. Poor planning causes issues later."}},{id:290,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Firewall Insights",question:"You want to identify unused or overly permissive firewall rules in your VPC. What tool should you use?",options:["Firewall Insights (part of Network Intelligence Center) to analyze firewall rule usage and identify shadowed, unused, or overly permissive rules","Manually review all rules","Delete all rules and recreate","Rule analysis is not available"],correct:0,explanation:"Firewall Insights analyzes firewall rules and provides insights about shadowed rules (overridden by higher priority), unused rules (no matching traffic), overly permissive rules, and rules that allow access from all IPs. Helps optimize security and reduce complexity.",wrongExplanations:{1:"Manual review of hundreds of rules is time-consuming and error-prone. Firewall Insights automates analysis and identifies issues.",2:"Deleting all rules causes service disruption. Firewall Insights identifies which rules can be safely removed or tightened.",3:"This is false. Firewall Insights specifically provides automated analysis of firewall rules for security optimization."}},{id:291,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Network Service Tiers",question:"You need to optimize costs for outbound internet traffic and can accept standard internet routing. What network service tier should you use?",options:["Standard Tier for lower-cost internet egress using standard routing","Premium Tier is always required","Network tiers don't affect cost","Service tiers are not available"],correct:0,explanation:"Network Service Tiers: Premium (default) routes traffic on Google's network globally for best performance. Standard routes using public internet, reducing egress costs. For cost-sensitive, less latency-critical workloads, Standard Tier reduces costs by 30-50% for egress.",wrongExplanations:{1:"This is false. Premium Tier provides best performance but higher cost. Standard Tier is available for cost-optimized egress at the expense of routing on public internet.",2:"This is false. Network Service Tiers (Premium vs Standard) directly impact egress pricing. Standard Tier costs 30-50% less for outbound internet traffic.",3:"This is false. Google Cloud offers Premium and Standard Network Service Tiers with different performance and cost characteristics."}},{id:292,domain:"Configuring access and security",subdomain:"4.2 Network security - Cloud IDS",question:"You need intrusion detection capabilities to identify network threats in your VPC traffic. What should you deploy?",options:["Cloud IDS (Intrusion Detection System) to detect malware, spyware, and command-and-control attacks","Firewall rules only","Cloud Armor only","Intrusion detection is not available"],correct:0,explanation:"Cloud IDS provides managed intrusion detection powered by Palo Alto Networks technology. It inspects traffic using Packet Mirroring, detects threats (malware, spyware, command-and-control), and integrates with Chronicle Security or SIEM tools for threat analysis.",wrongExplanations:{1:"Firewall rules block/allow based on IP, port, protocol but don't perform deep packet inspection for malware or exploits. Cloud IDS provides threat detection beyond basic filtering.",2:"Cloud Armor protects HTTP(S) applications from DDoS and web attacks but doesn't provide general network intrusion detection for all protocols. Cloud IDS monitors all traffic types.",3:"This is false. Cloud IDS specifically provides managed intrusion detection for VPC traffic."}},{id:293,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Bring your own IP",question:"Your organization has existing public IP addresses and wants to use them in Google Cloud for brand recognition and IP reputation. Is this possible?",options:["Yes, use Bring Your Own IP (BYOIP) to bring your own public IP addresses to GCP","No, you must use Google-provided IPs","BYOIP is only for private IPs","IP addresses cannot be transferred"],correct:0,explanation:"BYOIP allows bringing your own public IP address ranges (IPv4 /24 or larger) to Google Cloud. Useful for maintaining IP reputation, avoiding IP changes during migration, or meeting compliance requirements. Requires proof of ownership and ROA configuration.",wrongExplanations:{1:"This is false. BYOIP specifically enables using your own public IP addresses in GCP after verification and configuration.",2:"BYOIP is for public IP addresses. Private IPs are defined in your VPC subnets and don't require BYOIP since they're not globally routed.",3:"IP addresses can be transferred to GCP through BYOIP after proving ownership and configuring route origin authorization (ROA) with your RIR."}},{id:294,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Load balancer troubleshooting",question:"Your HTTP(S) Load Balancer is returning 502 Bad Gateway errors. What are likely causes to investigate?",options:["Backend instances are unhealthy, not responding, or responding with errors; check backend health and application logs","The load balancer is broken","Firewall rules are blocking frontend traffic","SSL certificate is expired"],correct:0,explanation:"502 errors indicate the load balancer received invalid responses from backends. Common causes: backends failing health checks, application crashes, backend timeouts, or backends returning errors. Check backend health status, application logs, and backend capacity.",wrongExplanations:{1:"Load balancers are highly reliable managed services. 502 errors typically indicate backend issues, not load balancer failure. Investigate backend health first.",2:"If firewall blocked frontend traffic, clients couldn't reach the load balancer (connection timeout). 502 errors mean load balancer received the request but backends responded improperly.",3:"Expired SSL certificates cause SSL/TLS errors, not 502. 502 specifically indicates bad responses from backends after successful SSL termination at the load balancer."}},{id:295,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Traffic Director",question:"You need advanced traffic management for microservices (traffic splitting, A/B testing) across GKE and Compute Engine with service mesh capabilities. What should you use?",options:["Traffic Director for service mesh traffic management across hybrid environments","Basic load balancers only","Manual traffic distribution","Advanced traffic management is not available"],correct:0,explanation:"Traffic Director provides service mesh traffic management including advanced routing, traffic splitting, A/B testing, and failover. Works with Envoy proxies across GKE, Compute Engine, and on-premises. Enables advanced microservices patterns without code changes.",wrongExplanations:{1:"Basic load balancers provide simple round-robin or weighted distribution but lack advanced routing rules, traffic splitting percentages, and service mesh capabilities.",2:"Manual traffic distribution requires code changes or complex configurations. Traffic Director provides declarative traffic management through service mesh.",3:"This is false. Traffic Director specifically provides advanced service mesh traffic management for microservices architectures."}},{id:296,domain:"Configuring access and security",subdomain:"4.2 Network security - Firewall rule logging",question:"You need to audit which firewall rules are allowing or denying traffic for compliance. What should you enable?",options:["Enable firewall rule logging on specific rules to log allowed or denied connections","Firewall logs are automatically enabled","Use VPC Flow Logs only","Firewall logging is not available"],correct:0,explanation:"Firewall rule logging can be enabled per rule to log connections allowed or denied by that rule. Logs include source/destination IPs, ports, protocol, and rule applied. Useful for auditing, troubleshooting, and compliance. Logs go to Cloud Logging.",wrongExplanations:{1:"Firewall rule logging must be explicitly enabled per rule. It's not automatic to avoid generating excessive logs for rules that don't require auditing.",2:"VPC Flow Logs show traffic patterns but don't indicate which firewall rule applied. Firewall rule logging specifically identifies the rule for each connection.",3:"This is false. Firewall rules support logging to track which rules allow or deny traffic for auditing and troubleshooting."}},{id:297,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - Network tags",question:"You need to apply firewall rules to a subset of instances without using IP addresses. What mechanism should you use?",options:["Use network tags to identify instances and apply firewall rules based on tags","List individual instance IPs in rules","Firewall rules apply to all instances only","Instance-specific rules are not possible"],correct:0,explanation:"Network tags are labels applied to instances that can be used in firewall rules as targets or source filters. This provides flexible, maintainable rules (e.g., 'allow SSH to instances tagged web-servers') without managing IP lists. Tags can be changed without modifying rules.",wrongExplanations:{1:"Listing individual IPs is brittle and doesn't scale. When instances change, IP-based rules must be updated. Tags provide dynamic, maintainable rule application.",2:"This is false. Firewall rules can target all instances, instances with specific tags, instances using specific service accounts, or instances in specific IP ranges.",3:"This is false. Network tags specifically enable applying firewall rules to specific subsets of instances without IP address management."}},{id:298,domain:"Ensuring successful operation of a cloud solution",subdomain:"3.3 Networking - Backend service health",question:"Your load balancer shows backends as unhealthy but the VMs are running. What should you investigate?",options:["Check health check configuration (protocol, port, path), verify application responds on health check endpoint, and check firewall rules allow health checker IPs","Restart all backends","Delete and recreate load balancer","Health check configuration cannot cause issues"],correct:0,explanation:"Common causes of unhealthy backends: health check configured for wrong port/path, application not responding on health endpoint, firewall blocking health checker IP ranges (35.191.0.0/16, 130.211.0.0/22), or application requiring too long to respond. Systematic troubleshooting identifies the issue.",wrongExplanations:{1:"Restarting backends doesn't fix configuration issues like wrong health check port or blocked health checker IPs. Investigate health check settings first.",2:"Deleting and recreating doesn't fix misconfigurations. The same issues would persist. Troubleshoot health check configuration systematically.",3:"This is false. Misconfigured health checks are a common cause of unhealthy backend status. Port, path, protocol, and firewall rules all affect health check success."}},{id:299,domain:"Planning and implementing a cloud solution",subdomain:"2.6 Advanced networking - IPv6 support",question:"Your application needs to support IPv6 clients. Can you configure IPv6 in Google Cloud VPCs?",options:["Yes, VPCs support IPv6 with dual-stack subnets (both IPv4 and IPv6) and IPv6 load balancing","No, only IPv4 is supported","IPv6 is only for external IPs","IPv6 requires separate VPCs"],correct:0,explanation:"GCP supports IPv6 with dual-stack VPC subnets (both IPv4 and IPv6 addresses). Global HTTP(S) Load Balancer supports IPv6 clients. VMs can have both IPv4 and IPv6 addresses. This enables serving IPv6-only clients while maintaining IPv4 compatibility.",wrongExplanations:{1:"This is false. GCP supports IPv6 in VPCs with dual-stack configuration, allowing both IPv4 and IPv6 communication.",2:"IPv6 is supported for both external (internet-facing) and internal (VPC) addressing in dual-stack configuration.",3:"VPCs support dual-stack (IPv4 + IPv6) within the same VPC. Separate VPCs are not required for IPv6 support."}},{id:300,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"A junior developer is struggling to write a complex `gcloud` command to create a custom Compute Engine instance with specific network tags and a startup script. They need to do this quickly without searching through extensive documentation. Which Google Cloud tool should they use to get a context-aware, executable `gcloud` command by describing their goal in natural language?",options:["Gemini Cloud Assist","Cloud Shell Editor","Google Cloud documentation search","Stack Overflow"],correct:0,explanation:"Gemini Cloud Assist is an AI-powered assistant integrated into the Google Cloud console that can generate configurations, code, and commands based on natural language prompts. It is the most direct and efficient tool for this specific task.",wrongExplanations:{1:"Cloud Shell Editor is an IDE, but it doesn't generate commands from natural language. The user would still need to know the command syntax.",2:"Searching the documentation is a valid but slower approach compared to using the AI assistant designed for this purpose.",3:"While Stack Overflow is a useful resource, it's external to the Google Cloud environment and may not provide a command tailored to the user's specific project context as Gemini can."}},{id:301,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Organization policies",question:"Your company's security team wants to enforce a policy that prevents any new VM instances from being created with public IP addresses across the entire Google Cloud organization, except for a specific project used for public-facing web servers. How can you implement this requirement in the most efficient way?",options:["Set an Organization Policy with the `compute.vmExternalIpAccess` constraint at the organization root, and disable it for the specific project.","Create a custom IAM role that denies the `compute.instances.addAccessConfig` permission and apply it to all users except those in the specific project.","Write a script that runs hourly to check for and delete VMs with public IPs in non-approved projects.","Configure VPC firewall rules in each project to deny egress traffic, except for the specific project."],correct:0,explanation:"Organization Policies are the standard, hierarchical way to enforce constraints on cloud resources. Applying the `compute.vmExternalIpAccess` constraint at the organization level and then modifying it for the specific project (by overriding the policy) is the most scalable and correct approach.",wrongExplanations:{1:"Managing this through IAM is cumbersome, error-prone, and doesn't prevent a user with sufficient privileges from adding a public IP. Organization policies provide a direct guardrail.",2:"A reactive script is inefficient and creates a time window where non-compliant resources can exist. A preventative control like an Organization Policy is the best practice.",3:"Firewall rules control traffic, not the configuration of the VM itself. A VM could still be created with a public IP, even if traffic is blocked."}},{id:302,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE Autopilot",question:"A development team wants to deploy a new containerized microservice on Google Kubernetes Engine (GKE). They want to focus solely on their application code and not manage the underlying nodes, scaling, or resource allocation. Their highest priority is operational efficiency and reduced management overhead. Which GKE mode of operation should they choose?",options:["Autopilot","Standard","Regional Cluster","Private Cluster"],correct:0,explanation:"GKE Autopilot is a fully managed mode of operation where Google manages the cluster's control plane, nodes, and resources. You only define your workloads (pods, deployments) and pay for the resources they consume, aligning perfectly with the team's requirement to eliminate node management.",wrongExplanations:{1:"GKE Standard requires you to manage your own node pools, including selecting machine types, configuring scaling, and performing upgrades. This introduces significant management overhead.",2:"'Regional Cluster' describes a high-availability feature available in both Standard and Autopilot modes, but it is not a mode of operation itself.",3:"'Private Cluster' describes a security feature for network isolation available in both modes, not a mode of operation."}},{id:303,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud NGFW",question:"A network administrator needs to implement a granular firewall policy for a complex VPC. They want to apply rules based on application identity (e.g., 'frontend-web-servers') rather than static IP addresses, to simplify management as services scale and IPs change. Which feature of Cloud NGFW (Next-Generation Firewall) should they use to achieve this?",options:["Secure tags and network firewall policies","Hierarchical firewall policies","VPC firewall rules with IP ranges","Identity-Aware Proxy (IAP)"],correct:0,explanation:"Cloud NGFW introduces secure tags, which are distinct from network tags. Secure tags are IAM-controlled attributes that can be attached to resources like VMs. Network firewall policies can then use these tags to define rules, allowing for identity-based microsegmentation independent of IP addresses.",wrongExplanations:{1:"Hierarchical firewall policies apply rules at the Organization/Folder level but don't inherently provide application-identity based rules like secure tags do.",2:"Using traditional VPC firewall rules with IP ranges is precisely the static, difficult-to-manage approach the administrator wants to avoid.",3:"IAP is used to control access to applications based on user identity (e.g., a specific Google user), not for defining network-layer traffic rules between services within a VPC."}},{id:304,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"A database administrator notices that a critical application using Cloud SQL for PostgreSQL is experiencing performance degradation. They suspect inefficient queries are the cause but need to identify the exact queries and users responsible without installing third-party tools. Which tool should they use?",options:["Query Insights","Cloud SQL logs in Cloud Logging","Cloud Monitoring metrics for Cloud SQL","Cloud Profiler"],correct:0,explanation:"Query Insights is a built-in tool for Cloud SQL that provides visual, detailed diagnostics for query performance. It helps you detect, diagnose, and prevent query performance problems for Cloud SQL databases by showing query load, problematic queries, and where the query load is originating from (user, IP address).",wrongExplanations:{1:"While Cloud SQL logs contain information, they are not designed for easy performance analysis and lack the visual dashboards and aggregated metrics of Query Insights.",2:"Cloud Monitoring provides high-level metrics like CPU and memory usage, but it does not give insight into the performance of individual SQL queries.",3:"Cloud Profiler is used for analyzing application code performance (CPU/memory profiling of the application itself), not for diagnosing database query performance."}},{id:305,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You need to grant a new DevOps engineer the ability to view almost all resources in a project and manage Compute Engine instances, but you must prevent them from modifying any IAM policies. Which combination of predefined roles should you assign?",options:["Viewer and Compute Admin","Project Owner","Editor and Security Reviewer","Compute Admin and Network Viewer"],correct:0,explanation:"The Viewer (`roles/viewer`) role provides read-only access to most Google Cloud resources. The Compute Admin (`roles/compute.admin`) role provides full control over Compute Engine resources. This combination meets the requirements without granting broad modification permissions on other services or IAM policies.",wrongExplanations:{1:"The Project Owner role is too permissive; it grants full control over all resources, including the ability to modify IAM policies.",2:"The Editor role is too broad, allowing modification of most resources, not just Compute Engine. Security Reviewer is a read-only role for security policies, but the combination with Editor is overly permissive.",3:"This combination is too restrictive. It only provides access to Compute Engine and networking resources, failing to meet the requirement of viewing almost all resources in the project."}},{id:306,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Database Center",question:"A fleet manager is responsible for over 50 Cloud SQL, Bigtable, and Spanner instances across multiple projects. They need a centralized dashboard to get a high-level overview of the health, performance, and security posture of their entire database fleet without navigating to each individual instance. Which Google Cloud feature provides this capability?",options:["Database Center","Cloud Monitoring Dashboards","The Cloud SQL instances page","Security Command Center"],correct:0,explanation:"Database Center is a centralized dashboard designed to provide a comprehensive overview of your entire database fleet. It consolidates performance metrics, security findings, and operational health information for various database types into a single pane of glass.",wrongExplanations:{1:"While you could build a custom Cloud Monitoring Dashboard, it would require significant effort to replicate the specialized, out-of-the-box fleet-level insights provided by Database Center.",2:"The Cloud SQL instances page only shows Cloud SQL instances and lacks the fleet-wide, multi-service overview that includes Bigtable and Spanner.",3:"Security Command Center focuses on security vulnerabilities and threats, not on operational health and performance metrics for databases, which is a key feature of Database Center."}},{id:307,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"Your company is migrating a large, mission-critical PostgreSQL database to Google Cloud. They require full PostgreSQL compatibility, high availability with a 99.99% SLA, and performance that is significantly better than standard open-source PostgreSQL. Which database service is the best fit?",options:["AlloyDB for PostgreSQL","Cloud SQL for PostgreSQL","Cloud Spanner with PostgreSQL interface","PostgreSQL on a Compute Engine VM"],correct:0,explanation:"AlloyDB for PostgreSQL is a fully managed, PostgreSQL-compatible database service designed for high-performance, high-availability workloads. It offers superior performance compared to standard PostgreSQL and a 99.99% availability SLA, making it the ideal choice for this demanding scenario.",wrongExplanations:{1:"Cloud SQL for PostgreSQL is a great choice for general-purpose PostgreSQL workloads but may not meet the 'significantly better' performance requirement. Its SLA is typically 99.95% for regional instances.",2:"Cloud Spanner is a globally distributed database. While it has a PostgreSQL interface, it's not fully wire-compatible and is designed for different use cases (global scale, horizontal scalability) than a traditional lift-and-shift of a PostgreSQL application.",3:"Running PostgreSQL on a VM provides full control but shifts all management responsibility (HA, backups, patching) to the user and does not inherently provide the required performance or SLA without complex custom architecture."}},{id:308,domain:"Section 4: Configuring access and security",subdomain:"Cloud NGFW policies",question:"A security admin is using Cloud NGFW with secure tags to control traffic. They have a VM tagged with `role=frontend` and a database tagged with `role=backend`. They need to create a rule that allows TCP port 5432 traffic from any frontend server to any backend server within the VPC. What should be the source and destination in the network firewall policy rule?",options:["Source: Tag `role=frontend`, Destination: Tag `role=backend`","Source: Network tag `frontend`, Destination: Network tag `backend`","Source: IP range of frontend VMs, Destination: IP range of backend VMs","Source: Service Account of frontend VMs, Destination: Service Account of backend VMs"],correct:0,explanation:"Network firewall policies in Cloud NGFW are designed to use secure tags for defining source and destination. This allows for an identity-based security posture where rules are not tied to ephemeral IP addresses, which is the core benefit of this feature.",wrongExplanations:{1:"Network tags are a legacy feature and are not used in the more advanced network firewall policies. The question specifically mentions secure tags, which are different.",2:"Using IP ranges is the static method that secure tags are designed to replace. It's less flexible and harder to manage.",3:"While firewall rules can use service accounts, secure tags are the more modern and flexible Cloud NGFW feature for creating granular, resource-identity-based policies."}},{id:309,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You are setting up an alerting policy in Cloud Monitoring for a critical Compute Engine instance group. You want to be notified if the average CPU utilization across the group exceeds 80% for a continuous period of 10 minutes. How should you configure the alerting policy condition?",options:["Metric: CPU utilization, Aggregation: mean, Condition: is above 80%, Duration: 10 minutes","Metric: CPU utilization, Aggregation: max, Condition: is above 80%, Duration: 10 minutes","Metric: CPU load (1m), Aggregation: mean, Condition: is above 0.80, Duration: 10 minutes","Create a Log-based metric for CPU utilization and alert when its count exceeds a threshold."],correct:0,explanation:"To monitor the average CPU utilization for a group, you should select the 'CPU utilization' metric, aggregate it across the group using 'mean', set the condition to trigger 'is above' a threshold of 80%, and set the duration for the condition to be met to '10 minutes'. This configuration precisely matches the requirement.",wrongExplanations:{1:"Using 'max' for aggregation would trigger an alert if any single instance spikes above 80%, not when the average of the group does.",2:"CPU load is a different metric than CPU utilization. While related, utilization is the direct measure of the percentage of CPU being used.",3:"Log-based metrics are for analyzing log entries. CPU utilization is a standard system metric available directly in Cloud Monitoring and does not require creating metrics from logs."}},{id:310,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"Your finance team needs to receive an automated notification when a specific development project's spending is projected to exceed its monthly budget of $500. You must implement this with minimal effort. What should you do?",options:["Create a budget in Cloud Billing for the project with a 100% projected spend alert.","Write a Cloud Function that is triggered daily to analyze billing data from BigQuery and send an email.","Set up a Cloud Monitoring alert on the 'Total Cost' metric for the project.","Manually export the billing report each week and check the project's spend."],correct:0,explanation:"Cloud Billing budgets are the native, simplest way to monitor and control costs. You can set a budget amount and configure alert thresholds based on actual or projected spend, which directly addresses the requirement.",wrongExplanations:{1:"This is overly complex. While possible, it requires setting up a billing export to BigQuery and writing custom code, which is not minimal effort.",2:"Cloud Monitoring is for performance metrics, not for sophisticated budget alerting based on projections. Billing budgets are the specific tool for this purpose.",3:"Manual checking is inefficient, error-prone, and not automated, failing to meet a key requirement."}},{id:311,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You are deploying a batch processing workload that is highly fault-tolerant and can be interrupted. The job runs for several hours. To minimize costs, which type of Compute Engine VM should you use?",options:["Spot VM","Preemptible VM","Standard VM","E2-series VM"],correct:0,explanation:"Spot VMs are the latest generation of interruptible instances, offering the largest discounts (60-91% off standard prices). They are the ideal choice for fault-tolerant, cost-sensitive workloads that can handle interruptions, as they provide the best cost savings.",wrongExplanations:{1:"Preemptible VMs are the older generation of interruptible instances. While they also offer discounts, Spot VMs are the newer, recommended option with generally better pricing and features.",2:"Standard VMs offer no discounts and are not the most cost-effective choice for an interruptible workload.",3:"E2-series VMs are a cost-effective machine series, but they are not specifically designed for interruptible workloads and do not offer the steep discounts of Spot VMs."}},{id:312,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"Your company has a strict data residency policy that requires all audit logs generated in a specific European project to be stored in a Cloud Storage bucket located in the EU for long-term retention. How can you automate this process?",options:["Create a log sink at the project level with a Cloud Storage bucket in the EU as the destination.","Write a script to run `gcloud logging read` and pipe the output to a file in an EU bucket.","Configure the default log bucket's location to be in the EU.","Set up a Pub/Sub topic to receive logs and a Dataflow job to write them to the bucket."],correct:0,explanation:"Log sinks are the standard mechanism in Cloud Logging for routing log entries to supported destinations. Creating a sink at the project level allows you to filter for specific logs (like audit logs) and send them to a Cloud Storage bucket, Pub/Sub topic, or BigQuery dataset, respecting the destination's location.",wrongExplanations:{1:"A manual script is not a reliable or scalable automation solution. Log sinks are the native, managed way to handle this.",2:"You cannot change the location of the default `_Default` and `_Required` log buckets. You must create a sink to route logs elsewhere.",3:"Using Pub/Sub and Dataflow is a valid but overly complex and expensive solution for a simple log routing and storage requirement. A direct sink is much more efficient."}},{id:313,domain:"Section 4: Configuring access and security",subdomain:"Service Accounts",question:"An application running on a Compute Engine VM needs to read files from a Cloud Storage bucket. Following the principle of least privilege, what is the most secure way to grant this access?",options:["Attach a service account with the 'Storage Object Viewer' role to the VM instance.","Generate a service account key, store it on the VM, and use it to authenticate.","Grant the 'Storage Object Viewer' role to the 'allUsers' special identifier on the bucket.","Assign the 'Editor' role to the VM's service account."],correct:0,explanation:"The best practice is to attach a service account with the minimal necessary permissions (in this case, `roles/storage.objectViewer`) directly to the Compute Engine instance. This allows the application to use the built-in metadata server to get credentials automatically, avoiding the security risk of managing and storing service account keys.",wrongExplanations:{1:"Storing service account keys on a VM is a security risk. If the VM is compromised, the key can be stolen and used from anywhere. This practice should be avoided whenever possible.",2:"Granting access to 'allUsers' would make the bucket public, which is a major security violation.",3:"The 'Editor' role is overly permissive. It grants read and write access to most project resources, which violates the principle of least privilege."}},{id:314,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"VPC Networking",question:"You are designing the network for a new application. You have two main services, a web frontend and a database backend, that will run in different subnets. You need to ensure the web frontend can communicate with the database on port 3306, but no other traffic is allowed to the database from the web subnet. What should you do?",options:["Create a VPC firewall rule with a network tag for the database, allowing ingress on TCP:3306 from the web frontend's network tag.","Create a Cloud NAT gateway to allow traffic between the subnets.","Configure Private Google Access for the database subnet.","Establish a VPC Peering connection between the two subnets."],correct:0,explanation:"VPC firewall rules are the standard way to control traffic between subnets and instances within a VPC. Using network tags allows you to apply the rule specifically to the database VMs (target tag) and allow traffic only from the web VMs (source tag) on the specified port, enforcing the principle of least privilege.",wrongExplanations:{1:"Cloud NAT is for allowing instances without external IPs to access the internet; it does not control internal traffic between subnets.",2:"Private Google Access allows instances without external IPs to reach Google APIs; it does not control internal traffic between subnets.",3:"VPC Peering is for connecting two separate VPCs. Since the subnets are in the same VPC, peering is not necessary."}},{id:315,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"A web application running on a managed instance group (MIG) is experiencing sudden traffic spikes. You need to ensure the group scales up quickly to handle the load and scales down to save costs when the traffic subsides. The scaling decisions should be based on the number of active user connections. Which autoscaling policy should you configure?",options:["Cloud Load Balancing utilization, with a capacity setting based on requests per second.","CPU utilization, with a target utilization of 60%.","A schedule-based autoscaling policy to increase capacity during peak hours.","A queue-based scaling policy using a Cloud Monitoring metric from Pub/Sub."],correct:0,explanation:"For a web application behind a load balancer, the most effective scaling signal is the load balancer's own utilization metric, specifically requests per second (RPS). This directly measures the application's load and allows the MIG to scale based on actual user traffic, which is more accurate than indirect metrics like CPU.",wrongExplanations:{1:"CPU utilization can be a good signal, but it's indirect. A high number of connections might not always correlate with high CPU, especially if requests are I/O-bound. Scaling on RPS is more direct.",2:"Schedule-based scaling is only appropriate for predictable traffic patterns, not for handling sudden, unpredictable spikes.",3:"Queue-based scaling is designed for backend processing workloads that pull tasks from a queue like Pub/Sub, not for a user-facing web application."}},{id:316,domain:"Section 4: Configuring access and security",subdomain:"Encryption",question:"Your company has a strict compliance requirement that all data at rest in a Cloud Storage bucket must be encrypted with a key that your company manages directly. Google should not have access to the unencrypted keys. Which encryption option should you use for the bucket?",options:["Customer-Supplied Encryption Keys (CSEK)","Customer-Managed Encryption Keys (CMEK) with Cloud KMS","Google-managed encryption","Application-layer encryption"],correct:0,explanation:"CSEK requires you to provide your own encryption key with every request to Cloud Storage. Google only holds the key in memory temporarily to perform the operation and does not store it. This gives you full control and ensures Google cannot access the data without the key you supply, meeting the strict requirement.",wrongExplanations:{1:"With CMEK, you manage the key in Cloud KMS, but Google's services still interact with KMS on your behalf to encrypt/decrypt data. While you control the key's lifecycle, Google's systems are involved in the process, which doesn't meet the 'Google should not have access' criteria as strictly as CSEK.",2:"Google-managed encryption is the default, where Google fully manages the keys. This does not meet the customer-managed requirement.",3:"Application-layer encryption is a valid strategy but refers to encrypting data within your application before sending it to Google. The question asks about Cloud Storage encryption options, and CSEK is the direct answer within that context."}},{id:317,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"A large enterprise is structuring their Google Cloud environment. They have multiple departments (e.g., Finance, Engineering, Marketing), each with its own set of applications and projects. They need to apply distinct IAM policies and network configurations at the department level. What is the recommended way to structure this using the resource hierarchy?",options:["Create a Folder for each department under the Organization node, and place each department's Projects inside their respective Folder.","Create a separate Organization node for each department.","Use labels on Projects to identify which department they belong to and apply policies based on labels.","Create a single Project for the entire company and use VPCs to isolate department resources."],correct:0,explanation:"Folders are the intended mechanism in the resource hierarchy for grouping projects that share common requirements, such as those belonging to a single department. You can apply IAM policies and Organization Policies to a Folder, and they will be inherited by all Projects within it, making it the ideal tool for departmental isolation and governance.",wrongExplanations:{1:"An Organization node is typically tied to a single company domain (e.g., mycompany.com). Creating multiple organizations is not the standard or recommended practice for departmental separation.",2:"Labels are for metadata, cost tracking, and filtering, but they are not a primary mechanism for enforcing hierarchical policy inheritance. Folders are designed for this purpose.",3:"A single project for a large enterprise is not scalable and does not provide sufficient isolation for billing, quotas, or security boundaries. This would be an administrative nightmare."}},{id:318,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"BigQuery",question:"You are designing a large BigQuery table that will store daily sales transaction data and will be queried frequently by region and transaction date. To optimize query performance and reduce costs, how should you configure the table?",options:["Partition the table by transaction date and cluster it by region.","Create a separate table for each day's transactions.","Cluster the table by both region and transaction date.","Use a wildcard query to select from multiple daily tables."],correct:0,explanation:"Partitioning by transaction date will create daily partitions. When queries filter on this date (a common pattern), BigQuery only scans the relevant partitions, drastically reducing the amount of data processed and thus lowering costs and improving speed. Clustering by region will then sort the data within each partition by region, further improving performance for queries that filter on region.",wrongExplanations:{1:"Creating thousands of individual tables is difficult to manage and query. Partitioning is the BigQuery-native solution to this problem.",2:"You can only cluster on up to four columns, but partitioning is a separate and more powerful mechanism for date-based pruning. The combination of partitioning and clustering is optimal here.",3:"Wildcard queries are a way to deal with the anti-pattern of having many small tables. Using a partitioned table is the correct and more performant design."}},{id:319,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Trace",question:"Your microservices-based application is experiencing high latency on certain user requests, but you cannot pinpoint which service is the bottleneck. The request flows through multiple services (e.g., Frontend -> API Gateway -> Order Service -> Database). Which tool would be most effective for visualizing the entire request flow and identifying the latency contribution of each service?",options:["Cloud Trace","Cloud Logging","Cloud Profiler","Cloud Monitoring"],correct:0,explanation:"Cloud Trace is specifically designed for distributed tracing. It collects latency data from your applications, tracks how requests propagate through different services, and displays it in a near real-time waterfall graph. This allows you to immediately see which service call is taking the most time and causing the bottleneck.",wrongExplanations:{1:"Cloud Logging can show you logs from each service, but it's very difficult to manually correlate timestamps across multiple services to diagnose a latency issue. Trace automates this.",2:"Cloud Profiler is used to analyze the code-level performance (CPU, memory) *within a single service*. It can't show you the end-to-end latency across multiple services.",3:"Cloud Monitoring can show you high-level latency metrics for each service, but it doesn't provide the detailed, request-level trace needed to understand the entire flow and pinpoint the exact source of delay."}},{id:320,domain:"Section 4: Configuring access and security",subdomain:"VPC Security",question:"You have a set of Compute Engine instances in a private subnet that need to download software updates from the internet. However, company policy forbids these instances from having public IP addresses, so they cannot be directly reached from the internet. How can you enable this outbound internet access securely?",options:["Configure a Cloud NAT gateway for the private subnet.","Set up Identity-Aware Proxy (IAP) for the instances.","Assign a public IP address to each instance temporarily.","Create a firewall rule to allow all egress traffic from the instances."],correct:0,explanation:"Cloud NAT is a managed service that allows instances in a private subnet (without external IPs) to initiate outbound connections to the internet, but prevents unsolicited inbound connections. This is the standard and most secure solution for providing internet access for patching, updates, or API calls from private resources.",wrongExplanations:{1:"IAP is for controlling *inbound* SSH/RDP or TCP traffic to your instances based on user identity. It does not provide general-purpose outbound internet access.",2:"Assigning public IPs violates the core security requirement that the instances should not be directly reachable from the internet.",3:"An egress firewall rule only permits traffic; it does not solve the networking problem of how a private IP can be routed to the public internet. You still need a NAT or a public IP for that."}},{id:321,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You need to store monthly backups of a database in Cloud Storage. The data is critical and must be retained for seven years for compliance. It will be accessed very rarely, likely only in a disaster recovery scenario. Which Cloud Storage class offers the lowest storage cost for this use case?",options:["Archive","Coldline","Nearline","Standard"],correct:0,explanation:"Archive Storage is designed for long-term data archiving, backup, and disaster recovery. It offers the absolute lowest at-rest storage cost but has higher retrieval costs and longer retrieval times compared to other classes. This aligns perfectly with the requirement for long-term, rarely accessed data.",wrongExplanations:{1:"Coldline Storage is for data accessed at most once a year. While it's a good option for backups, Archive is even cheaper for data that will be stored for many years with minimal expected access.",2:"Nearline Storage is for data accessed at most once a month. This is too frequent and more expensive than Coldline or Archive for this long-term backup scenario.",3:"Standard Storage is for frequently accessed ('hot') data and has the highest storage cost, making it completely unsuitable for this use case."}},{id:322,domain:"Section 1: Setting up a cloud solution environment",subdomain:"API enablement",question:"A developer in your team tries to run a `gcloud compute instances create` command in a newly created project but receives a permission denied error, indicating that the 'Compute Engine API' has not been enabled. What is the most direct way to resolve this issue?",options:["Enable the Compute Engine API in the Google Cloud Console under 'APIs & Services'.","Grant the developer the 'Project Owner' IAM role.","Run `gcloud services enable compute.googleapis.com`.","Wait for 60 minutes for the API to be enabled automatically."],correct:2,explanation:"The `gcloud services enable` command is the command-line equivalent of enabling an API in the console. It directly targets and enables the specified API (`compute.googleapis.com`) for the project, which is the root cause of the error. Both this and enabling it via the Console are correct, but this is a common exam-style question testing gcloud knowledge.",wrongExplanations:{0:"This is also a correct way to solve the problem, but exam questions often favor the `gcloud` command-line solution when available.",1:"Changing IAM roles will not fix the issue. Even a Project Owner cannot use a service if its API is not enabled for the project.",3:"APIs are not enabled automatically. They must be explicitly enabled before their resources can be provisioned or managed."}},{id:323,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Error Reporting",question:"An application deployed on Cloud Run is experiencing intermittent crashes. You want to automatically capture, group, and get notified about these application exceptions without instrumenting your code with a new library. Which Google Cloud's operations suite tool should you use?",options:["Cloud Error Reporting","Cloud Logging with a log-based metric","Cloud Monitoring dashboards","Cloud Debugger"],correct:0,explanation:"Cloud Error Reporting is designed specifically for this purpose. It automatically collects and aggregates crash and exception data from services like Cloud Run, App Engine, and Cloud Functions. It intelligently groups similar errors and can send notifications, allowing you to quickly identify and address new issues.",wrongExplanations:{1:"You could try to parse stack traces in Cloud Logging and create alerts, but it's a complex, manual process. Error Reporting does this automatically and more effectively.",2:"Monitoring dashboards show metrics like request count or latency, but they don't provide details about application-level exceptions or stack traces.",3:"Cloud Debugger is for interactively inspecting the state of a running application at a specific line of code. It's for active debugging, not for automated crash reporting and aggregation."}},{id:324,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Load balancing",question:"You are deploying a global web application with users in North America, Europe, and Asia. You need to provide a single, global IP address for all users and route them to the closest healthy backend instance group to minimize latency. Which Google Cloud Load Balancer should you choose?",options:["Global external Application Load Balancer","Regional external Application Load Balancer","Global external proxy Network Load Balancer","Internal TCP/UDP Load Balancer"],correct:0,explanation:"The Global external Application Load Balancer (formerly known as HTTP(S) Load Balancer) is designed for this exact use case. It provides a single global Anycast IP address and uses Google's global network to route user traffic to the nearest backend with available capacity, ensuring the lowest possible latency and high availability.",wrongExplanations:{1:"A Regional load balancer only distributes traffic to backends within a single Google Cloud region. It cannot route traffic globally.",2:"A proxy Network Load Balancer is for non-HTTP(S) traffic like TCP/SSL. For a web application (HTTP/S), the Application Load Balancer is the appropriate choice as it operates at Layer 7.",3:"An Internal load balancer is for traffic *within* your VPC and cannot be used for public, internet-facing applications."}},{id:325,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"A third-party auditing firm needs temporary, read-only access to review the configurations of all resources in one of your projects for a compliance check. The access should automatically expire after two weeks. How should you grant this access according to Google Cloud best practices?",options:["Grant the auditor's user account the Viewer role with an IAM Condition that expires after 14 days.","Create a new user account for the auditor, give it the Viewer role, and manually delete it after two weeks.","Give the auditor the Project Owner role so they don't encounter any permissions issues.","Export all resource configurations to a Cloud Storage bucket and grant the auditor access to the bucket."],correct:0,explanation:"IAM Conditions allow you to grant temporary and conditional access to resources. You can add a time-based condition to a role binding that specifies a start and end date. This is the most secure and automated way to provide temporary access that is automatically revoked, following the principle of least privilege.",wrongExplanations:{1:"Manual deletion is error-prone. An administrator might forget to delete the account, leaving a security vulnerability. IAM Conditions automate the revocation.",2:"The Project Owner role is extremely permissive and violates the principle of least privilege. An auditor only needs read-only (Viewer) access.",3:"Exporting data creates a static, point-in-time copy. The auditor needs to review the live configurations. This approach is also cumbersome and less secure."}},{id:326,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Run and Cloud Functions",question:"You need to build a lightweight service that responds to file uploads in a Cloud Storage bucket. Whenever a new image is uploaded, the service must resize it and save a thumbnail to another bucket. The processing for each image is quick and independent. Which compute service is the most cost-effective and operationally efficient for this task?",options:["Cloud Functions","Cloud Run","A GKE Autopilot cluster","A Compute Engine VM"],correct:0,explanation:"Cloud Functions is an event-driven, serverless compute platform. It is designed to execute code in response to events, such as a file upload to Cloud Storage. Since the task is small, quick, and triggered by an event, Cloud Functions is the most efficient and cost-effective choice, as you only pay for the execution time and don't manage any infrastructure.",wrongExplanations:{1:"Cloud Run is also serverless, but it is designed for running containerized applications that typically serve HTTP requests. While it *can* be configured to respond to events, Cloud Functions is a more direct and simpler solution for this specific event-driven use case.",2:"Using GKE, even Autopilot, is overkill for this simple task. It would be more expensive and complex than necessary.",3:"A Compute Engine VM requires you to manage the operating system, patching, and scaling. It is the least operationally efficient and cost-effective choice for a simple, event-driven task."}},{id:327,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Backup strategies",question:"You are managing a critical Cloud SQL for MySQL database. You need to be able to restore the database to its state at any specific minute within the last 7 days. What features must you enable on the instance?",options:["Automated backups and point-in-time recovery (binary logging)","Only automated backups","Manual backups and binary logging","Read replicas in another zone"],correct:0,explanation:"Point-in-time recovery (PITR) is the feature that allows you to restore a database to a specific moment. To enable PITR in Cloud SQL, you must first have automated backups enabled, and you must also enable binary logging. The combination of the last full backup and the subsequent binary logs allows Cloud SQL to reconstruct the database to any point in time.",wrongExplanations:{1:"Automated backups alone only allow you to restore to the specific time the backup was taken (e.g., 4:00 AM), not to any minute in between.",2:"Manual backups are point-in-time snapshots, but they do not support the continuous logging needed for PITR. PITR must be used with automated backups.",3:"Read replicas are for high availability and read-scaling, not for backup and restore. They replicate the current state of the primary, not historical states."}},{id:328,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You frequently work with two different Google Cloud projects: `project-dev` and `project-prod`. You want to be able to switch the default project for your `gcloud` commands easily without typing the full `--project` flag every time. Which `gcloud` command should you use?",options:["gcloud config configurations create/activate","gcloud projects set-default","gcloud init","gcloud auth login"],correct:0,explanation:"`gcloud config configurations` allows you to create named configuration sets. You can create one for `dev` (with `project-dev` as the default project) and one for `prod`. Then, you can easily switch between them using `gcloud config configurations activate [NAME]`. This is the most robust way to manage settings for multiple distinct projects or accounts.",wrongExplanations:{1:"This command does not exist. The correct command is `gcloud config set project [PROJECT_ID]`.",2:"`gcloud init` is used for initializing or re-initializing gcloud settings, which includes setting up a configuration. However, `gcloud config configurations` is the specific command set for managing and switching between multiple existing configurations.",3:"`gcloud auth login` is for authenticating your user account with Google Cloud. It does not manage project settings."}},{id:329,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Hybrid connectivity",question:"Your company needs to establish a highly reliable, low-latency, high-bandwidth connection between your on-premises data center and your Google Cloud VPC. The connection will carry business-critical traffic and requires a 99.99% availability SLA. Which connectivity option should you choose?",options:["Dedicated Interconnect","Partner Interconnect","Cloud VPN (HA)","Direct Peering"],correct:0,explanation:"Dedicated Interconnect provides a direct, private physical connection between your on-premises network and Google's network. When configured for high availability (two connections in different locations), it can provide a 99.99% SLA. It offers the highest bandwidth and lowest latency, making it suitable for business-critical traffic.",wrongExplanations:{1:"Partner Interconnect also provides a private connection but through a service provider. While it can also offer a high SLA (up to 99.99%), Dedicated Interconnect is the option that provides a direct physical link solely for your company.",2:"Cloud VPN (HA) provides a secure connection over the public internet. It is a great option for many use cases, but it cannot match the low latency and high, consistent bandwidth of a dedicated physical connection. Its SLA is typically 99.9%.",3:"Direct Peering is for exchanging traffic between your network and Google's public-facing properties (like YouTube, Search), not for connecting to your private VPC resources."}},{id:330,domain:"Section 4: Configuring access and security",subdomain:"Organization policies",question:"To prevent accidental data exfiltration, your security team wants to ensure that all new Cloud Storage buckets are created with Uniform bucket-level access enabled and do not use legacy Access Control Lists (ACLs). How can you enforce this across the entire organization?",options:["Set an Organization Policy with the `storage.uniformBucketLevelAccess` constraint enforced.","Create a custom IAM role that denies the `storage.buckets.setIamPolicy` permission for buckets without uniform access.","Run a daily script to find and convert buckets that are not using uniform bucket-level access.","Educate all developers on the new policy and trust them to comply."],correct:0,explanation:"Organization Policies provide centralized, programmatic control over your organization's cloud resources. The `storage.uniformBucketLevelAccess` constraint is specifically designed to enforce the use of uniform bucket-level access, which simplifies permissions and disables ACLs. Applying this at the organization root is the most effective way to enforce the policy.",wrongExplanations:{1:"Managing this through IAM is complex and indirect. An Organization Policy is a direct, preventative control.",2:"A reactive script is not a preventative measure. It only fixes non-compliant resources after they have been created.",3:"Relying on education alone is not a sufficient security control for a critical policy. An automated enforcement mechanism is required."}},{id:331,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You are managing a GKE Standard cluster and need to upgrade the Kubernetes version on your node pools with minimal disruption to running applications. You want to upgrade nodes one by one, moving workloads to a new node before draining the old one. Which upgrade strategy should you use?",options:["Surge upgrades","Blue-green upgrades","Canary upgrades","In-place upgrades"],correct:0,explanation:"Surge upgrades are the default and recommended strategy for GKE node pools. This strategy creates a new node with the updated version (the 'surge' node) before draining an old node. Once the old node is drained and its workloads are rescheduled, it is deleted. This rolling process minimizes disruption and ensures capacity is maintained during the upgrade.",wrongExplanations:{1:"Blue-green upgrades involve creating an entire parallel node pool with the new version, shifting traffic over, and then deleting the old pool. While a valid strategy, surge upgrades are the built-in GKE mechanism for rolling node pool updates.",2:"Canary upgrades are a deployment strategy for applications, not for underlying node infrastructure.",3:"In-place upgrades are not a standard GKE feature. The standard process involves replacing old nodes with new ones."}},{id:332,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE configurations",question:"You are deploying a critical application in GKE that must remain available even if an entire Google Cloud zone fails. Which type of GKE cluster should you create?",options:["A regional cluster","A zonal cluster","A private cluster","An Autopilot cluster"],correct:0,explanation:"A regional cluster distributes its control plane and nodes across multiple zones within a single region. If one zone fails, the control plane remains available, and workloads can continue running in the other zones, providing high availability against zonal failures.",wrongExplanations:{1:"A zonal cluster has its control plane and all its nodes located within a single zone. If that zone fails, the entire cluster becomes unavailable.",2:"A private cluster is a security feature that gives nodes private IPs; it does not determine the cluster's availability across zones.",3:"An Autopilot cluster can be either zonal or regional. The choice of regional is what provides the high availability, not the Autopilot mode itself."}},{id:333,domain:"Section 4: Configuring access and security",subdomain:"Cloud Audit Logs",question:"A security auditor needs to see a log of all administrative changes made to your Google Cloud project, such as creating a VM or modifying an IAM policy. They also need to know which user performed each action. Which log type in Cloud Audit Logs should you provide?",options:["Admin Activity audit logs","Data Access audit logs","System Event audit logs","Policy Denied audit logs"],correct:0,explanation:"Admin Activity audit logs record all API calls and other actions that modify the configuration or metadata of resources. They are enabled by default and cannot be disabled. They include the 'who, what, when, and where' for all administrative actions, which is exactly what the auditor requires.",wrongExplanations:{1:"Data Access audit logs record who accessed data (e.g., reading a file from a bucket). They are high-volume and disabled by default. The auditor is asking for administrative changes, not data access.",2:"System Event audit logs record actions taken by Google Cloud systems, not by user accounts.",3:"Policy Denied audit logs record when a user or service was denied access because of a security policy. While useful, they don't show the successful administrative changes."}},{id:334,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Quotas",question:"You are about to launch a new application that will require 200 vCPUs in a specific region. When you try to provision the VMs, you receive an error that you have exceeded your quota. The project is new, and this is a legitimate business need. What should you do?",options:["Request a quota increase for the vCPU quota in that region from the IAM & Admin Quotas page.","Create a new project, as it will have a fresh set of default quotas.","Spread the VMs across multiple regions to stay within the default quota for each.","Use smaller machine types to reduce the total vCPU count."],correct:0,explanation:"Quotas are limits on the amount of a particular resource you can use. They are a safety mechanism. When you have a valid need to exceed a default quota, the standard procedure is to navigate to the Quotas page in the console and submit a request to increase the specific quota for your project.",wrongExplanations:{1:"Creating a new project will not solve the problem, as it will have the same low default quota. The limit is per project.",2:"Spreading VMs across regions might be a temporary workaround, but it adds network latency and complexity. The correct solution is to get the necessary quota in the region where you need the resources.",3:"Using smaller machine types might compromise the performance of your application. If you need 200 vCPUs, you should request the appropriate quota rather than downgrading your architecture."}},{id:335,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Pub/Sub",question:"You are designing an application where a frontend service needs to send tasks to multiple, independent backend services for processing. Each task must be processed by exactly one of the backend services. The system needs to be highly scalable and decoupled. Which service should you use to facilitate this communication?",options:["Cloud Pub/Sub","Cloud Tasks","Cloud Load Balancing","Cloud Memorystore"],correct:0,explanation:"Cloud Pub/Sub is a fully-managed, real-time messaging service that allows you to send and receive messages between independent applications. By having the frontend publish a message to a topic and having the backend services subscribe to that topic, you create a decoupled, many-to-many communication system that is highly scalable.",wrongExplanations:{1:"Cloud Tasks is designed for asynchronous task execution, but it's more focused on guaranteed delivery and rate controls for specific webhook targets. Pub/Sub is better for fanning out messages to multiple, independent subscriber groups.",2:"Cloud Load Balancing is for distributing user-facing traffic to a set of identical backends; it is not a messaging service for backend decoupling.",3:"Cloud Memorystore is an in-memory database service (Redis/Memcached); it is not a messaging service."}},{id:336,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Serial console access",question:"A Linux Compute Engine instance has failed to boot correctly, and you cannot connect to it using SSH because the networking service has not started. You need to interact with the instance to diagnose the boot issue. Which method should you use to gain access?",options:["Connect to the serial console of the instance.","View the instance's screenshot.","Mount the instance's boot disk to another VM.","Check the instance's logs in Cloud Logging."],correct:0,explanation:"The serial console provides direct, low-level access to a VM instance, independent of its network state. It allows you to interact with the bootloader and operating system as if you had a physical monitor and keyboard connected, which is essential for diagnosing boot-time failures before SSH is available.",wrongExplanations:{1:"Viewing the screenshot can show you where the boot process is stuck, but it does not provide an interactive console to run diagnostic commands.",2:"Mounting the disk to another VM is a valid but more complex and disruptive troubleshooting step. Accessing the serial console is the first and most direct method.",3:"While logs might contain useful information, they may not capture the specific bootloader errors, and they do not provide an interactive way to fix the problem."}},{id:337,domain:"Section 4: Configuring access and security",subdomain:"Identity-Aware Proxy (IAP)",question:"You have an internal web application running on a Compute Engine instance. You want to allow employees to access it from the internet without using a VPN, but you must ensure that only authenticated users from your company's domain can reach it. What is the most secure and managed way to achieve this?",options:["Use Identity-Aware Proxy (IAP) with the external Application Load Balancer.","Restrict access to the application using a firewall rule that allows traffic only from your corporate IP range.","Install an SSH bastion host and have users tunnel to the application.","Assign a public IP to the instance and configure application-level authentication."],correct:0,explanation:"IAP is a service that uses user identity and context to guard access to applications. When placed in front of an application via a load balancer, it intercepts all requests and requires users to authenticate with their Google identity (e.g., their corporate Google Workspace account). This allows you to enforce fine-grained access policies without a VPN, following a zero-trust security model.",wrongExplanations:{1:"Restricting by IP is not a complete solution. It doesn't work for remote employees not on the corporate network, and it doesn't verify individual user identity.",2:"An SSH bastion host is a common pattern but is less user-friendly and harder to manage for web applications than the seamless experience provided by IAP.",3:"Exposing the instance directly with a public IP and relying solely on application-level authentication is less secure. IAP provides a robust, managed authentication and authorization layer before traffic even reaches your application."}},{id:338,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Dataflow",question:"Your company collects a continuous stream of sensor data into a Pub/Sub topic. You need to process this data in real-time: filter out invalid readings, enrich the data by calling an external API, and write the results to a BigQuery table for analysis. The solution must scale automatically based on the volume of incoming data. Which service is best suited for this task?",options:["Cloud Dataflow","A script running on a Compute Engine VM","Cloud Functions","BigQuery scheduled queries"],correct:0,explanation:"Cloud Dataflow is a fully managed service for stream and batch data processing. It is built on Apache Beam and is designed for complex data processing pipelines (ETL) like this one. It can read from Pub/Sub, perform transformations like filtering and enrichment, and write to BigQuery, all while scaling its worker resources automatically.",wrongExplanations:{1:"A script on a VM is not a managed or auto-scaling solution. You would be responsible for all reliability and scalability aspects.",2:"Cloud Functions could be used for simple transformations, but for a multi-stage pipeline with external API calls and potential for large volume, Dataflow provides a more robust and scalable framework.",3:"BigQuery scheduled queries are for running SQL against data already in BigQuery on a schedule (batch processing), not for real-time stream processing from Pub/Sub."}},{id:339,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Profiler",question:"An application running on GKE is consuming an unexpectedly high amount of CPU, and you need to determine which specific functions within the code are responsible for the high usage. The application is written in Go. Which tool should you use to analyze the code's performance in production with low overhead?",options:["Cloud Profiler","Cloud Trace","Cloud Monitoring","Cloud Debugger"],correct:0,explanation:"Cloud Profiler is a low-overhead production profiler that continuously analyzes CPU usage and memory allocation of your application's code. It presents the data in a flame graph, allowing you to quickly identify the most resource-intensive functions and code paths, which is exactly what is needed to diagnose this issue.",wrongExplanations:{1:"Cloud Trace is for analyzing request latency across distributed systems, not for analyzing CPU usage within a single application's code.",2:"Cloud Monitoring tracks high-level metrics like overall CPU usage for a container or node, but it cannot tell you which specific function inside your code is responsible.",3:"Cloud Debugger allows you to inspect application state without stopping it, but it is not a performance analysis tool for identifying CPU-intensive functions."}},{id:340,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Config Connector",question:"A platform engineering team manages all their application deployments on GKE using a GitOps workflow with declarative YAML files. They want to extend this workflow to manage Google Cloud infrastructure, such as Cloud SQL instances and IAM policy bindings, using the same tools (`kubectl`, Git). Which Google Cloud component should they install in their GKE cluster to enable this?",options:["Config Connector","Terraform","Cloud Deployment Manager","Cloud Build"],correct:0,explanation:"Config Connector is a Kubernetes addon that allows you to manage Google Cloud resources through Kubernetes-style resource definitions (CRDs). Once installed, you can create, update, and delete GCP resources like `SQLInstance` or `IAMPolicyMember` by applying YAML manifests to your cluster, integrating seamlessly with existing GitOps and Kubernetes tooling.",wrongExplanations:{1:"Terraform is an excellent infrastructure-as-code tool, but it is a separate binary and workflow from `kubectl`. Config Connector is the solution that integrates directly into the Kubernetes control plane.",2:"Cloud Deployment Manager is Google's native infrastructure deployment service, but it uses its own template format and is not integrated with the Kubernetes resource model.",3:"Cloud Build is a CI/CD service used for building and testing code, not for declaratively managing infrastructure from within a Kubernetes cluster."}},{id:341,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Database Center",question:"The Database Center dashboard shows a high 'CPU utilization' warning for a specific Cloud SQL instance. What is the most direct next step you can take from within the Google Cloud Console to diagnose which specific queries are causing the high CPU load?",options:["Click on the instance in Database Center to navigate directly to Query Insights.","Navigate to Cloud Monitoring and build a custom dashboard for the instance.","SSH into a VM and connect to the database to run `SHOW PROCESSLIST`.","Check the Admin Activity logs for the instance."],correct:0,explanation:"Database Center is designed as a centralized launchpad for database observability. It integrates tightly with other tools, and a primary workflow is to identify a problem (like high CPU) in the fleet-level view and then drill down into the specific instance's Query Insights page for detailed, query-level performance analysis.",wrongExplanations:{1:"While Cloud Monitoring has CPU metrics, it won't show you the individual queries. Query Insights is the specific tool for that level of detail.",2:"Manually connecting and running commands is an older, less efficient method of diagnosis. Query Insights provides historical data and a much richer user interface for identifying problematic queries.",3:"Admin Activity logs track administrative changes (e.g., changing instance settings), not query performance or CPU load."}},{id:342,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"You are new to Google Cloud and need to set up a two-tier web application with a load-balanced managed instance group and a Cloud SQL database. You understand the high-level architecture but are unsure of the specific steps and `gcloud` commands required. How can you use Gemini Cloud Assist to help you?",options:["Ask Gemini in the console sidebar: 'Show me the steps to create a load balancer with a MIG and a Cloud SQL backend'.","Use Gemini to write a Terraform script for the entire architecture.","Open Cloud Shell and wait for Gemini to suggest commands automatically.","Ask Gemini to troubleshoot why your application code is not working."],correct:0,explanation:"Gemini Cloud Assist excels at providing architectural guidance and generating the necessary commands. By describing your goal in natural language, Gemini can provide a step-by-step plan and the corresponding `gcloud` commands or code snippets to implement the desired architecture, significantly accelerating the learning and implementation process.",wrongExplanations:{1:"While Gemini can help write Terraform code, its primary integration in the console is for generating explanations and CLI commands, which is more direct for a beginner.",2:"Gemini does not automatically suggest commands in Cloud Shell without being prompted. You must actively ask it for help.",3:"Gemini assists with cloud infrastructure and operations. While it might offer general coding advice, it is not designed for debugging application-specific logic."}},{id:343,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud NGFW",question:"Your organization is migrating from traditional VPC firewall rules to network firewall policies using Cloud NGFW. You need to create a rule that blocks all outbound traffic to the internet (0.0.0.0/0) on TCP port 25 (SMTP) for all VMs in a project to prevent spam, but still allow all other internal VPC traffic. Where should this rule be configured?",options:["In a network firewall policy with a low priority (higher number) that applies to the project's VPC.","In a hierarchical firewall policy at the folder level.","As a VPC firewall rule with a priority of 65535.","In Cloud Armor as a security policy."],correct:0,explanation:"Network firewall policies are the next generation of VPC firewalls and are the correct place for this type of rule. A deny rule for a specific port to an external IP range should be given a lower priority (a higher number, e.g., 1000 or more) so that it doesn't override more specific allow rules for internal traffic, which would have a higher priority (lower number).",wrongExplanations:{1:"While a hierarchical policy could enforce this, network firewall policies are designed for VPC-specific rules. Hierarchical policies are better for broader, organization-wide guardrails.",2:"Legacy VPC firewall rules are being superseded by network firewall policies. While you could create this rule, the question implies a migration to the newer Cloud NGFW feature set.",3:"Cloud Armor is a Web Application Firewall (WAF) that protects against application-layer attacks (like SQL injection) on services behind an external load balancer. It does not control general egress traffic from VMs."}},{id:344,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"Query Insights shows that a specific query is consuming a large amount of CPU time and has a high 'I/O Wait' time. The query plan indicates a full table scan is being performed on a very large table. What is the most likely solution to improve this query's performance?",options:["Add an appropriate database index on the columns used in the `WHERE` clause of the query.","Increase the vCPU count of the Cloud SQL instance.","Migrate the database to BigQuery.","Rewrite the query to use a different `JOIN` type."],correct:0,explanation:"A full table scan combined with high I/O wait is a classic indicator of a missing index. Adding an index allows the database to seek directly to the relevant rows instead of reading the entire table from disk, which drastically reduces I/O and CPU consumption for the query.",wrongExplanations:{1:"Increasing the vCPU count (vertical scaling) might provide temporary relief but it does not fix the root cause of the inefficient query. The query will still perform a full table scan, just faster, and it's a very expensive solution.",2:"Migrating to BigQuery is a massive architectural change. BigQuery is an analytical data warehouse, not a transactional database, and this would not be an appropriate or simple solution.",3:"Changing the `JOIN` type would only be relevant if the query involved multiple tables. The core problem identified is a full table scan, which is addressed by indexing."}},{id:345,domain:"Section 4: Configuring access and security",subdomain:"VPC Service Controls",question:"Your company stores highly sensitive data in a BigQuery dataset. To prevent data exfiltration, you must ensure that this data can only be accessed by specific authorized VMs running within your VPC, and that the BigQuery API cannot be called from the public internet. Which security control should you implement?",options:["VPC Service Controls","An organization policy to restrict public IPs","IAM Conditions based on IP address","A VPC firewall rule to deny all egress traffic"],correct:0,explanation:"VPC Service Controls create a service perimeter around Google-managed services like BigQuery and Cloud Storage. This perimeter blocks access from outside the perimeter (e.g., the public internet) and prevents data from being exfiltrated to a location outside the perimeter, even by an identity with valid IAM permissions. It is the definitive tool for this use case.",wrongExplanations:{1:"Restricting public IPs on VMs prevents them from being reached from the internet, but it does not prevent a malicious actor on a VM from exfiltrating data by calling the public BigQuery API endpoint.",2:"IAM Conditions can restrict access by IP, but this is less secure and harder to manage than a service perimeter, as it doesn't control where the data can be sent.",3:"Denying all egress traffic from your VPC would break many legitimate functionalities, including the ability for VMs to call necessary Google APIs. VPC Service Controls provide a much more granular and effective boundary."}},{id:346,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Identity",question:"Your company uses Google Workspace for email and collaboration. You want to use the same user identities and groups to manage IAM permissions in Google Cloud. What is the most straightforward way to achieve this?",options:["Use your existing Google Workspace account as your Cloud Identity source. The users and groups will be available automatically.","Export all users from Google Workspace and import them into a new Cloud Identity domain.","Set up federation with your on-premises Active Directory to sync users to Google Cloud.","Create a new service account for each user in Google Workspace."],correct:0,explanation:"When you have a Google Workspace account, it can act as your Cloud Identity provider by default. This means all your existing users and groups are automatically available in Google Cloud for you to assign IAM roles to, providing a seamless identity management experience.",wrongExplanations:{1:"Exporting and importing users is an unnecessary and error-prone manual step. The integration is designed to be automatic.",2:"Federating with Active Directory would be the solution if your primary identity source was on-premises AD, not Google Workspace.",3:"Service accounts are for applications, not for human users. This approach is incorrect and unmanageable."}},{id:347,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You are hosting a static website on Cloud Storage. You have enabled versioning on the bucket to protect against accidental deletions. A user accidentally deletes the `index.html` file. How can you restore the most recent version of the file?",options:["Show noncurrent versions in the Cloud Console, find the previous version of `index.html`, and restore it.","The file is permanently gone because it was deleted.","Restore the entire bucket from the previous day's backup.","Enable object holds on the bucket and then try to undelete the file."],correct:0,explanation:"When object versioning is enabled, deleting an object simply creates a special 'delete marker' and makes the live version noncurrent. It is not actually deleted. You can view these noncurrent versions and choose to restore any of them, which makes it the new live version.",wrongExplanations:{1:"This is incorrect. The purpose of versioning is to prevent permanent data loss from deletions or overwrites.",2:"Cloud Storage does not have automatic daily backups. You would need to configure this yourself. Versioning is the direct feature for this use case.",3:"Object holds are a compliance feature to prevent deletion. They must be set *before* the deletion attempt and would not help recover an already deleted object."}},{id:348,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Storage",question:"You have a Cloud Storage bucket with a lifecycle policy that moves objects to Nearline storage after 30 days and deletes them after 365 days. You have a specific object, `legal-document.pdf`, that must be preserved indefinitely for legal reasons. How can you prevent this one object from being deleted by the lifecycle policy?",options:["Place an object hold on `legal-document.pdf`.","Create a new lifecycle rule that excludes `legal-document.pdf`.","Move the object to Archive storage class.","Copy the object to a different bucket without a lifecycle policy."],correct:0,explanation:"An object hold is a feature that prevents an object version from being deleted or overwritten, regardless of any bucket lifecycle policies. This is the intended mechanism for protecting individual objects for legal or compliance reasons.",wrongExplanations:{1:"Lifecycle rules can be based on prefixes, age, or storage class, but not on individual object names. It's not feasible to exclude a single file this way.",2:"Moving the object to Archive would still subject it to the 365-day deletion rule. The lifecycle policy applies to all objects in the bucket regardless of class.",3:"While copying the object would work, it's a manual workaround. The object hold is the built-in, correct feature to use within the same bucket."}},{id:349,domain:"Section 4: Configuring access and security",subdomain:"Security best practices",question:"You are reviewing your company's Google Cloud project and discover several developers have created and downloaded service account keys to their local machines. According to Google Cloud security best practices, what should you advise them to do instead?",options:["Advise them to use short-lived credentials by impersonating a service account with their user credentials.","Advise them to store the keys in a private Git repository.","Advise them to rotate their service account keys every 90 days.","Advise them to encrypt the keys on their local disk."],correct:0,explanation:"Long-lived static service account keys are a significant security risk. The best practice is to avoid them entirely. Service account impersonation allows a user with sufficient permissions to generate short-lived credentials for a service account, which automatically expire. This eliminates the risk of a leaked key.",wrongExplanations:{1:"Storing any credentials, even encrypted, in a source code repository is a major security anti-pattern.",2:"While key rotation is better than not rotating, the best practice is to eliminate the long-lived keys altogether.",3:"Encrypting the keys is a good practice, but it doesn't mitigate the risk if the decryption key is also compromised or if the key is accidentally exposed while decrypted in memory."}},{id:350,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Organization policies",question:"Your company wants to ensure that all new projects created within the organization automatically have a default set of essential APIs enabled, such as Compute Engine, Cloud Storage, and BigQuery. How can you automate this?",options:["There is no direct way to automate this; it must be done manually or with a custom script after project creation.","Use the `gcloud projects create` command with a flag to enable APIs.","Configure an Organization Policy to enforce API enablement.","Create a project template in Deployment Manager."],correct:0,explanation:"Currently, Google Cloud's Organization Policies and resource hierarchy do not have a built-in feature to automatically enable a specific set of APIs upon project creation. This process must be orchestrated using other tools, typically a script that runs after a project is created, often triggered by a log entry about the project creation event.",wrongExplanations:{1:"The `gcloud projects create` command does not have a flag to enable multiple APIs during the creation process.",2:"Organization Policies can be used to *restrict* API usage (e.g., deny access to a certain service), but not to *enable* them by default.",3:"Deployment Manager is for deploying resources *within* a project, not for configuring the project itself during creation."}},{id:351,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You need to run a legacy monolithic application on a Compute Engine VM. The application is single-threaded and performs best with the highest possible CPU clock speed. Which machine series should you choose to get the best performance?",options:["C3 or C3D (Performance-optimized)","E2 (Cost-optimized)","N2 (General-purpose)","M2 (Memory-optimized)"],correct:0,explanation:"The C-series (Compute-optimized) instances are designed for compute-intensive workloads and typically offer the highest per-core performance and clock speeds. For a single-threaded application that is CPU-bound, a compute-optimized instance will provide the best results.",wrongExplanations:{1:"E2 instances are cost-optimized and do not offer the highest performance. They are suitable for general-purpose workloads where cost is the primary concern.",2:"N2 instances are general-purpose and offer a good balance of price and performance, but the C-series is specifically optimized for maximum CPU performance.",3:"M2 instances are memory-optimized and provide a very high ratio of memory to vCPU. This would be wasteful and not cost-effective for a CPU-bound application."}},{id:352,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You want to create a single chart in a Cloud Monitoring dashboard that shows the average CPU utilization of all VMs in your project that have a specific label, for example, `app: my-critical-app`. How can you achieve this?",options:["Create a chart, select the CPU utilization metric, and then filter by the label `app: my-critical-app`.","Create a separate chart for each VM and arrange them on the dashboard.","Write a custom script using the Monitoring API to generate the chart data.","This is not possible; you can only filter by instance name or group."],correct:0,explanation:"Cloud Monitoring fully supports filtering and grouping metrics by resource labels. This is a core feature that allows you to create aggregated views of your resources. You can easily add a widget, select the metric, and apply a filter based on any label you have assigned to your VMs.",wrongExplanations:{1:"This is inefficient and doesn't provide the single aggregated view that was requested.",2:"This is overly complex. The functionality is built directly into the Monitoring UI.",3:"This is incorrect. Filtering by labels is a standard and powerful feature of Cloud Monitoring."}},{id:353,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You have a group of junior developers who need to deploy applications to a GKE cluster. They should be able to manage Deployments, Pods, and Services, but should not be able to view or modify the cluster's configuration itself (e.g., change the node pool size). Which role should you grant them?",options:["Kubernetes Engine Developer (`roles/container.developer`)","Kubernetes Engine Admin (`roles/container.admin`)","Project Editor (`roles/editor`)","Compute Viewer (`roles/compute.viewer`)"],correct:0,explanation:"The Kubernetes Engine Developer role provides the necessary permissions to interact with Kubernetes objects within a cluster (like Pods and Deployments) via `kubectl`, but it does not grant permissions to modify the underlying GKE cluster infrastructure. This adheres to the principle of least privilege.",wrongExplanations:{1:"The Kubernetes Engine Admin role is too permissive; it grants full control over the GKE cluster, including the ability to modify its configuration and delete it.",2:"The Project Editor role is far too broad. It grants permissions to modify almost all resources in the project, not just the Kubernetes objects.",3:"The Compute Viewer role is too restrictive. It only allows viewing of Compute Engine resources and does not grant any permissions to interact with GKE."}},{id:354,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud DNS",question:"You have a domain `example.com` registered with a third-party registrar. You want to manage its DNS records using Cloud DNS. What are the high-level steps you need to take?",options:["Create a managed public zone in Cloud DNS for `example.com`, then update the name server (NS) records at your registrar to point to the Cloud DNS name servers.","Transfer the domain registration from your current registrar to Google Domains.","Create a managed private zone in Cloud DNS for `example.com`.","Point your domain's A record at your registrar to a Google Cloud IP address."],correct:0,explanation:"To delegate DNS management to Cloud DNS, you first create a zone for your domain in the Cloud DNS service. Cloud DNS will then assign a set of authoritative name servers for that zone. The final step is to go to your domain registrar and replace the existing name server records with the ones provided by Cloud DNS. This tells the internet to query Cloud DNS for records related to `example.com`.",wrongExplanations:{1:"While you can transfer the domain to Google Domains, it is not a requirement for using Cloud DNS. You can use any registrar.",2:"A private zone is for resolving names *within* your VPCs and is not visible to the public internet. For a public website, you need a public zone.",3:"Simply pointing an A record does not delegate the entire management of the domain's DNS records to Cloud DNS."}},{id:355,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"A startup script on a Linux VM is failing intermittently. You need to view the full output of the startup script to diagnose the problem. Where can you find this output?",options:["In the instance's serial port 1 output.","In the Admin Activity audit log.","In the VM's metadata server.","In the `/var/log/syslog` file on the instance."],correct:0,explanation:"By default, the output of startup scripts (both stdout and stderr) is written to the instance's serial port console (specifically port 1). You can view this output in the Google Cloud Console or by using the `gcloud compute get-serial-port-output` command. This is the most reliable place to look for startup script logs.",wrongExplanations:{1:"Audit logs track API calls and administrative actions, not the output of scripts running inside a VM.",2:"The metadata server provides information *to* the VM; it does not store output *from* the VM.",3:"While the script's output *might* also be directed to syslog depending on the OS configuration, the serial port is the standard, guaranteed location provided by the Compute Engine platform."}},{id:356,domain:"Section 4: Configuring access and security",subdomain:"Cloud Armor",question:"Your e-commerce website, which is behind a global external Application Load Balancer, is experiencing a DDoS attack in the form of a large volume of HTTP requests from a specific country. How can you quickly mitigate this attack?",options:["Create a Cloud Armor security policy with a rule to deny traffic from that specific country and attach it to the load balancer's backend service.","Add a VPC firewall rule to deny ingress traffic from the country's IP ranges.","Use Identity-Aware Proxy to block the country.","Scale up your managed instance group to handle the extra traffic."],correct:0,explanation:"Cloud Armor is Google's DDoS protection and Web Application Firewall (WAF) service. It integrates with the global external Application Load Balancer and allows you to create rules to allow or deny traffic based on various attributes, including IP addresses, regions, and request headers. A geo-based block rule is a standard and effective way to mitigate this type of attack.",wrongExplanations:{1:"VPC firewall rules operate at the VM instance level. They cannot be attached to a global load balancer and would not be an effective way to stop a large-scale DDoS attack at the edge of Google's network.",2:"IAP is for user identity-based authentication, not for blocking large-scale, anonymous DDoS traffic.",3:"Scaling up might keep your site online temporarily, but it doesn't stop the attack and will result in extremely high costs. The best practice is to block the malicious traffic at the edge."}},{id:357,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"You are building a mobile gaming application with a global user base. You need a database that can handle high volumes of reads and writes, provide strong consistency, and offer low-latency access for users in multiple continents. The database must scale horizontally. Which Google Cloud database is the best choice?",options:["Cloud Spanner","Cloud SQL","Firestore","BigQuery"],correct:0,explanation:"Cloud Spanner is a globally distributed, strongly consistent, and horizontally scalable relational database. It is uniquely designed for use cases that require both global scale and strong transactional consistency, making it the perfect fit for a high-traffic global application like a mobile game.",wrongExplanations:{1:"Cloud SQL is a regional database. While you can use read replicas in other regions, it does not provide the same low-latency writes and horizontal scalability on a global scale as Spanner.",2:"Firestore is a NoSQL document database. While it offers multi-region support, Spanner is the better choice when strong, relational consistency is required.",3:"BigQuery is an analytical data warehouse, not a transactional database suitable for powering a live application backend."}},{id:358,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You need to get a detailed description of a specific Compute Engine instance named `web-server-1` in the zone `us-central1-a`, including its network interfaces and attached disks. You want the output to be in YAML format for easy parsing by a script. Which command should you use?",options:["gcloud compute instances describe web-server-1 --zone us-central1-a --format=yaml",'gcloud compute instances list --filter="name=web-server-1" --format=yaml',"gcloud compute instances get-details web-server-1 --zone us-central1-a","gcloud compute instances export web-server-1 --destination=config.yaml"],correct:0,explanation:"The `gcloud compute instances describe` command is used to get detailed information about a single resource. The `--format` flag is a global `gcloud` flag that allows you to specify the output format, with `yaml`, `json`, and `text` being common options. This combination directly fulfills the request.",wrongExplanations:{1:"The `list` command is for listing multiple resources. While you could filter it down to one, `describe` is the more direct command for a single resource. Also, the output of `list` is a list containing one item, whereas `describe` returns just the item itself.",2:"`get-details` is not a valid `gcloud compute instances` subcommand.",3:"`export` is not a valid `gcloud compute instances` subcommand for this purpose."}},{id:359,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You need to find all log entries with a `severity` of `ERROR` that occurred in your production project within the last 24 hours. How would you write this query in the Logs Explorer?",options:["severity=ERROR","level:error","SELECT * WHERE severity = 'ERROR'","filter by severity ERROR"],correct:0,explanation:"The Cloud Logging query language uses a simple `[FIELD_NAME]=[VALUE]` syntax for basic equality filters. To filter for error-level severity, you would simply type `severity=ERROR` into the query bar. The time range can be selected using the time range picker in the UI.",wrongExplanations:{1:"This syntax is not correct. The field name is `severity`.",2:"While the query language is powerful, it does not use SQL syntax like `SELECT *`.",3:"This is a natural language description, not a valid query for the query language."}},{id:360,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Config Connector",question:"After installing Config Connector in your GKE cluster, you create a YAML manifest for a `PubSubTopic` resource. What is the next step to actually create the Pub/Sub topic in your Google Cloud project?",options:["Apply the YAML manifest to your GKE cluster using `kubectl apply -f [filename]`. ","Run `gcloud pubsub topics create` with the YAML file as an argument.","Submit the manifest to Cloud Deployment Manager.","Check the manifest into a Git repository connected to Cloud Build."],correct:0,explanation:"Config Connector works by extending the Kubernetes API. You manage Google Cloud resources just like you manage native Kubernetes objects like Pods or Deployments. The standard workflow is to define the resource in a YAML file and then use `kubectl apply` to submit it to the Kubernetes API server. Config Connector's controllers will then see this new resource and provision the actual Pub/Sub topic in GCP.",wrongExplanations:{1:"`gcloud` commands are separate from the Config Connector workflow, which is based entirely on the Kubernetes API (`kubectl`).",2:"Cloud Deployment Manager is a different Infrastructure-as-Code tool and does not understand Kubernetes-style manifests.",3:"While checking the manifest into Git is part of a GitOps workflow, the action that directly creates the resource is the `kubectl apply`."}},{id:361,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"You are viewing the logs for a crashing Cloud Run service in the Logs Explorer and see a complex Java stack trace. You are not a Java expert. How can Gemini Cloud Assist help you understand the problem?",options:["Click the Gemini icon next to the log entry to get a natural language explanation of the error.","Ask Gemini to rewrite the Cloud Run service in a different language.","Ask Gemini to decompile the Java bytecode.","Gemini cannot help with application-level errors."],correct:0,explanation:"A key feature of Gemini Cloud Assist is its integration into services like Cloud Logging. It can analyze complex log entries, such as stack traces, and provide a concise, natural language summary of what the error means, what might have caused it, and suggest potential fixes. This is extremely valuable for troubleshooting errors in unfamiliar code or platforms.",wrongExplanations:{1:"Rewriting the service is a massive step and not a troubleshooting action. Gemini's purpose here is to explain the current error.",2:"Decompiling bytecode is not a relevant or helpful action for understanding a stack trace.",3:"This is incorrect. Explaining application errors found in logs is a core use case for Gemini Cloud Assist."}},{id:362,domain:"Section 4: Configuring access and security",subdomain:"Cloud NGFW policies",question:"What is a key advantage of using secure tags with network firewall policies over traditional network tags with VPC firewall rules?",options:["Secure tags are managed with IAM permissions, allowing for delegated security administration.","Secure tags can be applied to any resource, including Cloud Storage buckets.","Secure tags work across different VPCs without needing VPC Peering.","Secure tags are automatically applied to all VMs in a project."],correct:0,explanation:"A major difference is the governance model. Traditional network tags can be added to a VM by anyone with the `compute.instances.setTags` permission (part of Compute Instance Admin). Secure tags, however, are separate resources, and the ability to attach a specific tag to a resource is controlled by a distinct IAM permission (`tags.tagBindings.create`). This allows a central security team to control who can apply which tags, enabling a more secure, delegated administration model.",wrongExplanations:{1:"Secure tags can only be applied to Compute Engine VMs at this time.",2:"Network firewall policies, like VPC firewall rules, apply only within a single VPC network.",3:"Secure tags must be manually or programmatically applied to resources; they are not automatic."}},{id:363,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Database Center",question:"From the main Database Center dashboard, which of the following insights can you see at a glance for your entire database fleet?",options:["The total number of databases with high CPU utilization, and a list of databases with available security recommendations.","A real-time stream of all queries being executed across all databases.","The exact cost incurred by each database instance in the last hour.","A button to failover all high-availability database instances simultaneously."],correct:0,explanation:"Database Center is designed to provide a high-level, fleet-wide overview. It aggregates key performance metrics (like identifying which instances have high CPU) and security posture information (like highlighting instances that have security recommendations from services like Security Command Center) into a single dashboard.",wrongExplanations:{1:"Database Center provides aggregated metrics, not a real-time firehose of every query. You would use Query Insights for detailed query analysis on a per-instance basis.",2:"Cost information is found in Cloud Billing, not directly in the Database Center dashboard.",3:"Database Center is an observability tool, not a control plane for performing operational actions like failovers across the entire fleet."}},{id:364,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE Autopilot",question:"A team is deploying a standard web application on a GKE Autopilot cluster. They have provided a Deployment YAML file with resource requests (CPU and memory) for their pods. What is GKE Autopilot responsible for?",options:["Provisioning, managing, and scaling the underlying nodes to meet the pod's resource requests.","Automatically determining the correct CPU and memory requests for the pods.","Ensuring the application code inside the container has no bugs.","Creating the DNS records for the application's service."],correct:0,explanation:"The core value proposition of GKE Autopilot is that it abstracts away node management. You declare the resources your application pods need, and Autopilot handles the rest of the infrastructure lifecycle: creating appropriately sized nodes, adding new nodes as you scale up, removing nodes as you scale down, and performing node upgrades.",wrongExplanations:{1:"You are still responsible for specifying resource requests for your pods. Autopilot uses this information to provision the correct infrastructure. It does not determine the requests for you.",2:"GKE is a container orchestrator; it is not responsible for the quality of the application code itself.",3:"While GKE can create internal DNS records for service discovery, you are responsible for creating public-facing DNS records."}},{id:365,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"You have configured a billing export to a BigQuery dataset. What can you do with this data?",options:["Write detailed SQL queries to analyze costs by label, SKU, and project.","Modify past billing records to correct for accidental spending.","Use it to provision new resources directly from BigQuery.","Receive real-time alerts within milliseconds of a cost being incurred."],correct:0,explanation:"Exporting detailed billing data to BigQuery unlocks powerful, granular cost analysis capabilities. You can use standard SQL to query the data, allowing you to slice and dice your spending by any dimension present in the data, such as project, service, SKU, and any labels you've applied to your resources. This is essential for deep cost optimization.",wrongExplanations:{1:"The billing export is a read-only record of charges. You cannot modify it.",2:"BigQuery is an analytical data warehouse. You cannot use it to directly provision cloud resources.",3:"The billing export is updated periodically (a few times a day), not in real-time. For real-time alerting, you would use Billing budgets."}},{id:366,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"You are using Query Insights for a Cloud SQL for MySQL instance. What does the 'Query Load' chart primarily show you?",options:["The total active time of all queries, broken down by CPU, I/O, and Lock Wait.","The number of queries executed per second.","The financial cost of each query.","The amount of data returned by each query."],correct:0,explanation:"The primary chart in Query Insights shows the total query load, measured in 'CPU seconds'. It further breaks down this load into the main database wait states: CPU (active processing), I/O Wait (waiting for disk), and Lock Wait (waiting for a database lock). This allows you to see at a glance not just that the database is busy, but *why* it's busy.",wrongExplanations:{1:"While related, the query load is a more comprehensive metric than just queries per second. A single, slow query can generate more load than thousands of fast ones.",2:"Query Insights is a performance tool, not a cost analysis tool. It does not show the financial cost of queries.",3:"The amount of data returned is not the primary metric for load. A query can process terabytes of data but return only a single number, generating very high load."}},{id:367,domain:"Section 4: Configuring access and security",subdomain:"Service Accounts",question:"A Cloud Function needs to write an object to a Cloud Storage bucket. What is the recommended way to grant it the necessary permissions?",options:["Assign a service account with the `Storage Object Creator` role to the Cloud Function during deployment.","Use the default App Engine service account, which has the Editor role on the project.","Hardcode a user's credentials within the function's code.","Make the Cloud Storage bucket public."],correct:0,explanation:"Cloud Functions execute with the identity of a service account. The best practice is to create a dedicated service account for the function with only the minimal permissions it needs (in this case, `roles/storage.objectCreator`) and assign this service account to the function. This follows the principle of least privilege.",wrongExplanations:{1:"Using the default service account with the broad Editor role is a security risk. If the function were compromised, the attacker would have extensive permissions in the project.",2:"Hardcoding credentials is a major security anti-pattern. Credentials should never be stored in code.",3:"Making the bucket public is a massive security vulnerability and is not the correct way to grant write access to a specific service."}},{id:368,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud CDN",question:"You have a website with static assets (images, CSS, JS) stored in a Cloud Storage bucket. The website is served by a global external Application Load Balancer. Users are reporting slow load times in geographic locations far from the bucket's region. How can you improve performance for these users?",options:["Enable Cloud CDN on the backend bucket used by the load balancer.","Create replicas of the bucket in every region where you have users.","Increase the instance size of your backend VMs.","Choose a faster storage class, like Standard, for the bucket."],correct:0,explanation:"Cloud CDN (Content Delivery Network) is designed for this exact purpose. When you enable it on the backend of your load balancer, it caches your content at Google's globally distributed edge locations. When a user requests an asset, it is served from the edge cache closest to them, dramatically reducing latency and improving load times.",wrongExplanations:{1:"Manually replicating a bucket is complex and doesn't provide the same granular, global edge caching as a CDN.",2:"The assets are static and stored in Cloud Storage, not on backend VMs, so changing VM sizes would have no effect.",3:"The storage class affects retrieval time from the bucket itself and storage cost, but it does not solve the network latency problem for globally distributed users. A CDN is the correct solution for network latency."}},{id:369,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You have a critical web service and you need to be notified immediately if it stops responding to HTTP requests. You have configured an uptime check for the service's URL. What is the next step to ensure you get notified?",options:["Create an alerting policy that uses the uptime check as its condition.","Point the uptime check to Cloud Logging.","Configure the uptime check to send an email directly.","Create a custom dashboard to view the uptime check status."],correct:0,explanation:"An uptime check simply probes your endpoint and records the result (success or failure) as a metric. To actually get notified of a failure, you must create a separate alerting policy. In this policy, you select the uptime check metric as the signal and configure it to trigger an alert and send a notification (e.g., via email, SMS, PagerDuty) when the check fails for a specified duration.",wrongExplanations:{1:"Uptime checks generate metrics, not logs.",2:"Uptime checks themselves do not send notifications. They are the signal source for alerting policies, which handle the notifications.",3:"A dashboard is for visualization, not for generating alerts and notifications."}},{id:370,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"Your company has acquired another company that already has its own Google Cloud Organization. You need to manage both entities under a single billing account and apply some common security policies. What is the recommended approach?",options:["Keep both Organizations separate, but link them both to the same billing account. Use hierarchical firewall policies for common security rules.","Migrate all projects from the acquired company's Organization into your primary Organization.","Create a new, parent Organization and nest both existing Organizations underneath it.","Set up VPC Peering between all VPCs in both Organizations."],correct:1,explanation:"Migrating projects from one organization to another is the standard procedure for consolidating Google Cloud resources after an acquisition. This allows you to manage all projects under a single organizational structure, apply a unified set of Organization Policies, and simplify IAM and billing management.",wrongExplanations:{0:"While linking to a single billing account is possible, managing policies across two separate organizations is complex. Hierarchical firewalls can help, but a single org structure is cleaner. For full consolidation, migration is the goal.",2:"The Google Cloud resource hierarchy does not support nesting one Organization under another. The Organization is the root node.",3:"VPC Peering connects networks, but it doesn't address the core requirement of unified management, policy, and billing at the organizational level."}},{id:371,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"BigQuery",question:"A data analyst in your company frequently runs the same complex, resource-intensive query against a very large BigQuery table. To improve performance and control costs for this specific query, what BigQuery feature should you use?",options:["Create a materialized view that pre-computes the results of the query.","Cache the query results.","Increase the analyst's user quota for BigQuery.","Export the table to Cloud Storage and query it with Dataflow."],correct:0,explanation:"Materialized views are pre-computed views that periodically cache the results of a query. When a user's query is compatible with the materialized view, BigQuery can read the pre-computed results instead of executing the full, expensive query against the base table. This is ideal for common and predictable query patterns on large datasets.",wrongExplanations:{1:"BigQuery automatically caches query results for 24 hours, but this only helps if the *exact same query* is run again by any user and the underlying data hasn't changed. A materialized view is more robust and can accelerate even slightly different queries.",2:"User quotas do not affect the performance of a single query.",3:"This is a highly inefficient and complex workflow. BigQuery is the correct tool for querying data within BigQuery."}},{id:372,domain:"Section 4: Configuring access and security",subdomain:"VPC Service Controls",question:"You have created a VPC Service Controls perimeter that includes your project and the Cloud Storage service. A developer with valid IAM permissions is trying to copy a file from a bucket inside the perimeter to a public bucket outside the perimeter using their local machine. What will happen?",options:["The operation will be denied by VPC Service Controls.","The operation will succeed because the developer has the correct IAM permissions.","The operation will succeed, but it will be logged in the Data Access audit logs.","The developer will be prompted to provide a justification before the copy is allowed."],correct:0,explanation:"The core purpose of VPC Service Controls is to prevent data exfiltration. The service perimeter creates a virtual boundary. Even if a user has IAM permissions to read from the source bucket and write to the destination bucket, the VPC Service Controls policy will block the operation because it crosses the perimeter boundary (from a protected project to a public one).",wrongExplanations:{1:"This is incorrect. VPC Service Controls act as an additional layer of security on top of IAM.",2:"The operation will be denied, so no data will be copied to be logged.",3:"Access transparency and justifications are different features. VPC Service Controls will simply block the request."}},{id:373,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Trace",question:"You are using Cloud Trace to analyze the performance of your application. You want to see an aggregated view of latency for a specific transaction, for example, '/checkout', over the last week. Which feature of the Cloud Trace UI should you use?",options:["The Trace analysis report","The Trace list view","Cloud Logging","Cloud Monitoring dashboards"],correct:0,explanation:"The Trace analysis report provides an aggregated view of your trace data. You can generate reports to see trends in latency over time, find the root causes of performance changes by comparing distributions across different time periods, and see a percentile breakdown of latency for specific requests. This is the correct tool for historical, aggregated analysis.",wrongExplanations:{1:"The Trace list view shows a list of individual, recent traces. It's useful for inspecting a single request but not for seeing aggregated trends over a week.",2:"Cloud Logging stores logs, not aggregated trace data.",3:"While you can create charts in Monitoring for overall latency, the Trace analysis report provides a much more detailed and specialized view for diagnosing latency regressions."}},{id:374,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Load balancing",question:"You have a set of legacy TCP-based services running on VMs in a single region. You need to provide a single, private IP address within your VPC that internal clients can use to connect to these services. The load balancer should distribute traffic based on a 5-tuple hash (source/destination IP, source/destination port, protocol). Which load balancer should you use?",options:["Internal TCP/UDP Load Balancer","Internal Application Load Balancer","External TCP/UDP Network Load Balancer","Global external Application Load Balancer"],correct:0,explanation:"The Internal TCP/UDP Load Balancer is designed specifically for this use case. It provides a regional, non-proxied load balancing service for TCP and UDP traffic. It uses a private IP address from your VPC and distributes connections directly to your backend VMs, preserving the client source IP.",wrongExplanations:{1:"The Internal Application Load Balancer is a proxy-based Layer 7 load balancer for HTTP and HTTPS traffic only. It cannot be used for generic TCP services.",2:"An External load balancer has a public IP address and is for internet-facing traffic, not for internal clients within a VPC.",3:"An External Application Load Balancer is for global HTTP/S traffic, not internal TCP services."}},{id:375,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Quotas",question:"What is the primary purpose of resource quotas in Google Cloud?",options:["To prevent unforeseen spikes in usage and protect both the user from unexpected costs and Google's infrastructure.","To act as a hard limit on your monthly bill.","To reserve capacity for your project, ensuring resources are always available.","To enforce security policies on resource creation."],correct:0,explanation:"Quotas are safety limits. They protect the broader Google Cloud community by preventing resource abuse, and they protect you from yourself. For example, a bug in a script that creates VMs in a loop could generate a massive bill if not for the vCPU quota stopping it. They are a preventative control against runaway resource consumption.",wrongExplanations:{1:"Quotas limit resource count (e.g., number of VMs), not the dollar amount of your bill. You can easily hit your budget without hitting a quota. For cost control, you use budgets and alerts.",2:"Quotas are limits, not reservations. Having a quota for 100 VMs does not guarantee that you will be able to launch 100 VMs if the underlying physical capacity in a zone is exhausted.",3:"Security policies are enforced by IAM and Organization Policies, not by quotas."}},{id:376,domain:"Section 4: Configuring access and security",subdomain:"Encryption",question:"You are using Customer-Managed Encryption Keys (CMEK) with Cloud KMS to protect a BigQuery table. A security administrator temporarily disables the KMS key version used by the table. What is the immediate effect on the BigQuery table?",options:["The table and its data become inaccessible until the key version is re-enabled.","The table can still be queried, but no new data can be written to it.","BigQuery automatically re-encrypts the table with a Google-managed key.","There is no effect until the KMS key is permanently destroyed."],correct:0,explanation:"The core principle of CMEK is that the cloud service (BigQuery) needs to call the KMS API to get the key to decrypt the data for every query. If the key is disabled or destroyed, the API call fails, and BigQuery cannot decrypt the data. This makes the data completely inaccessible, giving you direct control over your data's availability.",wrongExplanations:{1:"Both read and write operations will fail because both require access to the encryption key.",2:"BigQuery will not and cannot change the encryption method of a table automatically. The data remains encrypted with the CMEK.",3:"Disabling the key has an immediate effect. You do not need to wait for it to be destroyed."}},{id:377,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE configurations",question:"You need to ensure that pods running in your GKE cluster can communicate with Google APIs like Cloud Storage and BigQuery without traversing the public internet. The GKE nodes themselves do not have public IP addresses. What should you configure?",options:["Enable Private Google Access on the subnet where the GKE nodes reside.","Configure a Cloud NAT gateway for the subnet.","Set up VPC Service Controls.","Deploy an egress proxy in the cluster."],correct:0,explanation:"Private Google Access is a feature that allows VMs (including GKE nodes) without external IP addresses to reach the public IP addresses of Google Cloud services. It configures special routing within Google's network so that traffic from your private subnet to Google APIs stays within Google's network, providing a secure and private communication path.",wrongExplanations:{1:"Cloud NAT is for allowing private instances to reach the *external internet*. For reaching Google APIs, Private Google Access is the more direct and efficient solution.",2:"VPC Service Controls protect services from data exfiltration, but they do not provide the underlying network path for connectivity.",3:"An egress proxy is a possible solution but is much more complex to set up and manage than the native, built-in Private Google Access feature."}},{id:378,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You want to retain all Admin Activity audit logs for your project for 10 years for compliance reasons. The default retention period is 400 days. What should you do?",options:["Create a log sink to export the audit logs to a Cloud Storage bucket and configure the bucket's retention policy.","Modify the retention period of the `_Required` log bucket.","Create a custom log bucket and configure its retention period to 10 years.","This is not possible; logs cannot be retained for more than 400 days."],correct:0,explanation:'The retention period for the default log buckets (`_Required` and `_Default`) cannot be changed. The standard and recommended way to achieve long-term retention is to create a log sink. The sink can filter for the specific logs you need (e.g., `logName:"logs/cloudaudit.googleapis.com%2Factivity"`) and route them to a destination designed for long-term storage, like a Cloud Storage bucket, where you can set a long retention period.',wrongExplanations:{1:"You cannot modify the retention period of the `_Required` log bucket.",2:"You could route logs to a custom bucket, but for long-term archival, Cloud Storage is generally more cost-effective and is the common pattern.",3:"This is incorrect. While the built-in retention is limited, you can achieve any retention period you need by exporting the logs."}},{id:379,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"What is the difference between a primitive role (Owner, Editor, Viewer) and a predefined role (e.g., `roles/compute.admin`)?",options:["Primitive roles are broad and apply to all services in a project, while predefined roles provide granular permissions for a specific service.","Primitive roles can only be applied at the project level, while predefined roles can be applied at any level in the hierarchy.","There is no difference; predefined roles are just aliases for primitive roles.","Primitive roles include permissions to manage IAM policies, while predefined roles do not."],correct:0,explanation:"This is a fundamental concept in IAM. Primitive roles (Owner, Editor, Viewer) were the original roles and grant sweeping permissions across all services within a project. Predefined roles were created to allow for more granular control and follow the principle of least privilege. For example, `roles/compute.admin` grants full control over Compute Engine, but not over BigQuery or Cloud Storage. It is always a best practice to use predefined roles over primitive roles.",wrongExplanations:{1:"This is incorrect. Both types of roles can be applied at various levels (e.g., project, folder, resource).",2:"This is incorrect. They are distinct sets of permissions.",3:"This is not always true. For example, the Editor role does not grant IAM management permissions, but some predefined roles (like `roles/resourcemanager.projectIamAdmin`) do."}},{id:380,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You have a group of stateless web servers in a managed instance group (MIG). You need to update the application to a new version. The update should be applied gradually with minimal impact, and you need the ability to quickly roll back if the new version has problems. Which update method should you use for the MIG?",options:["A rolling update with the `canary` type.","A `proactive` update.","An `opportunistic` update.","Stop all instances, update the instance template, and restart them."],correct:0,explanation:"A rolling update with the canary method is a sophisticated deployment strategy. The MIG first updates a small subset of instances (the canary) to the new version. You can then monitor its performance. If it's healthy, you can proceed to update the rest of the group. If it's unhealthy, you can easily roll back just the canary instances, minimizing the blast radius of a bad update.",wrongExplanations:{1:"A proactive update replaces all instances as quickly as possible. This is not a gradual rollout.",2:"An opportunistic update only replaces instances when they are manually stopped or recreated for other reasons. This is not suitable for a controlled, timely update.",3:"This 'stop the world' approach would cause significant downtime and is not a recommended practice for high-availability applications."}},{id:381,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Marketplace",question:"A developer needs to quickly deploy a standard LAMP stack (Linux, Apache, MySQL, PHP) for a new prototype. They want to avoid manually installing and configuring each component. What is the fastest way to get this environment running on Google Cloud?",options:["Deploy a pre-configured LAMP stack solution from the Google Cloud Marketplace.","Create a new Compute Engine instance with a base Linux image and manually install the components.","Write a startup script to install Apache, MySQL, and PHP on a new VM.","Containerize the application and deploy it to a GKE cluster."],correct:0,explanation:"The Cloud Marketplace offers thousands of pre-configured, ready-to-go solutions from Google and third-party vendors. You can find and deploy a fully configured LAMP stack with just a few clicks, which is by far the fastest and easiest method for standard application stacks.",wrongExplanations:{1:"Manual installation is time-consuming and error-prone, which is what the developer wants to avoid.",2:"Writing a startup script is a form of automation, but it still requires you to figure out all the installation and configuration commands. A Marketplace solution has this already done.",3:"Containerizing the application is a good practice for production, but for a quick prototype, it is significantly more work than deploying a pre-built Marketplace solution."}},{id:382,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"What is the purpose of a 'metric scope' in Cloud Monitoring?",options:["It defines which Google Cloud projects' metrics you can see in your workspace.","It defines the time range for the metrics displayed on a dashboard.","It sets the resolution at which metrics are collected.","It is a filter that you apply to a specific chart."],correct:0,explanation:"A Cloud Monitoring workspace can be configured to monitor multiple Google Cloud projects. The metric scope is the list of projects whose metrics are visible in that workspace. This allows you to create a centralized monitoring project that has visibility into many other projects, which is a common pattern for large organizations.",wrongExplanations:{1:"The time range is selected using the time-range picker on the dashboard or metrics explorer page.",2:"Metric resolution (the sampling period) is determined by the service generating the metric, not by the Monitoring scope.",3:"A filter applies to a chart, whereas the metric scope applies to the entire workspace."}},{id:383,domain:"Section 4: Configuring access and security",subdomain:"Security Command Center",question:"You have just enabled the Security Command Center (SCC) Standard tier. What is a key capability that SCC provides for your organization?",options:["A centralized dashboard of security findings and vulnerabilities from various Google Cloud services.","Real-time intrusion detection and prevention for your VPC network.","Automated patching of security vulnerabilities on your Compute Engine instances.","A managed service for storing and rotating secrets."],correct:0,explanation:"Security Command Center is a centralized security and risk management platform. Its primary function is to ingest findings from various sources (e.g., Security Health Analytics, Web Security Scanner, Event Threat Detection) and present them in a single pane of glass. This gives you a high-level overview of your organization's security posture.",wrongExplanations:{1:"Intrusion detection (Cloud IDS) is a separate service that can feed its findings *into* SCC, but SCC is the dashboard, not the detection engine itself.",2:"SCC identifies vulnerabilities (e.g., an OS that needs patching), but it does not perform the patching itself. You would use a tool like OS Config management to do the patching.",3:"A managed service for secrets is Secret Manager, not Security Command Center."}},{id:384,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"VPC Networking",question:"You have two separate VPCs, `vpc-dev` and `vpc-prod`. You need to allow VMs in `vpc-dev` to communicate with VMs in `vpc-prod` using their internal IP addresses. The VPCs should not have overlapping IP ranges. What should you configure?",options:["VPC Network Peering","A Shared VPC","Cloud VPN between the two VPCs","An external load balancer"],correct:0,explanation:"VPC Network Peering is specifically designed to allow two VPCs to connect and route traffic between them using internal IP addresses. The networks behave as if they are part of the same private networking space. A key requirement is that the peered VPCs cannot have overlapping CIDR ranges.",wrongExplanations:{1:"Shared VPC is a model where a central 'host' project owns the network, and other 'service' projects can launch resources into its subnets. It's for centralizing network management, not for connecting two already existing, distinct VPCs.",2:"While you could technically set up a VPN between two VPCs, it's more complex and less performant than VPC Peering, which uses Google's internal network fabric.",3:"An external load balancer is for internet-facing traffic and does not facilitate internal VPC-to-VPC communication."}},{id:385,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You need to get the logs for a specific pod named `frontend-xyz-123` that is running in your GKE cluster. Which `kubectl` command should you use?",options:["kubectl logs frontend-xyz-123","kubectl get pods frontend-xyz-123 --logs","kubectl describe pod frontend-xyz-123","gcloud container logs frontend-xyz-123"],correct:0,explanation:"`kubectl logs [POD_NAME]` is the standard and direct command for streaming the logs (from stdout/stderr) of a running pod. This is one of the most common commands used for day-to-day GKE operations and troubleshooting.",wrongExplanations:{1:"The `--logs` flag is not a valid flag for the `kubectl get pods` command.",2:"`kubectl describe pod` provides metadata and events related to the pod's state and lifecycle, but it does not show the application logs from inside the container.",3:"`gcloud container` has commands for managing the cluster itself, but for interacting with workloads *inside* the cluster, `kubectl` is the primary tool."}},{id:386,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You need to run a `gcloud` command to create a resource, but you are not sure of the exact syntax and available flags. What is the easiest way to get help and see examples for the `gcloud compute instances create` command directly in your terminal?",options:["gcloud compute instances create --help","gcloud help compute instances create","gcloud docs compute instances create","man gcloud-compute-instances-create"],correct:0,explanation:"The `--help` flag is a universal flag in `gcloud` that can be appended to any command or subcommand. It provides a detailed description of the command, lists all the available flags with explanations, and often includes usage examples right in your terminal. `gcloud help [COMMAND]` is an equivalent way to get the same information.",wrongExplanations:{1:"This is also a correct and equivalent command.",2:"`docs` is not a valid subcommand for `gcloud`.",3:"`gcloud` commands are not documented using traditional `man` pages."}},{id:387,domain:"Section 4: Configuring access and security",subdomain:"Service Accounts",question:"What is the primary difference between a user account and a service account in Google Cloud IAM?",options:["A user account is intended for a human user, while a service account is intended for an application or VM.","User accounts are authenticated with passwords, while service accounts are authenticated with API keys.","User accounts can be granted the Owner role, but service accounts cannot.","User accounts are created in Cloud Identity, while service accounts are created in the IAM section of a project."],correct:0,explanation:"This is the fundamental distinction. User accounts represent people who interact with Google Cloud through the console or CLI. Service accounts represent non-human workloads (applications, scripts, VMs) that need to authenticate and be authorized to call Google Cloud APIs. While both are 'identities' that can be granted IAM roles, their intended use is different.",wrongExplanations:{1:"Service accounts are typically authenticated using service account keys (JSON files) or, preferably, by using the attached identity of the compute resource they are running on. They do not use API keys for authentication.",2:"Service accounts can be granted the Owner role, although it is a very bad practice.",3:"This is true, but it's a detail of their creation process. The most important difference is their intended purpose (human vs. application)."}},{id:388,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Run",question:"You have a containerized web application that you want to deploy on Cloud Run. You want to deploy a new version of the application but only direct 10% of the production traffic to it initially to test it. How can you configure this?",options:["Deploy the new version as a new revision and use traffic splitting to direct 10% of traffic to the new revision and 90% to the old one.","Create a second Cloud Run service and use a load balancer to split traffic.","Deploy the new version to all instances and then quickly roll it back if there are issues.","Use Cloud Deployment Manager to manage traffic percentages."],correct:0,explanation:"Cloud Run has built-in support for revisions and traffic splitting. When you deploy a new version, it creates a new, immutable revision. You can then configure the service to split traffic between different revisions by percentage. This is the standard, idiomatic way to perform canary releases and gradual rollouts on Cloud Run.",wrongExplanations:{1:"This is overly complex and expensive. Cloud Run's native traffic splitting feature is designed for this and does not require a separate load balancer.",2:"This is a 'big bang' or 'all-at-once' deployment, not a gradual 10% test. The goal is to limit the blast radius of a potentially bad deployment.",3:"Cloud Deployment Manager is an Infrastructure-as-Code service for provisioning resources; it does not manage the live traffic routing of a Cloud Run service."}},{id:389,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Storage",question:"You are using a Cloud Storage bucket with object versioning enabled. A user overwrites an important file with a new, incorrect version. How do you revert to the previous version of the file?",options:["List the noncurrent versions of the object, and copy the desired previous version over the current one.","Delete the current version of the object; the previous version will automatically become the current version.","Restore the object from a daily backup.","The previous version is lost once it is overwritten."],correct:0,explanation:"When versioning is enabled, overwriting an object does not delete the old data. Instead, it makes the previous version 'noncurrent' and uploads the new data as the 'live' version. To revert, you can simply find the desired noncurrent version and either copy it to a new object or copy it over the live version to effectively restore it.",wrongExplanations:{1:"Deleting the current version makes the previous version noncurrent, but it also creates a 'delete marker' as the new live object. It does not automatically promote the old version. You must perform a restore/copy action.",2:"Cloud Storage does not have automatic backups. Versioning is the feature that protects against overwrites.",3:"This is incorrect; the entire purpose of versioning is to preserve old versions when objects are overwritten or deleted."}},{id:390,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Identity",question:"Your company does not use Google Workspace but wants to manage users for Google Cloud without the cost of a full Workspace subscription. Users need to be able to authenticate with a corporate identity (e.g., `user@yourcompany.com`). Which service should you use?",options:["Cloud Identity Free","Standard consumer Gmail accounts","IAM custom roles","Service accounts for each user"],correct:0,explanation:"Cloud Identity is Google's Identity as a Service (IDaaS) product. The free tier allows you to create and manage users and groups with your company's domain, providing the core identity management needed for Google Cloud IAM without the additional collaboration features and cost of Google Workspace.",wrongExplanations:{1:"Using consumer Gmail accounts is not a best practice for a corporate environment as it provides no centralized management or control over the user identities.",2:"IAM roles define permissions, they do not create or manage user identities.",3:"Service accounts are for applications, not human users."}},{id:391,domain:"Section 4: Configuring access and security",subdomain:"Security best practices",question:"A developer needs to connect to a Compute Engine instance that has no public IP address for a debugging session. According to Google's zero-trust security principles, what is the most secure method to provide this access?",options:["Use IAP TCP Forwarding.","Create a bastion host with a public IP in the same VPC.","Temporarily add a public IP to the instance and then remove it.","Set up a Cloud VPN connection from the developer's machine to the VPC."],correct:0,explanation:"Identity-Aware Proxy (IAP) for TCP Forwarding allows you to tunnel SSH, RDP, and other TCP traffic to a VM without requiring a public IP or a bastion host. Access is controlled by IAM permissions, not network-level controls, allowing you to grant access to specific users without exposing your instances to the internet. This is a core component of a zero-trust model.",wrongExplanations:{1:"A bastion host is a traditional approach, but it creates another publicly-exposed VM that must be secured and managed. IAP is the more modern, managed, and secure solution.",2:"Temporarily adding a public IP is risky and creates a window of exposure. It should be avoided.",3:"Setting up a full VPN for a single developer's debugging session is overly complex and not scalable. IAP provides on-demand, fine-grained access."}},{id:392,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"You need a managed database for a new web application that stores user session data. The data is simple key-value pairs, access needs to be extremely fast (sub-millisecond latency), and the data is temporary and can be lost without issue. Which service is the best fit?",options:["Memorystore for Redis","Firestore","Cloud SQL","Cloud Bigtable"],correct:0,explanation:"Memorystore for Redis is a fully managed in-memory data store service. It is designed for use cases that require extremely low latency, such as caching, session management, and real-time analytics. Since the session data is temporary, the in-memory nature of Redis is a perfect fit.",wrongExplanations:{1:"Firestore is a NoSQL document database. While fast, it is a durable, disk-based storage and typically has higher latency than an in-memory store like Redis.",2:"Cloud SQL is a relational database and is overkill, as well as too slow, for a simple, high-speed session store.",3:"Cloud Bigtable is a wide-column NoSQL database designed for very large analytical and operational workloads, not for low-latency key-value caching."}},{id:393,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Profiler",question:"You've used Cloud Profiler and the flame graph shows that a large amount of time is being spent in a function called `waitForNetworkResponse`. What does this likely indicate about your application's performance?",options:["The application is spending significant time waiting for responses from external services (I/O wait).","The function has a bug and is in an infinite loop.","The function is performing a very complex mathematical calculation.","The function is consuming a large amount of memory."],correct:0,explanation:"Cloud Profiler can track various metrics, including 'Wall Time'. If a significant portion of wall time is spent in a function related to network calls, it means the application's execution is blocked waiting for a response from another service. This is a classic I/O bottleneck, and it highlights an opportunity to optimize by making calls in parallel or reducing dependency on a slow external service.",wrongExplanations:{1:"An infinite loop would typically manifest as high CPU time, not just high wall time.",2:"A complex calculation would also manifest as high CPU time.",3:"To analyze memory consumption, you would look at the 'Heap' or 'Allocated Heap' profile types, not the wall time profile."}},{id:394,domain:"Section 4: Configuring access and security",subdomain:"Cloud Armor",question:"Your web application is experiencing a SQL injection (SQLi) attack. The application is served by an external Application Load Balancer. Which feature of Cloud Armor can help you mitigate this type of attack?",options:["The pre-configured WAF rules for SQLi.","A geo-based access control rule.","A rate-limiting rule.","An IP allowlist/denylist rule."],correct:0,explanation:"Cloud Armor acts as a Web Application Firewall (WAF). It includes pre-configured rules based on the ModSecurity Core Rule Set that are specifically designed to detect and block common web-based attacks, including SQL injection (SQLi), Cross-Site Scripting (XSS), and others. Enabling these rules provides an immediate layer of defense at the network edge.",wrongExplanations:{1:"A geo-based rule blocks traffic from a country; it does not inspect the traffic for SQLi payloads.",2:"A rate-limiting rule can help against volumetric attacks, but it won't stop a single, well-crafted SQLi request.",3:"An IP-based rule is only effective if the attack is coming from a small, known set of IP addresses, which is rarely the case."}},{id:395,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"Your project contains a mix of production and development resources. You need to be able to see a cost breakdown for just the development resources. What is the best way to achieve this?",options:["Apply a label (e.g., `env:dev`) to all development resources and filter the billing reports by that label.","Move all development resources to a separate project.","Create a separate billing account for development resources.","Manually track the cost of development resources in a spreadsheet."],correct:0,explanation:"Labels are key-value pairs that you can attach to resources for organization and cost allocation. The Cloud Billing reports are fully integrated with labels, allowing you to group and filter your costs by any label you define. This is the most flexible and standard way to get cost breakdowns within a single project.",wrongExplanations:{1:"While moving resources to a separate project is a valid strategy for isolation, it's a much heavier operation than simply applying labels, which can be done without re-provisioning resources. For simple cost tracking, labels are preferred.",2:"A project can only be linked to one billing account at a time. Creating a separate billing account is unnecessary and adds complexity.",3:"Manual tracking is inefficient and error-prone."}},{id:396,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"VPC Networking",question:"What is the purpose of a Shared VPC?",options:["To allow a central host project to own and manage a network that service projects can use.","To connect two VPCs from different organizations.","To share a single public IP address among multiple VM instances.","To extend your on-premises network into Google Cloud."],correct:0,explanation:"Shared VPC is a centralized networking model. A 'host project' owns the VPC network, subnets, and routes. Other 'service projects' can then be attached to the host project, allowing them to create resources (like VMs) that use the subnets from the host project. This is ideal for large organizations that want a central network administration team to manage a consistent and secure network, while allowing developer teams to manage their own resources in separate projects.",wrongExplanations:{1:"Connecting VPCs from different organizations is typically done with VPC Peering or a VPN.",2:"Sharing a public IP is the function of a load balancer, not a Shared VPC.",3:"Extending an on-premises network is done with Cloud Interconnect or Cloud VPN."}},{id:397,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You have deployed an application to GKE using a Deployment manifest. You now need to expose the application to other services within the cluster using a stable DNS name. Which Kubernetes resource should you create?",options:["A Service (of type ClusterIP)","An Ingress","A Pod","A StatefulSet"],correct:0,explanation:"A Kubernetes Service provides a stable abstraction over a set of pods. It gets a stable IP address and a DNS name within the cluster. By default, a Service is of type `ClusterIP`, which means it's only reachable from within the cluster. This is the standard way to enable service-to-service communication.",wrongExplanations:{1:"An Ingress is for exposing services to traffic *outside* the cluster, typically HTTP/S traffic. For internal communication, a Service is sufficient.",2:"A Pod is the workload itself; it does not provide a stable endpoint as pods are ephemeral and their IPs can change.",3:"A StatefulSet is a workload API object for stateful applications, but the Service is still the resource that provides the stable network endpoint for it."}},{id:398,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You grant a user the `roles/storage.objectCreator` role on a bucket. You then grant them the `roles/storage.objectViewer` role at the project level. What are the user's effective permissions on objects in that bucket?",options:["They can create and view objects in that bucket.","They can only create objects in that bucket.","They can only view objects in that bucket.","They have no permissions, as the roles conflict."],correct:0,explanation:"IAM policies are a union of all policies that apply to a resource. The user gets the permissions from the bucket-level policy (`storage.objectCreator`) plus the permissions from the project-level policy (`storage.objectViewer`) which are inherited by the bucket. Therefore, their effective permissions are the combination of both roles.",wrongExplanations:{1:"The project-level role is inherited, so they also get viewer permissions.",2:"The bucket-level role also applies, so they also get creator permissions.",3:"IAM policies are additive. There are no conflicts; permissions are combined."}},{id:399,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You need to create a group of 10 identical Compute Engine VMs for a web frontend. You want to manage them as a single logical unit and ensure that if one VM fails, it is automatically recreated. What should you create?",options:["A managed instance group (MIG).","An unmanaged instance group.","An instance template and then create 10 VMs from it.","A GKE cluster with 10 nodes."],correct:0,explanation:"A managed instance group (MIG) is the correct solution. It uses an instance template to create a set of identical VMs and then manages them for you. Key features include autoscaling, autohealing (recreating failed instances), and rolling updates, which perfectly match the requirements.",wrongExplanations:{1:"An unmanaged instance group is just a collection of heterogeneous VMs. It provides no autohealing or lifecycle management.",2:"This is a manual process. While you would use an instance template, the MIG is the resource that provides the management and autohealing capabilities.",3:"A GKE cluster is for running containerized workloads, not for managing a group of individual VMs directly."}},{id:400,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You have just run `gcloud init` and authenticated. You now want to set your default compute zone to `europe-west1-b` so you don't have to specify the `--zone` flag for every `gcloud compute` command. Which command should you run?",options:["gcloud config set compute/zone europe-west1-b","gcloud config set default_zone europe-west1-b","gcloud compute set-zone europe-west1-b","gcloud projects set-default-zone europe-west1-b"],correct:0,explanation:"The `gcloud config set` command is used to set properties in your active gcloud configuration. Properties are grouped into sections. The default zone and region for compute commands are in the `compute` section, so the correct property name is `compute/zone`.",wrongExplanations:{1:"`default_zone` is not a valid property name.",2:"`gcloud compute set-zone` is not a valid command.",3:"`gcloud projects set-default-zone` is not a valid command."}},{id:401,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You want to be notified whenever a specific error message, `FATAL: connection limit exceeded`, appears in your PostgreSQL logs in Cloud Logging. You need to create an alert for this. What is the first step you should take?",options:["Create a logs-based metric that counts the occurrences of the error message.","Create an uptime check for the database.","Create a log sink to export the logs to Cloud Storage.","Enable Data Access audit logs for the database."],correct:0,explanation:'Cloud Monitoring alerts on metrics, not directly on log entries. To alert on something in a log, you must first create a logs-based metric. This metric will be a counter that increments every time a log entry matching your filter (e.g., `textPayload: "FATAL: connection limit exceeded"`) is ingested. Once you have this metric, you can create a standard metric-based alerting policy for it.',wrongExplanations:{1:"An uptime check probes network connectivity; it cannot read the content of log files.",2:"A log sink is for routing logs, not for creating alerts.",3:"Data Access audit logs track who is accessing data, not application-level error messages from the database engine."}},{id:402,domain:"Section 4: Configuring access and security",subdomain:"VPC Security",question:"What is the purpose of the implied `deny all egress` firewall rule in a VPC network?",options:["It blocks any outgoing traffic that is not explicitly allowed by another firewall rule with a higher priority.","It prevents VMs from being created with public IP addresses.","It blocks all incoming traffic from the internet.","It denies traffic between different subnets within the VPC."],correct:0,explanation:"Every VPC network has two implied firewall rules that cannot be deleted, each with the lowest possible priority (65535): an `allow all egress` rule and a `deny all ingress` rule. However, if you create any custom egress rule, you might think of the default behavior as 'deny unless allowed'. The `deny all egress` rule, priority 65535, blocks any outgoing traffic that isn't matched by a higher-priority (lower number) rule. It acts as a fail-safe.",wrongExplanations:{1:"Preventing public IPs is the job of an Organization Policy, not a firewall rule.",2:"Blocking incoming traffic is the job of the implied `deny all ingress` rule.",3:"The implied `allow all egress` and the default `allow internal` rules permit traffic between subnets. The `deny all egress` rule applies to traffic leaving the VPC."}},{id:403,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You need to provide a user with a time-limited URL to upload a large file directly to a Cloud Storage bucket, without giving them permanent IAM permissions or making the bucket public. What should you generate?",options:["A signed URL (V4).","A service account key.","An access control list (ACL) for the user.","A public access URL for the bucket."],correct:0,explanation:"Signed URLs provide a way to grant time-limited access to a specific Cloud Storage resource. You can generate a URL that grants a specific user or service account permission to perform an action (like `PUT` for an upload) on a specific object for a defined period. The user can then use this URL to perform the action without needing any other Google Cloud credentials.",wrongExplanations:{1:"Giving a user a service account key grants them permanent (until the key is revoked) permissions, which is not what is required.",2:"ACLs are a legacy way to manage permissions and are not ideal for providing temporary, one-off access. Uniform bucket-level access and signed URLs are the modern approach.",3:"A public URL would allow anyone to access the bucket, which is insecure."}},{id:404,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You have a fleet of VMs and you want to ensure they all have the Cloud Monitoring agent installed and running correctly to collect detailed system metrics (like memory and disk usage). What is the most efficient way to manage the agent's installation and ensure it stays up-to-date across the entire fleet?",options:["Use the OS Config agent to manage the installation and version of the Monitoring agent via a policy.","Manually SSH into each VM and run the installation script.","Create a custom Compute Engine image with the agent pre-installed.","Use a startup script on each VM to check for and install the agent on every boot."],correct:0,explanation:"OS Config is a suite of tools for managing operating system configuration at scale. You can create a guest policy that defines the desired state for a package (like the `google-cloud-ops-agent`). The OS Config agent on each VM will then ensure that this package is installed and maintained at the specified version, providing a centralized and automated way to manage the fleet.",wrongExplanations:{1:"Manual SSH is not scalable or efficient for a fleet of VMs.",2:"A custom image is a good start, but it doesn't solve the problem of keeping the agent updated after the VMs have been created. OS Config manages the lifecycle of the agent.",3:"A startup script runs on every boot and can be slow and inefficient. OS Config is a more robust, state-based management tool."}},{id:405,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"At which level of the Google Cloud resource hierarchy can a billing account be attached?",options:["Organization and Project","Folder","Resource (e.g., a VM)","VPC Network"],correct:0,explanation:"A billing account is what pays for the resources consumed. It can be linked directly to an Organization, in which case all projects under that organization can be linked to it. It can also be linked directly to individual projects that are not part of an organization. It cannot be attached to folders or individual resources.",wrongExplanations:{1:"Folders are for grouping projects and applying policies; they do not have a direct billing relationship.",2:"Billing is not managed at the individual resource level.",3:"Billing is not managed at the network level."}},{id:406,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You want to see a history of all changes made to the IAM policy of a project, including who made the change and when. Where would you find this information?",options:["In the Admin Activity audit logs for the project.","In the project's activity feed.","In the IAM section of the Cloud Console.","In the Data Access audit logs."],correct:0,explanation:"Modifying an IAM policy (e.g., granting a user a new role) is a significant administrative action. All such actions are recorded in the Admin Activity audit logs, which are enabled by default. These logs capture the API call (`SetIamPolicy`), the identity that made the call, and the timestamp.",wrongExplanations:{1:"The activity feed provides a high-level summary of recent actions but may not have the full historical detail that the audit logs provide.",2:"The IAM page shows the *current* policy, not the history of changes to it.",3:"Data Access logs are for tracking access to data (e.g., reading a file), not for administrative changes."}},{id:407,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE Autopilot",question:"What is a key security benefit of using a GKE Autopilot cluster compared to a GKE Standard cluster?",options:["The underlying nodes are managed by Google, reducing the attack surface and management burden for the user.","All pods are automatically deployed into a secure sandbox.","It automatically encrypts all secrets in the cluster.","It requires the use of VPC-native clusters."],correct:0,explanation:"In Autopilot mode, Google manages the nodes, including the OS, runtime, and networking. This means Google is responsible for security patching and hardening of the nodes. This significantly reduces the attack surface that the user is responsible for, as they cannot SSH into nodes or modify their low-level configuration.",wrongExplanations:{1:"While Autopilot does implement various security measures, the term 'secure sandbox' is not a standard feature. Both modes use standard Linux container isolation.",2:"Both Standard and Autopilot clusters support encryption of secrets at rest, but it's not an automatic feature unique to Autopilot.",3:"Both Standard and Autopilot clusters are VPC-native. This is not a differentiating security benefit."}},{id:408,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Observability",question:"What are the three core pillars of Google Cloud's operations suite (formerly Stackdriver)?",options:["Logging, Monitoring, and Trace","Billing, IAM, and Networking","Compute, Storage, and Databases","Security, Compliance, and Auditing"],correct:0,explanation:"Google Cloud's operations suite is built around the three fundamental pillars of observability: Cloud Logging (for collecting and analyzing log data), Cloud Monitoring (for collecting and visualizing metrics and creating alerts), and Cloud Trace (for distributed tracing to understand request latency). Error Reporting, Profiler, and Debugger are also part of the suite but these three are the core.",wrongExplanations:{1:"These are core infrastructure services, not observability tools.",2:"These are core infrastructure categories, not observability tools.",3:"These are security concepts, not the pillars of the operations suite."}},{id:409,domain:"Section 4: Configuring access and security",subdomain:"Secret Manager",question:"Your application running on a Compute Engine VM needs to connect to a third-party API using an API key. According to best practices, how should you store and retrieve this API key?",options:["Store the key in Secret Manager and grant the VM's service account the Secret Manager Secret Accessor role.","Store the key in a file on the VM's boot disk.","Hardcode the key as a string in the application's source code.","Store the key in the VM's custom metadata."],correct:0,explanation:"Secret Manager is a dedicated service for storing secrets like API keys, passwords, and certificates. It provides versioning, auditing, and fine-grained access control. The best practice is to store the secret in Secret Manager and grant the service account of the application that needs it the specific IAM role to access it (`roles/secretmanager.secretAccessor`). The application can then retrieve the secret at runtime.",wrongExplanations:{1:"Storing secrets on disk in plaintext is insecure. If the VM is compromised, the secret is immediately exposed.",2:"Hardcoding secrets in source code is a major security anti-pattern and makes rotation extremely difficult.",3:"While custom metadata is more secure than putting secrets in code, it's still not as secure as using a dedicated secret management service like Secret Manager, which provides much better auditing and access control."}},{id:410,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Run",question:"You have a Cloud Run service that needs to connect to a Cloud SQL database instance that has only a private IP address. How can the Cloud Run service establish this connection?",options:["Configure a Serverless VPC Access connector.","Assign a public IP address to the Cloud SQL instance.","Use the Cloud SQL Auth Proxy in your container.","This type of connection is not possible."],correct:0,explanation:"Serverless VPC Access connectors create a bridge between your serverless environment (like Cloud Run or Cloud Functions) and your VPC network. By routing the Cloud Run service's traffic through the connector, the service can access resources in the VPC, such as a Cloud SQL instance, using their internal, private IP addresses.",wrongExplanations:{1:"Assigning a public IP to the database is a security risk and is often against company policy. The goal is to connect privately.",2:"The Cloud SQL Auth Proxy is used for secure connections, often over the public internet or from environments like GKE. For private IP connections from Cloud Run, the VPC connector is the required networking component.",3:"This is incorrect; it is a very common and supported pattern."}},{id:411,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Organization policies",question:"Your company has a policy that all Cloud Storage buckets must be created in the `EU` multi-region to comply with data residency regulations. How can you enforce this and prevent buckets from being created in other locations?",options:["Set an Organization Policy with the `storage.location` constraint, allowing only the `EU` value.","Create an IAM role that only allows creating buckets in the `EU`.","Write a Cloud Function that is triggered when a new bucket is created and deletes it if it's in the wrong location.","Trust your developers to follow the documented guidelines."],correct:0,explanation:"Organization Policies are the correct tool for enforcing resource constraints. The `gcp.resourceLocations` or the more specific `storage.location` constraint can be used to define an allowed list of locations for resource creation. By setting this at the organization or folder level, any attempt to create a bucket in a non-allowed location will be blocked by the API.",wrongExplanations:{1:"IAM controls *who* can perform an action, but not *where* they can perform it. This is not the right tool for location constraints.",2:"This is a reactive approach. It's better to prevent the non-compliant resource from being created in the first place, which is what an Organization Policy does.",3:"Relying on trust is not an adequate compliance control. An automated, preventative measure is required."}},{id:412,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Backup strategies",question:"What is a key benefit of using Cloud Storage for backups of on-premises data compared to using traditional on-premises tape backups?",options:["Geographic redundancy, durability, and lower operational overhead.","Faster restore times for large datasets.","Lower upfront hardware costs.","Better integration with on-premises applications."],correct:0,explanation:"Cloud Storage provides 99.999999999% (11 nines) of durability and automatically replicates data across multiple locations (for regional and multi-regional buckets). This provides incredible data protection against disasters. It also eliminates the operational overhead of managing physical tapes, drives, and off-site storage logistics.",wrongExplanations:{1:"Restoring large datasets from the cloud can be limited by network bandwidth, and may be slower than restoring from a local tape drive if the network connection is slow. The primary benefits are durability and accessibility.",2:"While true, this is a part of the broader benefit of lower operational overhead. The durability and redundancy are more significant technical benefits.",3:"Cloud storage does not inherently integrate better with on-premises applications than on-premises storage. The key benefits are related to the nature of the cloud platform itself."}},{id:413,domain:"Section 4: Configuring access and security",subdomain:"Cloud Audit Logs",question:"You have enabled Data Access audit logs for your Cloud Storage bucket. Which of the following actions would be recorded in these logs?",options:["A user reading the contents of a file (`storage.objects.get`).","A user changing the bucket's storage class.","A user deleting the bucket.","A user modifying the IAM policy of the bucket."],correct:0,explanation:"Data Access audit logs are specifically for tracking when data is created, read, or modified. Actions like `storage.objects.get` (reading a file), `storage.objects.create` (writing a file), and `storage.objects.list` are typical examples of what would be captured. Administrative changes are captured in the Admin Activity logs.",wrongExplanations:{1:"Changing the bucket's configuration is an administrative action and would be in the Admin Activity logs.",2:"Deleting the bucket is an administrative action and would be in the Admin Activity logs.",3:"Modifying the IAM policy is a critical administrative action and would be in the Admin Activity logs."}},{id:414,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Filestore",question:"You are migrating a legacy application from on-premises to Google Cloud. The application requires a shared file system (NFS) that can be mounted by multiple Compute Engine instances simultaneously. Which Google Cloud service provides a fully managed NFS server?",options:["Cloud Filestore","Cloud Storage with FUSE","Persistent Disk in Multi-Writer mode","A self-managed NFS server on a Compute Engine VM"],correct:0,explanation:"Cloud Filestore is Google's fully managed Network File System (NFS) service. It provides a familiar file system interface and is designed for applications that require a shared file system. It's the direct, managed solution for this common lift-and-shift migration scenario.",wrongExplanations:{1:"Cloud Storage FUSE allows you to mount a Cloud Storage bucket as a file system, but it does not provide true POSIX compliance or the performance of a dedicated NFS solution. It's best for different use cases.",2:"Persistent Disk in Multi-Writer mode allows a disk to be attached to multiple VMs, but it requires the use of a clustered file system (like GFS2) and does not provide an NFS interface.",3:"While you could build your own NFS server, it would not be a managed service. You would be responsible for its availability, performance, and maintenance. Filestore is the managed solution."}},{id:415,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You are creating an alerting policy. You want to receive notifications through multiple channels: email to your SRE team's mailing list and a message to a specific Slack channel. What do you need to configure?",options:["Configure a single notification channel that points to Slack, and set up a rule in Slack to forward the message to email.","Create two separate notification channels, one for email and one for Slack, and attach both to the alerting policy.","This is not possible; an alerting policy can only have one notification channel.","Write a Cloud Function that is triggered by the alert and sends the notifications to both channels."],correct:1,explanation:"Cloud Monitoring alerting policies support attaching multiple notification channels. The correct procedure is to configure each desired channel (e.g., an Email channel, a Slack channel, a PagerDuty channel) separately under the main Monitoring settings, and then when you create the alerting policy, you can select checkboxes for all the channels you want to notify.",wrongExplanations:{0:"While this might work, it's a workaround. The native solution is to use multiple channels directly.",2:"This is incorrect. Multiple channels are fully supported.",3:"This is overly complex. The functionality is built directly into Cloud Monitoring."}},{id:416,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cost management",question:"Your company gives all developers their own 'sandbox' projects. You want to ensure that if a developer forgets to shut down a large VM, they don't accidentally run up a huge bill. What is the most effective way to automatically control spending in these projects?",options:["Set a billing budget with a notification, and link it to a Pub/Sub topic that triggers a Cloud Function to disable billing for the project.","Grant developers a custom role that prevents them from creating large VMs.","Rely on low default quotas to limit the number of resources they can create.","Send a daily email to all developers reminding them to shut down their resources."],correct:0,explanation:"This is a key cost control pattern. You can create a budget and set an alert threshold (e.g., 100% of the budget). Instead of just sending an email, you can have this alert publish a message to a Pub/Sub topic. This message can then trigger a Cloud Function that programmatically disables billing for that project, which shuts down all running resources and prevents any further charges. This is the most reliable and automated way to enforce a hard spending limit.",wrongExplanations:{1:"This restricts experimentation and might prevent them from testing something they legitimately need to. Controlling the total cost is more flexible.",2:"Quotas limit resource count, not cost. A developer could still run a very expensive type of resource for a long time without hitting a quota.",3:"A reminder email is not an automated control and is not reliable."}},{id:417,domain:"Section 4: Configuring access and security",subdomain:"IAM Recommender",question:"You are a new administrator for a Google Cloud project that has been running for a year. You suspect that many users have overly broad permissions. What is the easiest way to identify IAM roles that have been granted but are not being used?",options:["Use IAM Recommender to get insights on unused and excessive permissions.","Manually review the Admin Activity audit logs for every user.","Ask each user what permissions they think they need.","Remove all roles and see who complains."],correct:0,explanation:"IAM Recommender is an intelligent service that analyzes your IAM usage patterns over time. It can automatically identify permissions that have been granted but not used within the last 90 days and provide recommendations to remove or replace overly permissive roles with more granular ones. This is the most efficient and data-driven way to enforce the principle of least privilege.",wrongExplanations:{1:"Manually reviewing logs would be an incredibly time-consuming and difficult task. IAM Recommender automates this analysis.",2:"Users often don't know the exact permissions they need or may ask for more than is necessary. A data-driven approach is better.",3:"This is a disruptive and unprofessional way to manage permissions."}},{id:418,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Build",question:"You want to automate the process of building a Docker container image from a Dockerfile in your source code repository every time you push a new commit to the `main` branch. The built image should be pushed to Artifact Registry. Which service should you use?",options:["Cloud Build","Cloud Functions","Cloud Deployment Manager","Cloud Run"],correct:0,explanation:"Cloud Build is a fully managed continuous integration, delivery, and deployment (CI/CD) platform. It can execute builds based on triggers, such as a commit to a Git repository. A common use case is to define a build pipeline in a `cloudbuild.yaml` file that uses the Docker build step to create an image and then pushes it to Artifact Registry.",wrongExplanations:{1:"Cloud Functions is for running event-driven code, not for orchestrating a container build pipeline.",2:"Cloud Deployment Manager is for provisioning infrastructure, not for building software artifacts.",3:"Cloud Run is for running containers, not for building them."}},{id:419,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"You are troubleshooting a performance issue on a Windows Server Compute Engine instance. You need to access the graphical user interface of the server to use a diagnostic tool. How can you connect to the instance?",options:["Generate Windows credentials and connect using an RDP client.","Connect using SSH from the Cloud Console.","Use the serial console to access the command prompt.","View a screenshot of the instance."],correct:0,explanation:"The standard way to connect to the graphical desktop of a Windows Server instance is by using the Remote Desktop Protocol (RDP). In Google Cloud, you would first generate a username and password for the instance, and then you can use any RDP client (like the one built into Windows or others on macOS/Linux) to connect to the instance's IP address.",wrongExplanations:{1:"SSH is for command-line access to Linux instances, not for graphical access to Windows.",2:"The serial console on a Windows instance provides a command-line interface (Special Administration Console), not the full graphical desktop.",3:"A screenshot is a static image and does not provide an interactive session."}},{id:420,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"Your team is adopting Infrastructure as Code. You need to create a Terraform configuration to provision a new VPC network with three subnets in different regions. How can Gemini Cloud Assist accelerate this process?",options:["Ask Gemini: 'Generate a Terraform configuration for a VPC with subnets in us-central1, europe-west1, and asia-east1'.","Use Gemini to analyze the cost of the VPC network.","Ask Gemini to apply the Terraform configuration for you.","Use Gemini to write unit tests for your Terraform code."],correct:0,explanation:"Gemini Cloud Assist is adept at generating code and configuration in various languages, including HCL for Terraform. By describing the desired infrastructure in natural language, you can get a valid and well-structured Terraform configuration file as a starting point, which you can then customize and apply. This significantly reduces the time spent looking up syntax and resource definitions.",wrongExplanations:{1:"Cost analysis is typically done using the Google Cloud Pricing Calculator or by analyzing billing data. Gemini's primary role here is code generation.",2:"Gemini can generate the configuration files, but it does not execute commands like `terraform apply`. You would still run that yourself.",3:"While a powerful AI, generating specific unit tests for IaC is a more advanced task. Its core strength lies in generating the primary configuration."}},{id:421,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud NGFW",question:"You are using Cloud NGFW network firewall policies. You have a default 'allow all egress' rule with a priority of 1000. You need to add a rule to explicitly block all VMs from making outbound connections to a known malicious IP range, `203.0.113.0/24`. What priority should you assign to this new 'deny' rule?",options:["A lower number than 1000 (e.g., 900).","A higher number than 1000 (e.g., 1100).","The same priority, 1000.","Priority does not matter for deny rules."],correct:0,explanation:"In Google Cloud firewall policies, rules are evaluated in order of priority, from the lowest number (highest priority) to the highest number (lowest priority). To ensure your specific 'deny' rule is evaluated *before* the general 'allow' rule, it must have a higher priority, which means a lower number.",wrongExplanations:{1:"If the deny rule has a lower priority (higher number) than the allow rule, traffic to the malicious IP range would be permitted by the allow rule first, and the deny rule would never be evaluated.",2:"If two rules have the same priority, their behavior is not guaranteed. You should always use distinct priorities.",3:"Priority is critical for all rules to ensure a deterministic and predictable firewall policy."}},{id:422,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Database Center",question:"Which of the following is NOT a feature or capability of the Database Center?",options:["Executing SQL queries directly against your databases.","Viewing a centralized list of all database instances across multiple projects.","Seeing an overview of performance metrics like CPU utilization for the entire database fleet.","Identifying database instances with security recommendations from Security Command Center."],correct:0,explanation:"Database Center is an observability and management dashboard. It provides high-level views of performance, health, and security. It is not a query execution tool or an IDE. To run queries, you would use the specific tool for that database, such as the Cloud SQL Studio, the `bq` command-line tool, or a standard database client.",wrongExplanations:{1:"A centralized inventory is a core feature of Database Center.",2:"Fleet-level performance overviews are a key benefit.",3:"Integration with Security Command Center to show security posture is a key feature."}},{id:423,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"You have created a custom IAM role for your application's service accounts. Later, you realize you added a permission that is too broad and you need to remove it. What is the process for updating the custom role?",options:["Edit the existing custom role to remove the unwanted permission. The change will apply to all identities that have been granted the role.","You cannot edit a custom role. You must create a new role and grant it to all the service accounts.","Delete the custom role and recreate it with the correct permissions.","Run `gcloud iam roles update` and provide only the permissions you want to keep."],correct:0,explanation:"Custom IAM roles are mutable. You can edit them after they have been created to add or remove permissions. When you update a role's definition, the changes are propagated and take effect for all principals (users, groups, service accounts) that have been assigned that role.",wrongExplanations:{1:"Custom roles are editable, so this is incorrect.",2:"Deleting the role would revoke access for all service accounts. Editing the existing role is the correct, non-disruptive procedure.",3:"While this describes the command, the key concept is that existing roles can be edited, and the change applies everywhere."}},{id:424,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Config Connector",question:"You are using Config Connector to manage a Cloud SQL instance. You define the desired state of the instance in a YAML file and apply it to your GKE cluster. What happens if an engineer later goes into the Cloud Console and manually changes a setting on that Cloud SQL instance, such as the machine type?",options:["Config Connector's controllers will detect the drift and automatically revert the change to match the state defined in the YAML file.","The manual change will persist, and Config Connector will update its status to show a discrepancy.","Config Connector will delete and recreate the Cloud SQL instance to match the desired state.","Config Connector will not be aware of the manual change."],correct:0,explanation:"Config Connector operates on the principle of continuous reconciliation. It periodically checks the state of the Google Cloud resources it manages against the desired state declared in your Kubernetes manifests. If it detects any manual changes (drift), it will automatically take action to bring the resource back into compliance with your declared configuration.",wrongExplanations:{1:"The default behavior is to actively enforce the desired state, not just report on it.",2:"Deletion and recreation is a drastic step that Config Connector would only take if necessary. For a simple setting change, it would perform an update.",3:"Drift detection and correction is a core feature of Config Connector."}},{id:425,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Console",question:"You want to quickly find a specific Compute Engine instance named `instance-123` among hundreds of other resources in your project. What is the most efficient way to do this in the Google Cloud Console?",options:["Use the search bar at the top of the Cloud Console.","Go to the Compute Engine page and manually look through the list of instances.","Use Cloud Shell to run `gcloud compute instances list` and grep for the name.","Check the project's activity logs."],correct:0,explanation:"The search bar at the top of the Cloud Console is a powerful, unified search tool. It allows you to quickly find resources by name, ID, label, and other attributes across all services in your project. This is much faster than navigating to a specific service's page and searching there.",wrongExplanations:{1:"Manual searching is inefficient for a large number of resources.",2:"Using the CLI is a valid approach, but the question asks for the most efficient way *in the Cloud Console*.",3:"Activity logs show actions taken on resources, but they are not a tool for finding a resource's current state."}},{id:426,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"A developer is complaining that their queries to a Cloud SQL instance are slow. Using Query Insights, you identify their specific query and see that its top 'Wait State' is 'Lock Wait'. What does this indicate?",options:["The query is waiting for another transaction to release a lock on a row or table it needs to access.","The query is waiting for data to be read from disk.","The query is actively using the CPU to process data.","The database instance does not have enough memory."],correct:0,explanation:"'Lock Wait' specifically means that the query is blocked because it is trying to access a resource (like a row, a page, or a whole table) that is currently locked by another active transaction. This is a common source of performance issues in transactional databases and points to problems with transaction contention or long-running transactions.",wrongExplanations:{1:"Waiting for disk is categorized as 'I/O Wait'.",2:"Active processing is categorized as 'CPU'.",3:"While low memory can cause performance issues (like more I/O), the specific 'Lock Wait' state points directly to resource locking contention."}},{id:427,domain:"Section 4: Configuring access and security",subdomain:"VPC Service Controls",question:"You have set up a VPC Service Controls perimeter. A VM inside the perimeter needs to call a third-party API on the public internet, but the perimeter is blocking all egress traffic to non-supported services. How can you allow this specific outbound traffic while maintaining the perimeter's security?",options:["Configure an egress policy for the service perimeter that allows traffic to the required external host.","Create a VPC firewall rule to allow the traffic.","Temporarily disable the service perimeter.","Use Cloud NAT to provide an outbound path."],correct:0,explanation:"VPC Service Controls are designed to be configurable. An egress policy is a specific feature that allows you to define granular exceptions to the perimeter's egress blocking. You can specify which identities are allowed to call which external services (defined by hostnames, IP ranges, etc.), providing a secure and auditable way to allow necessary external communication.",wrongExplanations:{1:"A VPC firewall rule will not override a VPC Service Controls policy. The perimeter policy is enforced at a higher level.",2:"Disabling the perimeter would completely remove the security boundary and is not a recommended practice.",3:"Cloud NAT provides a network path, but it does not override the denial from the VPC Service Controls policy."}},{id:428,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"BigQuery",question:"You have a very large (petabyte-scale) BigQuery table. You need to run a query that processes all the data in the table every morning. To manage costs and ensure predictable performance, which pricing model should you use for this project?",options:["Capacity-based pricing (slots reservations/commitments).","On-demand pricing.","Storage-based pricing.","Flat-rate pricing."],correct:0,explanation:"On-demand pricing charges you per byte processed, which can be very expensive and have variable performance for large, recurring queries. Capacity-based pricing (often called flat-rate) allows you to reserve a specific amount of query processing capacity (slots) for a fixed price. This provides predictable costs and performance, making it ideal for large, enterprise-scale data warehousing workloads.",wrongExplanations:{1:"On-demand pricing would be very costly and unpredictable for this use case.",2:"Storage-based pricing refers to the cost of storing data, not the cost of querying it.",3:"Flat-rate is the older term for what is now called capacity-based pricing. This is also correct, but 'capacity-based' is the more modern term."}},{id:429,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Error Reporting",question:"Your application is logging errors to Cloud Logging with correctly formatted JSON, including an `error` field and a stack trace. You have enabled Error Reporting, but the errors are not appearing in the Error Reporting dashboard. What is a likely cause?",options:["The log entries are not formatted as expected by Error Reporting, such as missing a `serviceContext` or a correctly formatted `message` field with a stack trace.","You need to install the Error Reporting agent on your VMs.","The IAM permissions for the Error Reporting service are incorrect.","Error Reporting only works for applications written in specific languages."],correct:0,explanation:"For Error Reporting to automatically parse errors from Cloud Logging, the log entries must be in a specific JSON format. It looks for a `message` field containing the stack trace and a `serviceContext` object that identifies the application. If these fields are missing or incorrectly formatted, Error Reporting will not be able to group the errors correctly.",wrongExplanations:{1:"There is no general 'Error Reporting agent'. It works by integrating with services or by parsing structured logs.",2:"If logs are appearing in Cloud Logging, it's unlikely to be a permissions issue for the Error Reporting service itself, but rather a formatting issue in the logs.",3:"While there are client libraries for specific languages, Error Reporting can parse errors from any language as long as the logs are structured correctly."}},{id:430,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"A project's spending has been unexpectedly shut down after its budget was exceeded. Developers in the project report they can no longer use any billable services. What action must be taken to re-enable services for the project?",options:["Unlink and relink the project to the billing account in the Cloud Console.","Increase the budget amount in the project's budget settings.","Grant the developers the 'Billing Account User' role.","The services will re-enable automatically at the start of the next billing cycle."],correct:0,explanation:"When billing is programmatically disabled for a project (often via a Cloud Function triggered by a budget alert), the link between the project and its billing account is severed. To restore services, you must manually re-enable billing by relinking the project to a valid billing account.",wrongExplanations:{1:"Simply increasing the budget amount does not re-enable billing if it has been disabled. The link must be restored first.",2:"IAM roles control who can manage billing, but they do not re-enable a disabled billing link.",3:"Services do not automatically re-enable. Manual intervention is required."}},{id:431,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You need to provide a set of sensitive configuration files to a new Compute Engine instance at boot time. These files should not be stored in the boot disk image or in a public repository. What is the most secure method to provide this data?",options:["Pass the data as custom metadata to the instance during creation.","Store the files in a public Cloud Storage bucket and download them with a startup script.","SSH into the instance after it boots and manually copy the files.","Embed the files directly into the startup script."],correct:0,explanation:"Instance metadata is a secure way to pass small amounts of configuration data to an instance. The data is accessible only from within the instance itself via the metadata server, and it's not part of the persistent disk image. This is the standard method for securely bootstrapping an instance.",wrongExplanations:{1:"Storing sensitive files in a public bucket is a major security risk.",2:"Manual copying is not automated and is inefficient for scalable deployments.",3:"Embedding large or sensitive files in a startup script is cumbersome and exposes the data in the instance's metadata, but using the dedicated metadata feature is cleaner."}},{id:432,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You need to view logs from all projects within a specific Folder in a single, centralized view. What should you do?",options:["Create a log sink at the Folder level to route all logs to a single BigQuery dataset or Cloud Storage bucket.","Create a Cloud Monitoring workspace and add all the projects to its metric scope.","Navigate to each project individually to view its logs.","This is not possible; logs can only be viewed on a per-project basis."],correct:0,explanation:"Log sinks can be configured at any level of the resource hierarchy (Organization, Folder, or Project). Creating a sink at the Folder level and setting its `includeChildren` property to true will capture all log entries from all projects within that folder and route them to a single, specified destination for centralized analysis.",wrongExplanations:{1:"A Monitoring workspace is for aggregating metrics, not logs.",2:"This is inefficient and does not provide the required centralized view.",3:"This is incorrect. Aggregated sinks are a core feature of Cloud Logging."}},{id:433,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"A service account in `project-a` needs to be able to read objects from a Cloud Storage bucket in `project-b`. What is the best practice for granting this cross-project access?",options:["In `project-b`, grant the `Storage Object Viewer` role to the service account from `project-a` on the specific bucket.","Create a new service account in `project-b` with the required role and download its key to the application in `project-a`.","Make the bucket in `project-b` public.","Set up VPC Peering between the projects' VPCs."],correct:0,explanation:"IAM principals (including service accounts) are global. You can grant a role to any principal on any resource, regardless of which project the principal or resource belongs to. The best practice is to directly grant the service account from `project-a` the specific, least-privilege role it needs on the resource in `project-b`.",wrongExplanations:{1:"Downloading and managing service account keys is a security risk and should be avoided. Direct role grants are more secure.",2:"Making the bucket public is a major security vulnerability.",3:"VPC Peering enables network connectivity, but it does not grant IAM permissions. Authorization is still handled by IAM."}},{id:434,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud SQL",question:"You are configuring a Cloud SQL for MySQL instance for high availability. What feature should you enable to ensure the instance can survive a zonal failure?",options:["Enable high availability (HA) configuration.","Enable automated backups.","Create a read replica in another region.","Increase the instance's vCPU and memory."],correct:0,explanation:"The high availability (HA) configuration for Cloud SQL provisions a primary instance in one zone and a standby instance in a different zone within the same region. Data is synchronously replicated between them. If the primary zone fails, Cloud SQL automatically fails over to the standby instance with no data loss, providing resilience against zonal failures.",wrongExplanations:{1:"Backups are for disaster recovery (restoring data), not for providing high availability (automatic failover).",2:"A cross-region read replica is for disaster recovery and read scaling in another region, not for automatic HA failover within the same region.",3:"Increasing instance size (vertical scaling) improves performance but does not improve availability."}},{id:435,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"Your managed instance group (MIG) is configured with an autoscaler. The autoscaler is not scaling up the group even though the instances are reporting high CPU utilization. What is a likely reason for this?",options:["The MIG's size is already at the maximum number of instances (`maxReplicas`) defined in the autoscaler policy.","The autoscaler is configured to scale based on a schedule, not CPU.","The health check for the instance group is failing.","You have not enabled the Cloud Monitoring API."],correct:0,explanation:"Every autoscaler policy has a minimum and maximum number of replicas. Even if the scaling signal (like CPU) exceeds the target, the autoscaler will not create new instances if the group has already reached its configured maximum size. This is a common issue to check when troubleshooting autoscaling.",wrongExplanations:{1:"This is a possible reason, but the most common constraint is the maximum size limit.",2:"Failing health checks cause the MIG to recreate instances (autohealing), but they do not prevent the autoscaler from adding new instances if the existing healthy instances are overloaded.",3:"The Monitoring API is enabled by default in most projects, and autoscaling relies on it. It's less likely to be the issue than a simple configuration limit."}},{id:436,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You want to find all Compute Engine instances in your project that have a specific network tag, `backend-server`. Which `gcloud` command should you use?",options:['gcloud compute instances list --filter="tags.items=backend-server"',"gcloud compute instances search --tags backend-server","gcloud compute instances list --tags backend-server","gcloud compute instances list | grep backend-server"],correct:0,explanation:"The `gcloud` command-line tool has a powerful `--filter` flag that allows you to apply server-side filtering based on resource properties. The syntax for filtering on tags, which is a list, is `tags.items=[VALUE]`.",wrongExplanations:{1:"`search` is not a valid subcommand for `gcloud compute instances`.",2:"There is no `--tags` flag for the list command; filtering is done with the `--filter` flag.",3:"Using `grep` is client-side filtering. It is less efficient than using `--filter`, which performs the filtering on the server before returning the results."}},{id:437,domain:"Section 4: Configuring access and security",subdomain:"Organization policies",question:"Your company wants to restrict which public container images can be run on your GKE clusters. You only want to allow images from Google's curated base images (gcr.io/google-containers) and your company's private Artifact Registry. How can you enforce this?",options:["Use the 'Define allowed container image repositories' Organization Policy constraint (`constraints/container.trustedImageProjects`).","Use a Kubernetes admission controller like OPA Gatekeeper to validate image sources.","Create a custom IAM role that only allows pulling from specific repositories.","Manually review all Kubernetes manifests before they are deployed."],correct:0,explanation:"This is the exact use case for the `container.trustedImageProjects` Organization Policy constraint. It allows you to specify a list of Google Cloud project IDs from which container images can be deployed. Any attempt to deploy a pod with an image from an untrusted source will be blocked.",wrongExplanations:{1:"While an admission controller can also enforce this, the Organization Policy is a higher-level, simpler, and fully managed Google Cloud native solution.",2:"IAM controls permissions for *users* to perform actions, but it does not control the content (like the image source) of the resources they create.",3:"Manual review is not a scalable or reliable security control."}},{id:438,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You need to upload a very large file (50 GB) to a Cloud Storage bucket from a machine with an unstable internet connection. What is the most reliable way to perform this upload?",options:["Use `gsutil` with the `-m` (parallel) and resumable upload feature.","Use the Cloud Console's file upload button.","Write a custom script to split the file into 1 GB chunks and upload them individually.","Mount the bucket using Cloud Storage FUSE and copy the file."],correct:0,explanation:"`gsutil` is the command-line tool for Cloud Storage. By default, it performs resumable uploads for large files. If the upload is interrupted, you can run the same command again, and it will resume from where it left off. The `-m` flag enables parallel uploads, which can significantly speed up the process for large files by uploading multiple chunks simultaneously.",wrongExplanations:{1:"The Cloud Console upload is not designed for very large files or unstable connections and is likely to fail and require a full restart.",2:"Manually splitting the file is complex. `gsutil` handles this chunking and reassembly for you automatically with its parallel upload feature.",3:"Using FUSE over an unstable connection for a large file copy would be very slow and prone to errors."}},{id:439,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You want to create an alert that notifies you if the 99th percentile latency of your external Application Load Balancer exceeds 500ms. Which metric should you use for the alerting policy?",options:["The load balancer's `request_latencies` metric, using percentile aggregation.","The backend instances' `cpu/utilization` metric.","The load balancer's `request_count` metric.","A custom metric that your application writes to the Monitoring API."],correct:0,explanation:"The external Application Load Balancer automatically exports detailed metrics to Cloud Monitoring, including a distribution metric for request latencies. When creating an alerting policy, you can choose to base the condition on a specific percentile of this distribution (e.g., 50th, 95th, 99th), which is the correct way to monitor tail latency.",wrongExplanations:{1:"CPU utilization is an indirect and often unreliable indicator of request latency.",2:"Request count tells you the volume of traffic, not the performance of individual requests.",3:"While you could create a custom metric, it's unnecessary work. The load balancer provides this metric out of the box."}},{id:440,domain:"Section 4: Configuring access and security",subdomain:"Secret Manager",question:"Your application needs to access a new version of a database password that is stored in Secret Manager. The application code already fetches the secret by its name. What is the simplest way to roll out the new password to the application without changing its code?",options:["Add the new password as a new version of the secret and configure the application's secret reference to point to the 'latest' alias.","Create a new secret with the new password and update the IAM policy.","Delete the old version of the secret.","Store the new password in the application's environment variables."],correct:0,explanation:"Secret Manager supports versioning for secrets. You can add a new password as a new version. By having your application reference the secret using the 'latest' alias, it will automatically pick up the new version the next time it fetches the secret. This allows for seamless secret rotation without code changes or redeployments.",wrongExplanations:{1:"Creating a completely new secret would require changing the application's code to reference the new secret's name.",2:"Deleting the old version before all instances of the application have picked up the new one would cause an outage.",3:"Using environment variables is less secure than using Secret Manager, as they can be more easily exposed."}},{id:441,domain:"Section 1: Setting up a cloud solution environment",subdomain:"gcloud",question:"You are logged into Cloud Shell and want to see which Google Cloud project is currently configured as the default for your `gcloud` commands. Which command should you run?",options:["gcloud config get-value project","gcloud projects list","gcloud auth list","gcloud info"],correct:0,explanation:"The `gcloud config` command group is used to manage your gcloud configurations. The `get-value` subcommand allows you to retrieve the value of a specific property. To get the currently configured project, you would ask for the `project` property.",wrongExplanations:{1:"`gcloud projects list` shows all projects you have access to, not the one that is currently configured as the default.",2:"`gcloud auth list` shows the accounts you are logged in with, not the default project.",3:"`gcloud info` provides general information about your gcloud installation, which includes the project but `get-value` is the more direct command."}},{id:442,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"GKE",question:"You are deploying a containerized application to a GKE cluster. The application needs to mount a persistent disk that can be read and written to by a single pod at a time. The data must persist even if the pod is deleted and recreated. Which Kubernetes objects do you need to create?",options:["A PersistentVolumeClaim and a Deployment that references the claim.","A StatefulSet with a volumeClaimTemplates section.","An Ephemeral Volume attached to the pod.","A hostPath volume."],correct:0,explanation:"This describes the standard 'ReadWriteOnce' access mode for persistent storage in Kubernetes. The developer creates a PersistentVolumeClaim (PVC) to request storage. The GKE cluster will dynamically provision a Persistent Disk to satisfy this claim. The Deployment can then reference the PVC in its volume mounts, allowing the pods to use the disk. The data will persist independently of the pod's lifecycle.",wrongExplanations:{1:"A StatefulSet is used for stateful applications where each pod needs its own unique, persistent volume. For a single shared volume, a Deployment with a PVC is simpler.",2:"An Ephemeral Volume's lifecycle is tied to the pod. If the pod is deleted, the data is lost.",3:"A hostPath volume mounts a directory from the underlying node, which is not durable storage. If the pod is rescheduled to a different node, the data will not be available."}},{id:443,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Managing Compute",question:"You have a MIG with autohealing enabled. The health check is configured to check for a response on TCP port 80 every 30 seconds. A VM in the group fails its health check three consecutive times. What action will the MIG take?",options:["It will automatically delete the unhealthy VM and create a new one to replace it.","It will stop the VM and attempt to restart it.","It will send an alert to Cloud Monitoring but take no action.","It will remove the VM from the load balancer's backend service but leave it running."],correct:0,explanation:"The purpose of autohealing is to maintain the health and availability of the application. When a VM is confirmed to be unhealthy (by failing a specified number of consecutive health checks), the MIG's autohealing policy will automatically recreate the instance. This involves deleting the failed VM and creating a new one based on the same instance template.",wrongExplanations:{1:"The standard autohealing action is to recreate, not just restart, the instance to ensure it comes up in a clean state.",2:"The MIG takes direct action. Sending an alert is a separate function you can configure in Cloud Monitoring, but it's not part of the autohealing process itself.",3:"While the load balancer will stop sending traffic to an unhealthy instance, the MIG's autohealing policy will go further and actually replace the instance."}},{id:444,domain:"Section 4: Configuring access and security",subdomain:"Security Command Center",question:"Security Command Center has reported a 'Public Cloud Storage ACL' finding for one of your buckets. What does this finding indicate?",options:["The bucket has an Access Control List (ACL) that grants access to `allUsers` or `allAuthenticatedUsers`, making it publicly accessible.","The bucket is not encrypted with a Customer-Managed Encryption Key (CMEK).","The bucket does not have Uniform Bucket-Level Access enabled.","The bucket does not have logging enabled for data access."],correct:0,explanation:"Security Health Analytics, a built-in service in Security Command Center, automatically scans for common misconfigurations. One of the most critical findings is when a bucket is made public through legacy ACLs. The 'Public Cloud Storage ACL' finding specifically identifies buckets that grant broad access to `allUsers` (anyone on the internet) or `allAuthenticatedUsers` (any authenticated Google account).",wrongExplanations:{1:"While CMEK usage might be a best practice, this specific finding is about public access, not the encryption method.",2:"Not using UBLA is a related misconfiguration that can lead to this, but the finding's title directly refers to the public ACL itself.",3:"While access logging is a good security practice, this finding is about access control, not auditing."}},{id:445,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Functions",question:"You have written a Cloud Function that is triggered by messages on a Pub/Sub topic. The function processes the message and can sometimes take up to 15 minutes to complete. When you deploy the function, it fails with a timeout error. What is the most likely cause?",options:["The function's timeout setting is at its default value, which is less than 15 minutes.","The Pub/Sub topic has too many messages.","The service account for the function does not have the correct permissions.","The function has a memory leak."],correct:0,explanation:"Cloud Functions have a configurable timeout, which defaults to 60 seconds. The maximum allowed timeout for an HTTP-triggered function is 9 minutes (or 60 minutes for Cloud Functions 2nd gen and event-driven functions). Since the processing takes 15 minutes, it is exceeding the default timeout, and you must explicitly increase it during deployment.",wrongExplanations:{1:"A high volume of messages would just trigger more concurrent instances of the function; it wouldn't cause a single invocation to time out.",2:"A permissions error would likely cause the function to fail immediately with an access denied error, not a timeout.",3:"A memory leak would likely cause the function to crash with an out-of-memory error, not a timeout."}},{id:446,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"How are IAM policies inherited through the Google Cloud resource hierarchy?",options:["Policies are inherited downwards. A policy set at a Folder applies to all Projects and resources within that Folder.","Policies are inherited upwards. A policy set on a Project applies to its parent Folder and the Organization.","Policies are not inherited; they must be applied at each level explicitly.","Only policies set at the Organization level are inherited."],correct:0,explanation:"The resource hierarchy allows for top-down inheritance of policies. If you grant a user the Viewer role at the Organization level, they will have Viewer permissions on all Folders and Projects within that Organization. Similarly, a policy on a Folder applies to all Projects under it. This allows for efficient management of broad access controls.",wrongExplanations:{1:"Inheritance is strictly top-down, not bottom-up.",2:"Inheritance is a key feature of the hierarchy.",3:"Policies can be set and inherited from any level (Organization, Folder, Project)."}},{id:447,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You need to perform maintenance on a specific node in your GKE Standard cluster. You want to safely evict all the user pods running on that node and prevent new pods from being scheduled on it, without terminating the node itself. Which `kubectl` command should you use first?",options:["kubectl drain [NODE_NAME]","kubectl cordon [NODE_NAME]","kubectl delete node [NODE_NAME]","kubectl stop node [NODE_NAME]"],correct:0,explanation:"The `kubectl drain` command is the standard, safe procedure for taking a node out of service. It performs two actions: first, it marks the node as unschedulable (cordons it), so no new pods are placed there. Second, it respects pod disruption budgets and gracefully evicts the existing pods, allowing them to be rescheduled on other available nodes.",wrongExplanations:{1:"`kubectl cordon` only marks the node as unschedulable; it does not evict the existing pods.",2:"`kubectl delete node` removes the node object from the Kubernetes API, which can lead to GKE's cluster manager recreating the underlying VM. It is a destructive action.",3:"`stop node` is not a valid `kubectl` command."}},{id:448,domain:"Section 4: Configuring access and security",subdomain:"VPC Security",question:"You have a VPC network with a default-allow-internal firewall rule. You need to create a more restrictive security posture where two subnets, `subnet-a` and `subnet-b`, cannot communicate with each other at all, but both can still communicate with a third shared `subnet-c`. What is the most effective way to do this?",options:["Create a deny-all firewall rule with a priority higher than the default-allow-internal rule, targeting traffic between `subnet-a` and `subnet-b` using network tags or service accounts.","Move `subnet-a` and `subnet-b` to separate VPC networks.","Delete the default-allow-internal firewall rule.","Configure VPC Service Controls to isolate the subnets."],correct:0,explanation:"Firewall rules are evaluated by priority. The default-allow-internal rule has a low priority (65534). To override it for specific traffic, you create a new rule with a higher priority (a lower number, e.g., 1000). This new rule would have a 'deny' action, with a source of one subnet and a destination of the other, effectively blocking communication between them while not affecting their ability to communicate with other subnets.",wrongExplanations:{1:"Moving subnets to new VPCs is a very disruptive and complex solution for a simple traffic isolation requirement.",2:"You cannot delete the default firewall rules. Also, deleting it would break all internal communication, which is not what is required.",3:"VPC Service Controls protect managed services; they do not control network traffic between subnets within a VPC."}},{id:449,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You are creating a new Compute Engine instance and want to ensure that it is created on a physical server that is not shared with any other customer's VMs. Which feature should you use?",options:["Sole-tenant nodes","Shielded VMs","Confidential Computing","A compute-optimized machine type"],correct:0,explanation:"Sole-tenant nodes are physical Compute Engine servers dedicated to your project. They ensure that your VMs have exclusive access to the underlying hardware and are not co-located with workloads from other customers. This is often used for security, compliance (e.g., licensing), or performance isolation requirements.",wrongExplanations:{1:"Shielded VMs provide verifiable integrity of the boot process, but they still run on multi-tenant hardware.",2:"Confidential Computing encrypts data while it is in use (in memory), but the VMs still run on multi-tenant hardware.",3:"Machine type determines the vCPU and memory configuration, not the tenancy of the underlying physical host."}},{id:450,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cloud Shell",question:"What is a key feature of Google Cloud Shell?",options:["It is a browser-based shell environment with `gcloud` and other common utilities pre-installed and authenticated.","It is a downloadable client for managing Google Cloud from your local machine.","It is a dedicated, persistent VM for running production workloads.","It is a tool for monitoring and debugging applications."],correct:0,explanation:"Cloud Shell provides a temporary Compute Engine VM instance accessible directly from your browser. It comes with the `gcloud` CLI, Docker, Terraform, `kubectl`, and many other development tools pre-installed. Your authentication credentials and project context are automatically configured, making it the quickest way to start managing your Google Cloud resources.",wrongExplanations:{1:"The downloadable client is the Google Cloud SDK. Cloud Shell is browser-based.",2:"Cloud Shell is for interactive, administrative tasks, not for hosting production applications. Its instance is ephemeral.",3:"Tools for monitoring and debugging are Cloud Monitoring, Logging, etc. Cloud Shell is a management environment."}},{id:451,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"You want to find all log entries from a specific Compute Engine instance. Which field in the Logs Explorer should you filter by?",options:["resource.labels.instance_id","source_instance","jsonPayload.instanceName","gce_instance"],correct:0,explanation:"In Cloud Logging, logs from Google Cloud services have a `resource` field that describes the source of the log. For Compute Engine, the resource type is `gce_instance`, and it has labels that identify the specific instance, such as `instance_id` and `zone`. Filtering on `resource.labels.instance_id` is the standard and correct way to isolate logs from a particular VM.",wrongExplanations:{1:"`source_instance` is not a standard field.",2:"`jsonPayload` contains the actual log message content. While it might contain the instance name, filtering on the resource label is the correct way to query the log's metadata.",3:"`gce_instance` is the resource *type*, not the field you would filter on for a specific instance ID."}},{id:452,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"You have a Cloud Storage bucket that will store data that is accessed frequently for the first 30 days, then infrequently for the next 90 days, and then rarely after that. You want to automatically manage the storage costs of this data. What should you configure?",options:["A lifecycle management policy on the bucket.","A Cloud Function to move the objects between storage classes.","Object versioning.","A separate bucket for each access pattern."],correct:0,explanation:"Cloud Storage lifecycle management is a feature that lets you define rules to automatically take action on objects based on their age or other conditions. You can create a rule to transition objects from Standard to Nearline storage after 30 days, and then from Nearline to Coldline or Archive after another 90 days. This automates the cost optimization process.",wrongExplanations:{1:"A Cloud Function is a valid but overly complex solution. Lifecycle policies are the built-in, declarative way to achieve this.",2:"Object versioning is for protecting against deletes/overwrites, not for managing storage class and cost.",3:"Using separate buckets is a manual and difficult-to-manage approach. A single bucket with a lifecycle policy is the correct design."}},{id:453,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"What is the purpose of the `iam.serviceAccountUser` role?",options:["It allows a user to run jobs and act as (impersonate) a service account.","It allows a user to create and manage service accounts.","It is a role that you assign to service accounts to let them use services.","It allows a user to manage the IAM policies of a service account."],correct:0,explanation:"The Service Account User role is a critical part of service account security. It does not grant permissions to manage the service account itself, but rather to *use* it. A user with this role can impersonate the service account to get short-lived credentials, or they can attach the service account to a resource like a Compute Engine VM.",wrongExplanations:{1:"The role for managing service accounts is `iam.serviceAccountAdmin`.",2:"You assign roles like `Storage Object Viewer` to a service account to let it use other services. The `serviceAccountUser` role is granted *to users*, not to the service account itself.",3:"Managing the IAM policy of a service account is done by the `iam.serviceAccountAdmin` role."}},{id:454,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"VPC Networking",question:"You are creating a new VPC network. What is a key reason to choose a 'Custom mode' VPC over an 'Auto mode' VPC?",options:["Custom mode allows you to define your own IP address ranges for subnets, which is essential for avoiding conflicts with on-premises networks.","Custom mode automatically creates a set of useful firewall rules for you.","Custom mode is required for using Compute Engine.","Custom mode provides higher network throughput."],correct:0,explanation:"Auto mode VPCs automatically create a subnet in each Google Cloud region with a predefined, non-configurable IP range. This can easily lead to IP range overlap if you need to connect your VPC to an on-premises network via VPN or Interconnect. Custom mode gives you full control over which subnets are created and what their IP ranges are, which is a best practice for all production environments.",wrongExplanations:{1:"Auto mode creates the default firewall rules. In Custom mode, you start with a minimal set of rules and must create your own.",2:"Both modes support Compute Engine. Auto mode is often used by beginners for simplicity.",3:"The VPC mode does not affect the available network throughput."}},{id:455,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"An alerting policy has been configured with a condition and a notification channel. The condition of the alert is met, and an incident is created in Cloud Monitoring. After 24 hours, the condition is still met. What is the default behavior of the alerting policy?",options:["The incident will remain open, but no new notifications will be sent unless the incident is closed and re-opened.","A new notification will be sent every 5 minutes until the incident is acknowledged.","The incident will automatically close after 24 hours.","A new, separate incident will be created."],correct:0,explanation:"By default, an alerting policy sends a notification only when an incident is first opened. It will not send repeated notifications for an ongoing, open incident. The incident will stay open as long as the condition is met. To get repeated notifications, you would need to configure that explicitly in the policy's documentation/notification settings.",wrongExplanations:{1:"This describes a 'nagging' or re-notification feature, which is not the default behavior.",2:"Incidents only close when the condition is no longer met or if they are manually closed. There is a maximum auto-close duration of 7 days if data stops arriving, but not 24 hours for an active condition.",3:"A new incident is only created if the previous one was closed and the condition is met again."}},{id:456,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"Your company has negotiated a special discounted rate for a specific Google Cloud service. What is the name for this type of discount?",options:["A Private Pricing Agreement.","A Sustained Use Discount.","A Committed Use Discount.","A Promotional Credit."],correct:0,explanation:"A Private Pricing Agreement (or Private Offer) is a custom deal negotiated between a customer and Google (or a partner via the Marketplace) that provides a special price for a specific product, which is not publicly available.",wrongExplanations:{1:"Sustained Use Discounts are automatic discounts applied to Compute Engine for running instances for a significant portion of the month.",2:"Committed Use Discounts are discounts you receive in exchange for committing to a certain level of resource usage (e.g., vCPUs or RAM) for a 1 or 3-year term.",3:"Promotional credits are one-time credits applied to an account, often for free trials."}},{id:457,domain:"Section 4: Configuring access and security",subdomain:"Cloud KMS",question:"What is the primary function of Cloud Key Management Service (KMS)?",options:["To manage cryptographic keys and perform encryption and decryption operations.","To store and manage service account keys.","To manage SSH keys for Compute Engine instances.","To store SSL certificates for load balancers."],correct:0,explanation:"Cloud KMS is a centralized service for creating, importing, managing, and using cryptographic keys. Other Google Cloud services (like Cloud Storage, BigQuery, and Persistent Disk) can integrate with KMS to use these keys for encrypting data (a pattern known as Customer-Managed Encryption Keys or CMEK). You can also call the KMS API directly to perform encryption/decryption.",wrongExplanations:{1:"Service account keys are managed in IAM, although the best practice is to store sensitive keys in a service like Secret Manager, which can use KMS for encryption.",2:"SSH keys are managed as part of Compute Engine's metadata.",3:"SSL certificates are managed in Certificate Manager or directly on the load balancer."}},{id:458,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Artifact Registry",question:"Your organization is standardizing on using Artifact Registry to store all of its container images. You need to create a new repository to store Docker images for your web-app. What type of repository should you create?",options:["A Docker repository.","A Maven repository.","An npm repository.","A generic artifact repository."],correct:0,explanation:"Artifact Registry is a universal repository manager that supports multiple package formats. When you create a repository, you must specify the format of the artifacts it will store. For Docker container images, you must choose the 'Docker' format. This configures the repository to understand the Docker V2 API for `docker push` and `docker pull` commands.",wrongExplanations:{1:"Maven is a repository format for Java artifacts.",2:"npm is a repository format for Node.js packages.",3:"While Artifact Registry is a generic manager, you must specify the format for each repository you create."}},{id:459,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Health",question:"You have received an email from Google Cloud stating that one of your Compute Engine instances is scheduled for host maintenance in the next week. The instance's live migration setting is turned on. What do you need to do?",options:["No action is required. Google will automatically live migrate the instance to a new host with no downtime.","You must manually stop and restart the instance during the maintenance window.","You need to migrate the instance's persistent disk to a new host.","You must request a maintenance extension to avoid downtime."],correct:0,explanation:"Live migration is a feature of Compute Engine that allows Google to move a running VM from one physical host to another without any impact on the application or the need for a reboot. When live migration is enabled (the default for most instances), Google handles the maintenance automatically, and you typically do not need to take any action.",wrongExplanations:{1:"This is only necessary if live migration is turned off for the instance.",2:"The persistent disk is network-attached and does not need to be manually migrated; it will be reattached automatically after the live migration.",3:"An extension is not necessary because live migration is designed to be a zero-downtime event."}},{id:460,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"AlloyDB",question:"Which of the following is a primary advantage of using AlloyDB for PostgreSQL over a self-managed PostgreSQL instance on a Compute Engine VM?",options:["Fully managed service with automated backups, patching, and high availability.","Complete control over the operating system and database configuration.","Lower cost for small, non-critical workloads.","Ability to install any third-party PostgreSQL extension."],correct:0,explanation:"AlloyDB is a fully managed database service. This means Google handles the complex and time-consuming administrative tasks such as provisioning infrastructure, setting up high availability, managing backups, and applying security patches. This allows teams to focus on their application rather than on database administration.",wrongExplanations:{1:"Complete control is an advantage of a self-managed instance, not AlloyDB. The trade-off for a managed service is giving up some control for operational ease.",2:"For very small workloads, a small Compute Engine VM might be cheaper, but it comes with a high operational cost. AlloyDB is designed for performance and availability, which comes at a higher price point than a minimal VM.",3:"While AlloyDB supports many popular extensions, a self-managed instance gives you the freedom to install any extension, which might be a requirement for some legacy applications. This is a point of flexibility for self-managed, not an advantage of AlloyDB."}},{id:461,domain:"Section 4: Configuring access and security",subdomain:"Cloud NGFW policies",question:"A network administrator wants to create a global network firewall policy that can be applied to all VPCs across the entire organization. What should they configure?",options:["A hierarchical firewall policy at the Organization root.","A network firewall policy in each VPC.","A Cloud Armor policy.","A Shared VPC network."],correct:0,explanation:"Hierarchical firewall policies are designed for this exact use case. They are configured at the Organization or Folder level in the resource hierarchy and are inherited by all VPCs below them. This allows for centralized enforcement of baseline security rules (e.g., blocking known malicious IPs) across the entire enterprise.",wrongExplanations:{1:"Configuring policies in each VPC is decentralized and does not meet the requirement for a single, global policy.",2:"Cloud Armor is a WAF for load balancers, not a general-purpose firewall for VPCs.",3:"A Shared VPC centralizes network management but does not in itself apply a global firewall policy."}},{id:462,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Gemini Cloud Assist",question:"You are trying to diagnose a networking issue between two VMs. You suspect a firewall rule is blocking traffic. How can Gemini Cloud Assist help you troubleshoot this in the Cloud Console?",options:["Ask Gemini: 'Why can't my VM named 'frontend-1' connect to 'backend-1' on port 5432?'","Run `gcloud compute ssh` into the VM.","Look at the VPC flow logs.","Ask Gemini to write a network diagnostic script."],correct:0,explanation:"Gemini Cloud Assist is context-aware. When troubleshooting in the console, you can ask it natural language questions about your resources. It can analyze your current project's configuration, including firewall rules and network tags, to provide a specific explanation for why traffic might be blocked and suggest the exact firewall rule change needed to fix it.",wrongExplanations:{1:"SSHing into the VM allows you to test connectivity (e.g., with `ping` or `telnet`), but it doesn't directly tell you *why* it's failing at the cloud network level.",2:"VPC flow logs are a valid but more complex troubleshooting tool. You would have to manually search and interpret the logs. Gemini can automate this analysis for you.",3:"While Gemini can write scripts, its primary troubleshooting value in the console is its ability to directly analyze your project's configuration and provide an immediate answer."}},{id:463,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Query Insights",question:"When using Query Insights, you notice that a particular query has a very high 'Rows Scanned' value compared to the 'Rows Returned' value. What does this indicate?",options:["The query is inefficient and is reading much more data than necessary, likely due to a missing index or un-optimized `WHERE` clause.","The query is performing well because it is scanning data quickly.","The database table is well-clustered.","The user who ran the query has overly broad permissions."],correct:0,explanation:"A large discrepancy between rows scanned and rows returned is a key indicator of an inefficient query. It means the database had to read a large number of rows from disk to find the small number of rows that actually matched the query's conditions. This often points to a full table scan where a more efficient index scan could have been used.",wrongExplanations:{1:"High scan count is a sign of poor performance, not good performance.",2:"If the table were well-clustered for this query, the number of rows scanned would be much closer to the number of rows returned.",3:"Query Insights is a performance tool; it provides no information about IAM permissions."}},{id:464,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"Your team is building a new application and prefers a flexible, schema-less data model. The application will store JSON-like documents and needs to scale automatically from zero to millions of users. Which database service is the best choice?",options:["Firestore","Cloud SQL","Cloud Spanner","Memorystore"],correct:0,explanation:"Firestore is a fully managed, serverless, NoSQL document database. It is designed for storing, syncing, and querying data for mobile, web, and IoT applications. Its key features include a flexible, JSON-like data model, automatic scaling, and real-time data synchronization, which perfectly match the requirements.",wrongExplanations:{1:"Cloud SQL is a relational database and requires a predefined schema. It does not use a document-based data model.",2:"Cloud Spanner is a relational database with a schema, not a schema-less document store.",3:"Memorystore is an in-memory key-value store, not a durable document database suitable for being the primary data store for an application."}},{id:465,domain:"Section 4: Configuring access and security",subdomain:"Security Command Center",question:"You want to proactively scan your container images stored in Artifact Registry for known security vulnerabilities (CVEs) before they are deployed to GKE. Which feature, which integrates with Security Command Center, should you enable?",options:["Container Analysis","Cloud Armor","Binary Authorization","Web Security Scanner"],correct:0,explanation:"Container Analysis is the service that provides vulnerability scanning for container images in Artifact Registry or Container Registry. It scans your images for known CVEs in OS packages and application libraries. The findings are then surfaced in the Security Command Center dashboard, allowing you to identify and remediate vulnerabilities early in the development lifecycle.",wrongExplanations:{1:"Cloud Armor is a WAF for protecting running applications from network attacks; it does not scan container images at rest.",2:"Binary Authorization is a deployment-time security control that *enforces* policies based on attestations (e.g., ensuring an image has been scanned and has no critical vulnerabilities), but the scanning itself is done by Container Analysis.",3:"Web Security Scanner is for scanning running web applications for vulnerabilities like XSS, not for scanning container images."}},{id:466,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Deployment Manager",question:"You want to define a set of Google Cloud resources (a VM, a firewall rule, and a Cloud Storage bucket) as code in a declarative template file. You want to use Google's native Infrastructure as Code service. Which service should you use?",options:["Cloud Deployment Manager","Terraform","Config Connector","Cloud Build"],correct:0,explanation:"Cloud Deployment Manager is Google Cloud's native infrastructure deployment service that allows you to specify all the resources needed for your application in a declarative format using YAML. You can create templates and reuse them to provision infrastructure in a repeatable and predictable way.",wrongExplanations:{1:"Terraform is a very popular third-party IaC tool, but the question specifically asks for Google's *native* service.",2:"Config Connector is for managing GCP resources via the Kubernetes API, which is a different workflow.",3:"Cloud Build is a CI/CD service for building and deploying code, not for declarative infrastructure management."}},{id:467,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Load balancing",question:"You have an application running on GKE that needs to be exposed to the internet via HTTP. You want to manage traffic using native Kubernetes resources. Which resource should you create to have GKE automatically provision and configure an external Application Load Balancer?",options:["A Kubernetes Ingress object.","A Kubernetes Service of type LoadBalancer.","A Cloud Armor policy.","A Gateway object from the Gateway API."],correct:0,explanation:"In GKE, creating a Kubernetes Ingress object is the standard way to expose HTTP and HTTPS routes from outside the cluster to services within the cluster. The GKE Ingress controller watches for these Ingress objects and automatically provisions and configures a Google Cloud external Application Load Balancer to handle the routing.",wrongExplanations:{1:"Creating a Service of type LoadBalancer will provision an external *Network* Load Balancer (Layer 4), which is not suitable for HTTP routing and does not provide features like path-based routing or SSL termination.",2:"A Cloud Armor policy attaches to a load balancer but does not create it.",3:"The Gateway API is a newer, more expressive set of APIs for service networking, but Ingress is the long-standing and more commonly tested resource for this function."}},{id:468,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"You want to be alerted if your project's daily spending, as reported by Cloud Billing, exceeds $100. How can you set up this alert?",options:["This cannot be done in Cloud Monitoring; you must create a budget and alert in Cloud Billing.","Create an alerting policy in Cloud Monitoring based on the `billing/quota` metric.","Create a logs-based metric from the billing audit logs.","Write a custom application that calls the Billing API every minute and sends an email."],correct:0,explanation:"While Cloud Monitoring is the primary tool for performance metrics, cost control and alerting is the specific responsibility of the Cloud Billing service. The correct and simplest way to achieve this is to go to the Billing section, create a budget for your project, and set an alert threshold to send a notification when spending reaches a certain amount.",wrongExplanations:{1:"There is no such metric for direct cost alerting in Cloud Monitoring.",2:"Billing data is not typically sent to logs in a way that would be suitable for creating logs-based metrics for alerting.",3:"This is an overly complex, custom solution for a feature that is provided out-of-the-box by Cloud Billing."}},{id:469,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"What is the purpose of a Google Group in the context of IAM?",options:["To grant a set of permissions to multiple users at once, simplifying user management.","To create a shared email inbox for a team.","To define a custom set of permissions.","To act as a service account for a group of applications."],correct:0,explanation:"Using Google Groups is a best practice for managing IAM policies. Instead of assigning roles to individual users one by one, you can create a group (e.g., `gcp-project-admins@yourcompany.com`), grant the necessary roles to that group, and then simply add or remove users from the group. This decouples user management from policy management and is much more scalable.",wrongExplanations:{1:"While a Google Group does provide a mailing list address, its primary function in IAM is as a collection of principals for role assignment.",2:"A custom set of permissions is a custom role, not a group.",3:"A group is a collection of user accounts, not a non-human identity like a service account."}},{id:470,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You need to run a high-performance computing (HPC) workload that requires the lowest possible network latency between the participating VMs. Which feature should you use when creating the VMs?",options:["Create the VMs as a compact placement policy.","Place the VMs in a sole-tenant node group.","Use a premium network service tier.","Choose VMs with the highest vCPU count."],correct:0,explanation:"A compact placement policy is a feature that instructs Compute Engine to place the specified VMs on physical hardware that is in close proximity to each other. This reduces the network distance between the VMs, resulting in significantly lower inter-VM network latency, which is critical for tightly coupled HPC applications.",wrongExplanations:{1:"Sole-tenancy provides hardware isolation, but it does not guarantee that the nodes within the group are physically close to each other for low latency.",2:"The premium network tier optimizes for traffic between your VMs and the internet, not for latency *between* VMs within a zone.",3:"A high vCPU count provides more processing power but does not affect the network latency between instances."}},{id:471,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Organization policies",question:"A security administrator wants to prevent developers from creating service account keys, as they want to enforce the best practice of attaching service accounts to resources directly. Which Organization Policy constraint should they enforce?",options:["`iam.disableServiceAccountKeyCreation`","`iam.allowedServiceAccountKeys`","`compute.vmExternalIpAccess`","`iam.serviceAccountUser`"],correct:0,explanation:"The `iam.disableServiceAccountKeyCreation` constraint is designed for this exact purpose. When this boolean constraint is enforced on a project, folder, or organization, it blocks all API calls that attempt to create new external service account keys, forcing developers to use more secure methods like impersonation or attaching service accounts to VMs.",wrongExplanations:{1:"This is not a valid constraint name.",2:"This constraint restricts the creation of VMs with external IPs; it has nothing to do with service account keys.",3:"This is an IAM role, not an Organization Policy constraint."}},{id:472,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Logging",question:"What is a log sink inclusion filter used for?",options:["To specify which log entries should be exported by the sink.","To specify which log entries should be excluded from the sink.","To define the IAM permissions for the sink's destination.","To set the retention period for the exported logs."],correct:0,explanation:"A log sink routes log entries to a destination. The inclusion filter uses the Logging query language to define which logs should be captured and routed. For example, you could create a filter `severity >= ERROR` to create a sink that only exports logs with a severity of ERROR or higher.",wrongExplanations:{1:"To exclude logs, you use an exclusion filter. The inclusion filter defines what to send.",2:"IAM permissions for the destination are managed on the sink's service account, not in the filter.",3:"Retention is configured at the destination (e.g., the Cloud Storage bucket), not in the sink's filter."}},{id:473,domain:"Section 4: Configuring access and security",subdomain:"Identity-Aware Proxy (IAP)",question:"You have configured IAP to protect a web application running on App Engine. What happens when a user who is not on the IAP access list tries to access the application's URL?",options:["IAP will block the request and show the user a Google-branded access denied screen, without the request ever reaching the application.","The request will reach the application, which is then responsible for denying access.","The user will be prompted to request access from the project owner.","The user will see a generic 404 Not Found error."],correct:0,explanation:"IAP acts as an authenticating and authorizing proxy. It sits in the request path before your application. It intercepts all requests, checks the user's identity against the IAM policy, and if the user is not authorized, it blocks the request immediately. The application code is never executed for unauthorized users.",wrongExplanations:{1:"This is incorrect. The core benefit of IAP is that it prevents unauthorized requests from ever reaching your backend.",2:"This is not a feature of IAP. Access management is done through standard IAM role grants.",3:"The user will see a 403 Access Denied error, not a 404 Not Found error."}},{id:474,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Database selection",question:"You need to store a massive dataset (terabytes to petabytes) of wide-column data, such as time-series data from IoT devices. The workload requires very high throughput for both reads and writes, and low latency is critical. Which database is designed for this use case?",options:["Cloud Bigtable","BigQuery","Cloud SQL","Firestore"],correct:0,explanation:"Cloud Bigtable is a fully managed, scalable NoSQL wide-column database. It is the same database that powers many core Google services like Search and Maps. It's specifically designed for large analytical and operational workloads with high throughput and low latency, making it ideal for time-series, IoT, and financial data.",wrongExplanations:{1:"BigQuery is an analytical data warehouse designed for SQL queries and analysis, not for low-latency point reads and writes required by a live application.",2:"Cloud SQL is a relational database and cannot handle the scale or data model required.",3:"Firestore is a document database and is not optimized for the massive scale and wide-column data model that Bigtable excels at."}},{id:475,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Error Reporting",question:"Cloud Error Reporting has grouped several different error messages from your application into a single error group. What is the most likely reason for this?",options:["The errors all originated from the same location in the application's source code (i.e., they have similar stack traces).","The errors all occurred at the same time.","The errors all have the same severity level.","The errors all came from the same user."],correct:0,explanation:"The primary mechanism Error Reporting uses for grouping is by analyzing the stack trace. It identifies the root cause of an exception and groups different error messages that share a similar causal stack trace. This is powerful because a single bug (e.g., a null pointer exception) can manifest with slightly different error messages, and Error Reporting correctly identifies them as a single issue.",wrongExplanations:{1:"Time is a factor in viewing errors, but it's not the primary grouping mechanism.",2:"Severity is not used for grouping.",3:"User information is not used for grouping."}},{id:476,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Billing",question:"You are the billing administrator for your organization. A team has created a new project, but they are unable to enable any APIs or create resources because it is not linked to a billing account. What should you do in the Cloud Console?",options:["Navigate to the Billing section, select the project, and link it to your organization's active billing account.","Grant the project's creator the 'Billing Account Administrator' role.","Create a new billing account just for this project.","Submit a support ticket to Google to have the project linked."],correct:0,explanation:"A project must be associated with an active billing account before it can use any billable Google Cloud services. As the billing administrator, the standard procedure is to go to the Billing Account management page, find the unbilled project, and explicitly link it to the appropriate billing account.",wrongExplanations:{1:"Granting a role to the user doesn't link the project. The linking is a separate administrative action.",2:"Creating a new billing account is unnecessary if the organization already has one. This would also complicate billing management.",3:"This is a self-service action that a billing administrator can and should perform directly."}},{id:477,domain:"Section 4: Configuring access and security",subdomain:"VPC Service Controls",question:"Which of the following services can be protected by a VPC Service Controls perimeter?",options:["BigQuery, Cloud Storage, and Cloud SQL","Compute Engine and Google Kubernetes Engine","Cloud DNS and Cloud Load Balancing","Cloud Identity and IAM"],correct:0,explanation:"VPC Service Controls are designed to protect Google-managed services that store data and have public API endpoints. This includes major data services like BigQuery, Cloud Storage, Pub/Sub, Cloud SQL, and Spanner. It does not apply to foundational infrastructure like Compute Engine or networking services, which are controlled by VPC firewalls and IAM.",wrongExplanations:{1:"Compute Engine and GKE are not directly protected by perimeters. You protect the data services they access.",2:"These are networking services and are not protected by perimeters.",3:"These are identity and access management services and are not protected by perimeters."}},{id:478,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Compute Engine",question:"You have a Compute Engine instance that runs a nightly batch job. You want to ensure the instance is only running when it is needed to save costs. What is the most efficient way to automate this?",options:["Use Cloud Scheduler to send a message to a Pub/Sub topic that triggers a Cloud Function to start and stop the instance.","Keep the instance running at all times but use an E2 machine type.","Manually start and stop the instance every day from the Cloud Console.","Configure an autoscaler to set the instance group size to 0 when CPU is low."],correct:0,explanation:"This is a common serverless automation pattern. Cloud Scheduler is a managed cron service that can trigger an event on a schedule. You can have it publish a message to two different Pub/Sub topics (e.g., `start-instance` and `stop-instance`) at the desired times. These topics can then trigger two simple Cloud Functions that use the Compute Engine API to start or stop the target instance. This is a fully automated and serverless solution.",wrongExplanations:{1:"Keeping the instance running is the opposite of the goal, which is to save costs by only running it when needed.",2:"Manual operations are not automated and are prone to being forgotten.",3:"An autoscaler is for managing a group of instances based on load, not for scheduling a single instance to turn on and off at specific times."}},{id:479,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"GKE operations",question:"You are trying to troubleshoot a pod in GKE that is stuck in the `ImagePullBackOff` state. What is the most likely cause of this issue?",options:["The GKE node cannot pull the container image specified in the pod manifest, likely due to an incorrect image name or insufficient permissions.","The pod does not have enough CPU or memory resources requested to start.","The cluster's CNI (Container Network Interface) plugin is misconfigured.","A readiness probe for the pod is failing."],correct:0,explanation:"The `ImagePullBackOff` status means that Kubernetes tried to pull the container image from the specified registry but failed. It will keep retrying with an increasing back-off delay. Common reasons for this failure include a typo in the image name or tag, or the GKE node's service account not having permission to read from the image repository (e.g., a private Artifact Registry).",wrongExplanations:{1:"Insufficient resources would likely result in a `Pending` state (if no node can fit it) or a crash after starting, not an image pull failure.",2:"A CNI issue would likely affect pod networking after the container starts, not the image pull process.",3:"A failing readiness probe would cause the pod to not receive traffic, but it wouldn't prevent the container image from being pulled in the first place."}},{id:480,domain:"Section 4: Configuring access and security",subdomain:"IAM",question:"What is the purpose of an IAM Condition?",options:["To add constraints to a role binding, such as making it temporary or only allowing access to resources with specific names or labels.","To define the set of permissions included in a custom role.","To enforce a security posture across an organization, such as restricting resource locations.","To trigger an alert when a user performs a specific action."],correct:0,explanation:"IAM Conditions add an extra layer of attribute-based access control to role bindings. A role binding grants a role to a principal on a resource. A condition adds a constraint to that binding, for example, by specifying that the access is only valid for a certain time period, or only applies to VMs whose names start with `prod-`, or only from a specific IP range.",wrongExplanations:{1:"The set of permissions in a role is its definition. A condition applies to the *binding* of that role, not the role itself.",2:"Enforcing broad constraints like resource location is the job of Organization Policies.",3:"Alerting is done through Cloud Audit Logs and Cloud Monitoring, not IAM Conditions."}},{id:481,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Cost management",question:"You are running a predictable, steady-state workload on Compute Engine 24/7. What is the best way to significantly reduce your compute costs for this workload?",options:["Purchase a 1-year or 3-year Committed Use Discount (CUD) for the required vCPU and memory.","Use Spot VMs for all instances.","Rely on automatic Sustained Use Discounts.","Choose the E2 machine series."],correct:0,explanation:"Committed Use Discounts provide the largest discounts (up to 70%) in exchange for committing to pay for a certain amount of vCPU and memory for a 1 or 3-year term, regardless of whether you use them. For a workload that is stable and runs 24/7, a CUD provides a guaranteed, significant cost reduction.",wrongExplanations:{1:"Spot VMs are for interruptible, fault-tolerant workloads, not for a steady-state workload that needs to run 24/7.",2:"Sustained Use Discounts are automatic but provide a much smaller discount than a CUD.",3:"While the E2 series is cost-effective, the discount from a CUD on a standard machine series will be much greater."}},{id:482,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Build",question:"Your `cloudbuild.yaml` file defines several steps that need to be run in a specific order. How does Cloud Build handle the execution of these steps?",options:["By default, it runs the steps serially in the order they are defined in the file.","It runs all steps in parallel to speed up the build.","You must explicitly define dependencies between steps using the `waitFor` keyword.","It runs the steps in alphabetical order based on the step's `id`."],correct:0,explanation:"By default, the steps in a `cloudbuild.yaml` file are executed serially, one after the other, in the order they appear in the list. The build will only proceed to the next step if the previous one completes successfully.",wrongExplanations:{1:"To run steps in parallel, you must explicitly configure dependencies using `waitFor`.",2:"The `waitFor` keyword is used to explicitly control dependencies and allow for parallel execution, but the default behavior is serial.",3:"Execution order is based on the list order in the YAML file, not on the `id` field."}},{id:483,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Monitoring",question:"What is an 'uptime check' in Cloud Monitoring used for?",options:["To periodically send a request to a URL, IP, or TCP port to verify that a service is responsive.","To monitor the CPU and memory utilization of a VM.","To check if your project has exceeded its quota.","To verify that a user has the correct IAM permissions."],correct:0,explanation:"Uptime checks are a form of black-box monitoring. They simulate a user's request from various locations around the world to check if your application is available and responding correctly. You can configure checks for HTTP, HTTPS, and TCP endpoints, and then create alerting policies based on the success or failure of these checks.",wrongExplanations:{1:"CPU and memory are system metrics collected by the Monitoring agent (white-box monitoring), not by an uptime check.",2:"Quota monitoring is a separate feature within Monitoring.",3:"IAM permissions are not checked by Monitoring."}},{id:484,domain:"Section 4: Configuring access and security",subdomain:"Binary Authorization",question:"You want to enforce a policy in your GKE cluster that only allows container images that have been approved by your QA team to be deployed. How can you automate this enforcement at deployment time?",options:["Use Binary Authorization with attestations.","Use an Organization Policy to restrict image repositories.","Use Cloud Armor to block deployments.","Use IAM roles to restrict who can deploy."],correct:0,explanation:"Binary Authorization is a deployment-time security control. You can create a policy that requires one or more 'attestations' for an image before it can be deployed. Your CI/CD pipeline can be configured so that after the QA team validates a build, it creates a cryptographic signature (an attestation) for that specific image digest. The Binary Authorization enforcer in GKE will then block any deployment attempt of an image that lacks this required attestation.",wrongExplanations:{1:"Restricting repositories is a good first step, but it doesn't verify that a specific image within that repository has passed QA.",2:"Cloud Armor is a network security tool, not a deployment control tool.",3:"IAM controls who can perform the deploy action, but it doesn't validate the content of what they are deploying."}},{id:485,domain:"Section 2: Planning and implementing a cloud solution",subdomain:"Cloud Storage",question:"What happens when you enable 'Requester Pays' on a Cloud Storage bucket?",options:["The user who downloads the data from the bucket pays for the network egress charges, not the bucket owner.","The user who uploads data to the bucket pays for the storage costs.","All users must have a service account to access the bucket.","The bucket becomes publicly accessible."],correct:0,explanation:"Ordinarily, the owner of a bucket pays for all costs associated with it, including network egress when someone downloads data. The 'Requester Pays' feature flips this for network and data access charges. The person or service making the request must have a billing-enabled project and they will be billed for the data they download. This is often used for sharing large datasets where you want to provide the data but not pay for everyone's download costs.",wrongExplanations:{1:"Storage costs are always paid by the bucket owner.",2:"Access is still controlled by IAM. Requester Pays only changes the billing model for access charges.",3:"It does not make the bucket public; IAM permissions still apply."}},{id:486,domain:"Section 1: Setting up a cloud solution environment",subdomain:"Resource hierarchy",question:"You have been given the `roles/resourcemanager.projectCreator` role at the Organization level. What does this allow you to do?",options:["Create new Google Cloud projects within the Organization.","Create new Folders within the Organization.","Become the Owner of any existing project in the Organization.","Manage the billing account for the Organization."],correct:0,explanation:"The `roles/resourcemanager.projectCreator` role grants the single permission `resourcemanager.projects.create`. This allows the user to create new projects. By default, when a user creates a project, they are automatically granted the Owner role for that new project.",wrongExplanations:{1:"Creating folders requires the `roles/resourcemanager.folderCreator` role.",2:"This role does not grant any permissions on existing projects.",3:"Managing billing requires a `billing. role."}},{id:487,domain:"Section 3: Ensuring successful operation of a cloud solution",subdomain:"Cloud Health",question:"You are looking at the 'Recommendations' page in the Google Cloud Console and see a recommendation to 'Rightsize VM instance' for one of your VMs. What does this mean?",options:["The VM has been consistently underutilized, and you could save money by switching to a smaller machine type.","The VM's operating system is out of date and needs to be patched.","The VM is running in a region with high carbon emissions, and you should move it to a greener region.","The VM does not have the Monitoring agent installed."],correct:0,explanation:"The Active Assist Recommender uses machine learning to analyze your resource usage patterns. A 'rightsizing' recommendation for a VM means it has observed that the VM's CPU and/or memory has been very low for a significant period. It will suggest a smaller, cheaper machine type that could handle the workload, thus saving you money.",wrongExplanations:{1:"OS patching recommendations come from the OS Config service.",2:"Carbon footprint recommendations are a different category of recommendation.",3:"Agent installation is a different type of recommendation, often related to observability."}}],om=()=>{const[P,ae]=Be.useState(!0),[Z,m]=Be.useState(!1),[R,_]=Be.useState(!1),[J,Se]=Be.useState("landing"),[D,x]=Be.useState(null),[N,V]=Be.useState(()=>new Array(En.length).fill(null)),[ne,Ne]=Be.useState(()=>new Array(En.length).fill(!1)),[be,fe]=Be.useState(0),[et,ze]=Be.useState({}),[Oe,ve]=Be.useState(null),[Qe,tt]=Be.useState(!1),[He,O]=Be.useState(null),$=Be.useMemo(()=>D?D.map(E=>En.find(l=>l.id===E)).filter(Boolean):En,[D]),le=$[be];Be.useEffect(()=>{be>=$.length&&fe(Math.max(0,$.length-1))},[$,be]);const It=E=>{const l=[...E];for(let f=l.length-1;f>0;f--){const S=Math.floor(Math.random()*(f+1));[l[f],l[S]]=[l[S],l[f]]}return l},nt=E=>{const l={};return E.forEach(f=>{const S=En.find(k=>k.id===f);if(S){const k=Array.from({length:S.options.length},(z,I)=>I);l[f]=It(k)}}),l},Ge=()=>{const E=It(En.map(S=>S.id)).slice(0,Math.min(50,En.length));x(E),V(S=>{const k=[...S];return E.forEach(z=>k[z]=null),k}),Ne(S=>{const k=[...S];return E.forEach(z=>k[z]=!1),k});const l=nt(E);ze(l),fe(0);const f=Date.now();O(f),ve(7200),tt(!1),localStorage.setItem("examStartTime",f.toString()),localStorage.setItem("examSessionIds",JSON.stringify(E)),localStorage.setItem("shuffledOptions",JSON.stringify(l)),Se("exam"),typeof window<"u"&&ae(window.innerWidth>900)},Mt=()=>{x(null),fe(0),ve(null),O(null),ze({}),localStorage.removeItem("examStartTime"),localStorage.removeItem("examSessionIds"),localStorage.removeItem("shuffledOptions")},ht=()=>{const E=$.filter(f=>N[f.id]!==null).length;if(E===0){alert("You haven't answered any questions yet!");return}window.confirm(`You have answered ${E} of ${$.length} questions.

Are you sure you want to submit your exam?`)&&(ve(null),localStorage.removeItem("examStartTime"),localStorage.removeItem("examSessionIds"),localStorage.removeItem("shuffledOptions"),Se("results"))},ot=E=>{const l=Math.floor(E/3600),f=Math.floor(E%3600/60),S=E%60;return`${l}:${f.toString().padStart(2,"0")}:${S.toString().padStart(2,"0")}`},v=()=>{x(null);const E=En.map(l=>l.id);ze(nt(E)),fe(0),Se("practice"),typeof window<"u"&&ae(window.innerWidth>900)};Be.useEffect(()=>{const E=()=>{const l=window.innerWidth<=900;m(l),ae(!l)};return E(),window.addEventListener("resize",E),()=>window.removeEventListener("resize",E)},[]),Be.useEffect(()=>{if(J!=="exam"||Oe===null)return;const E=setInterval(()=>{ve(l=>l===null||l<=0?(clearInterval(E),alert("Time is up! Your exam will now be submitted."),ht(),0):(l===900&&!Qe&&(tt(!0),alert("Warning: Only 15 minutes remaining!")),l-1))},1e3);return()=>clearInterval(E)},[J,Oe,Qe]),Be.useEffect(()=>{const E=localStorage.getItem("examStartTime"),l=localStorage.getItem("examSessionIds"),f=localStorage.getItem("shuffledOptions");if(E&&l&&J==="exam"){const S=parseInt(E),k=Math.floor((Date.now()-S)/1e3),z=Math.max(0,7200-k);if(z>0){if(ve(z),O(S),f)try{ze(JSON.parse(f))}catch(I){console.error("Failed to restore shuffled options",I)}}else localStorage.removeItem("examStartTime"),localStorage.removeItem("examSessionIds"),localStorage.removeItem("shuffledOptions")}},[]);const T=E=>{if(!le)return;const l=et[le.id],f=l?l[E]:E,S=[...N];S[le.id]=f,V(S)},j=()=>{if(!le)return;const E=[...ne];E[le.id]=!E[le.id],Ne(E)},oe=()=>{const E=["id","question","selected","correct","flagged"],l=$.map(I=>{const Q=N[I.id],Pe=I.correct;return[I.id,'"'+I.question.replace(/"/g,'""')+'"',Q===null?"":Q.toString(),Pe.toString(),ne[I.id]?"1":"0"].join(",")}),f=[E.join(","),...l].join(`
`),S=new Blob([f],{type:"text/csv"}),k=URL.createObjectURL(S),z=document.createElement("a");z.href=k,z.download="results.csv",z.click(),URL.revokeObjectURL(k)};if(J==="landing")return A.jsx("div",{className:"landing-hero",style:{minHeight:"100vh",display:"flex",alignItems:"center",justifyContent:"center",background:"linear-gradient(180deg, #0b4bd8 0%, #0b2e63 100%)",color:"#fff",padding:0,margin:0},children:A.jsxs("div",{style:{width:"100%",textAlign:"center",padding:"80px 24px"},children:[A.jsx("div",{style:{display:"flex",justifyContent:"center",marginBottom:18},"aria-hidden":!0,children:A.jsxs("svg",{width:"72",height:"48",viewBox:"0 0 64 48",fill:"none",xmlns:"http://www.w3.org/2000/svg",children:[A.jsx("defs",{children:A.jsxs("linearGradient",{id:"g1",x1:"0",x2:"1",children:[A.jsx("stop",{offset:"0",stopColor:"#fff",stopOpacity:"0.95"}),A.jsx("stop",{offset:"1",stopColor:"#e6f0ff",stopOpacity:"0.9"})]})}),A.jsx("path",{d:"M20 30c-6 0-10-5-10-10 0-5 4-10 10-10 2 0 4 0 6 1 3-5 9-6 15-3 5 3 7 9 5 14-1 4-5 8-11 8H20z",fill:"url(#g1)"})]})}),A.jsxs("h1",{style:{fontSize:56,margin:"0 0 8px",fontWeight:800,lineHeight:1.05},children:["Google Cloud",A.jsx("br",{}),"Associate Engineer"]}),A.jsx("p",{style:{marginTop:8,fontSize:18,opacity:.95},children:"Practice Exam - 2025 Edition  50 Questions"}),A.jsx("div",{style:{marginTop:36,display:"flex",justifyContent:"center"},children:A.jsxs("div",{style:{width:"min(820px, 92%)",textAlign:"center",padding:"26px 30px",borderRadius:14,background:"rgba(0,0,0,0.12)"},children:[A.jsx("h2",{style:{color:"#fff",marginTop:0,marginBottom:6},children:"Start New Exam"}),A.jsx("p",{style:{color:"rgba(255,255,255,0.9)",marginTop:0},children:"Full-length practice exam simulating the real GCE certification test. All questions include explanations."}),A.jsxs("div",{style:{display:"flex",gap:12,justifyContent:"center",marginTop:18},children:[A.jsx("button",{onClick:()=>{Ge(),ae(!0)},style:{background:"#1a73e8",color:"#fff",border:"none",padding:"12px 22px",borderRadius:8,fontSize:16},children:"Start New Exam "}),A.jsx("button",{onClick:()=>{v()},style:{background:"transparent",color:"#fff",border:"2px solid rgba(255,255,255,0.16)",padding:"10px 18px",borderRadius:8,fontSize:16},children:"Practice"})]})]})})]})});if(J==="results"){const E=$.filter(k=>N[k.id]===k.correct).length,l=$.length,f=Math.round(E/l*100),S=f>=70;return A.jsx("div",{className:"results-container",style:{minHeight:"100vh",padding:"40px 20px",background:"linear-gradient(to bottom right, #1e293b, #1e40af, #1e293b)"},children:A.jsxs("div",{style:{maxWidth:1200,margin:"0 auto"},children:[A.jsxs("div",{className:"results-header",style:{background:"rgba(51,65,85,0.95)",borderRadius:16,padding:40,marginBottom:32,backdropFilter:"blur(12px)",border:"1px solid rgba(255,255,255,0.1)",boxShadow:"0 20px 60px rgba(0,0,0,0.3)"},children:[A.jsx("h1",{style:{margin:0,fontSize:36,color:"#f9fafb",textAlign:"center"},children:"Exam Results"}),A.jsxs("div",{style:{marginTop:24,textAlign:"center",fontSize:72,fontWeight:800,color:S?"#34d399":"#f87171"},children:[f,"%"]}),A.jsx("div",{style:{marginTop:16,textAlign:"center",fontSize:24,fontWeight:600,color:S?"#34d399":"#f87171"},children:S?" PASSED":" FAILED"}),A.jsxs("div",{style:{marginTop:16,textAlign:"center",fontSize:18,color:"#9ca3af"},children:["You answered ",E," out of ",l," questions correctly"]}),A.jsx("div",{style:{marginTop:32,textAlign:"center"},children:A.jsx("button",{onClick:()=>{Mt(),Se("landing")},style:{background:"linear-gradient(135deg, rgba(59,130,246,0.9), rgba(96,165,250,0.9))",color:"#fff",border:"none",padding:"14px 32px",borderRadius:8,fontSize:16,fontWeight:600,cursor:"pointer"},children:"Back to Home"})})]}),A.jsxs("div",{className:"results-breakdown",children:[A.jsx("h2",{style:{fontSize:24,color:"#f9fafb",marginBottom:24},children:"Detailed Breakdown"}),$.map((k,z)=>{const I=N[k.id],Q=I===k.correct;return et[k.id],A.jsxs("div",{style:{background:"rgba(51,65,85,0.95)",borderRadius:12,padding:24,marginBottom:24,backdropFilter:"blur(12px)",border:`2px solid ${Q?"rgba(52,211,153,0.3)":"rgba(248,113,113,0.3)"}`,boxShadow:"0 8px 24px rgba(0,0,0,0.2)"},children:[A.jsxs("div",{style:{display:"flex",alignItems:"center",gap:12,marginBottom:16},children:[A.jsx("div",{style:{fontSize:24,fontWeight:700,color:Q?"#34d399":"#f87171"},children:Q?"":""}),A.jsxs("div",{style:{fontSize:14,color:"#9ca3af"},children:["Question ",z+1," of ",l]})]}),A.jsx("div",{style:{marginBottom:12},children:A.jsx("span",{className:"domain-badge",style:{fontSize:12},children:k.domain})}),A.jsx("h3",{style:{fontSize:18,color:"#f9fafb",marginBottom:16,lineHeight:1.5},children:k.question}),A.jsx("div",{style:{display:"grid",gap:12,marginBottom:20},children:k.options.map((Pe,ge)=>{const Gt=I===ge,Vt=k.correct===ge;return A.jsx("div",{style:{padding:"12px 16px",borderRadius:8,border:`2px solid ${Vt?"rgba(52,211,153,0.6)":Gt?"rgba(248,113,113,0.6)":"rgba(255,255,255,0.1)"}`,background:Vt?"rgba(52,211,153,0.1)":Gt?"rgba(248,113,113,0.1)":"rgba(51,65,85,0.5)",color:"#f9fafb"},children:A.jsxs("div",{style:{display:"flex",alignItems:"center",gap:8},children:[Vt&&A.jsx("span",{style:{color:"#34d399",fontWeight:600},children:" Correct"}),Gt&&!Vt&&A.jsx("span",{style:{color:"#f87171",fontWeight:600},children:" Your Answer"}),A.jsx("span",{children:Pe})]})},ge)})}),A.jsxs("div",{style:{background:"rgba(59,130,246,0.1)",border:"1px solid rgba(59,130,246,0.2)",borderRadius:8,padding:16,marginBottom:12},children:[A.jsx("div",{style:{fontWeight:600,color:"#60a5fa",marginBottom:8},children:" Explanation:"}),A.jsx("div",{style:{color:"#e5e7eb",lineHeight:1.6},children:k.explanation})]}),!Q&&I!==null&&k.wrongExplanations&&k.wrongExplanations[I]&&A.jsxs("div",{style:{background:"rgba(248,113,113,0.1)",border:"1px solid rgba(248,113,113,0.2)",borderRadius:8,padding:16},children:[A.jsx("div",{style:{fontWeight:600,color:"#f87171",marginBottom:8},children:" Why your answer was wrong:"}),A.jsx("div",{style:{color:"#e5e7eb",lineHeight:1.6},children:k.wrongExplanations[I]})]})]},k.id)})]})]})})}return A.jsxs("div",{className:"app-layout",children:[A.jsx("div",{className:"exam-banner fixed-banner",children:A.jsxs("div",{style:{display:"flex",justifyContent:"space-between",alignItems:"center",flexWrap:"wrap",gap:12},children:[A.jsxs("div",{style:{display:"flex",alignItems:"center",gap:16},children:[A.jsx("div",{style:{fontWeight:700},children:J==="exam"?`Exam  ${$.length} questions`:"Practice"}),J==="exam"&&Oe!==null&&A.jsxs("div",{style:{fontWeight:600,fontSize:18,color:Oe<900?"#f87171":"#60a5fa",padding:"4px 12px",borderRadius:8,background:Oe<900?"rgba(248,113,113,0.1)":"rgba(96,165,250,0.1)"},children:[" ",ot(Oe)]})]}),A.jsxs("div",{style:{display:"flex",gap:8,alignItems:"center",flexWrap:"wrap"},children:[J==="exam"&&A.jsxs("button",{className:"submit-btn",onClick:ht,style:{background:"linear-gradient(135deg, rgba(34,197,94,0.9), rgba(74,222,128,0.9))",fontWeight:600},children:["Submit Exam (",$.filter(E=>N[E.id]!==null).length,"/",$.length,")"]}),J!=="exam"&&A.jsx("button",{className:"csv-btn",onClick:()=>{Ge(),ae(window.innerWidth>900)},children:"Start 50-Question Exam"}),A.jsx("button",{className:"csv-btn",onClick:()=>{Mt(),Se("landing"),ae(!1)},children:"Back to Home"}),!Z&&A.jsx("button",{className:"csv-btn",onClick:()=>ae(E=>!E),children:P?"Hide Sidebar":"Show Sidebar"})]})]})}),A.jsx("div",{className:"sidebar-wrapper",children:P&&!Z&&A.jsxs("aside",{className:"sidebar fixed-sidebar",children:[A.jsx("h3",{style:{marginTop:0},children:"Navigator"}),A.jsx("div",{className:"navigator-grid",children:$.map((E,l)=>A.jsx("button",{className:`nav-item ${N[E.id]!==null?"answered":""} ${ne[E.id]?"flagged":""} ${l===be?"current":""}`,onClick:()=>fe(l),"aria-pressed":l===be,title:`Question ${l+1}`,children:l+1},E.id))}),A.jsx("div",{style:{marginTop:12},children:A.jsx("button",{className:"submit-all-btn",onClick:oe,children:"Export CSV"})})]})}),Z&&J!=="landing"&&A.jsxs(A.Fragment,{children:[A.jsx("button",{className:"mobile-nav-fab",onClick:()=>_(!0),"aria-label":"Open navigator",children:""}),R&&A.jsx("div",{className:"mobile-navigator-sheet",children:A.jsxs("div",{className:"sheet-panel",children:[A.jsxs("div",{className:"sheet-header",children:[A.jsx("strong",{children:"Navigator"}),A.jsx("button",{className:"sheet-close",onClick:()=>_(!1),"aria-label":"Close",children:""})]}),A.jsx("div",{className:"nav-grid",children:$.map((E,l)=>A.jsx("button",{className:`nav-item ${l===be?"current":""} ${N[E.id]!==null?"answered":""} ${ne[E.id]?"flagged":""}`,onClick:()=>{fe(l),_(!1)},children:l+1},E.id))}),A.jsx("div",{className:"sheet-footer",children:A.jsx("button",{onClick:()=>{oe(),_(!1)},children:"Export CSV"})})]})})]}),A.jsx("main",{className:"main-area",style:{marginLeft:!Z&&P?"calc(var(--sidebar-width) + 32px)":0,marginTop:0},children:A.jsx("div",{style:{padding:20},children:le?A.jsxs("div",{children:[A.jsxs("div",{style:{marginBottom:16},children:[A.jsx("span",{className:"domain-badge",children:le.domain}),le.subdomain&&A.jsx("div",{style:{marginTop:8,fontSize:14,color:"#9ca3af"},children:le.subdomain})]}),A.jsxs("div",{style:{display:"flex",justifyContent:"space-between",alignItems:"center",marginBottom:24},children:[A.jsxs("div",{style:{fontSize:14,color:"#9ca3af"},children:["Question ",be+1," of ",$.length]}),A.jsxs("button",{onClick:j,className:`flag-button ${ne[le.id]?"flagged":""}`,children:[" ",ne[le.id]?"Flagged":"Flag"]})]}),A.jsx("h2",{style:{marginTop:0,fontSize:22,lineHeight:1.5,color:"#f9fafb",fontWeight:500},children:le.question}),A.jsx("div",{style:{display:"grid",gap:8,marginTop:12},children:(()=>{const E=et[le.id];return(E?E.map(f=>le.options[f]):le.options).map((f,S)=>{const k=E?E[S]:S,z=N[le.id]===k;return A.jsx("div",{className:`option ${z?"selected":""}`,onClick:()=>T(S),role:"button",tabIndex:0,onKeyDown:I=>{(I.key==="Enter"||I.key===" ")&&T(S)},children:f},S)})})()}),A.jsxs("div",{style:{marginTop:18,display:"flex",gap:8},children:[A.jsx("button",{onClick:()=>fe(E=>Math.max(0,E-1)),className:"nav-btn",children:"Previous"}),A.jsx("button",{onClick:()=>fe(E=>Math.min($.length-1,E+1)),className:"nav-btn",children:"Next"})]})]}):A.jsx("div",{children:"No questions available"})})})]})},im=document.getElementById("root"),am=nm.createRoot(im);am.render(A.jsx(om,{}));
